{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Word2Vec in Gensim and making it work!\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making and assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. \n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec and actually get it to work! I have heard a lot of complaints about poor performance etc, but its really a combination of two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) and extended with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "Next, is our dataset. The secret to getting Word2Vec really working for you is to have lots and lots of text data. In this case I am going to use data from the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset. This dataset has full user reviews of cars and hotels. I have specifically concatenated all of the hotel reviews into one big file which is about 97MB compressed and 229MB uncompressed. We will use the compressed file for this tutorial. Each line in this file represents a hotel review. You can download the OpinRank Word2Vec dataset here.\n",
    "\n",
    "To avoid confusion, while gensim’s word2vec tutorial says that you need to pass it a sequence of sentences as its input, you can always pass it a whole review as a sentence (i.e. a much larger size of text), and it should not make much of a difference. \n",
    "\n",
    "Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "data_file=\"reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization, lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-19 12:19:56,429 : INFO : reading file reviews_data.txt.gz...this may take a while\n",
      "2018-02-19 12:19:56,429 : INFO : read 0 reviews\n",
      "2018-02-19 12:19:58,636 : INFO : read 10000 reviews\n",
      "2018-02-19 12:20:00,853 : INFO : read 20000 reviews\n",
      "2018-02-19 12:20:03,414 : INFO : read 30000 reviews\n",
      "2018-02-19 12:20:05,773 : INFO : read 40000 reviews\n",
      "2018-02-19 12:20:08,437 : INFO : read 50000 reviews\n",
      "2018-02-19 12:20:11,057 : INFO : read 60000 reviews\n",
      "2018-02-19 12:20:13,309 : INFO : read 70000 reviews\n",
      "2018-02-19 12:20:15,688 : INFO : read 80000 reviews\n",
      "2018-02-19 12:20:17,979 : INFO : read 90000 reviews\n",
      "2018-02-19 12:20:20,453 : INFO : read 100000 reviews\n",
      "2018-02-19 12:20:23,002 : INFO : read 110000 reviews\n",
      "2018-02-19 12:20:25,636 : INFO : read 120000 reviews\n",
      "2018-02-19 12:20:28,664 : INFO : read 130000 reviews\n",
      "2018-02-19 12:20:31,621 : INFO : read 140000 reviews\n",
      "2018-02-19 12:20:33,945 : INFO : read 150000 reviews\n",
      "2018-02-19 12:20:35,920 : INFO : read 160000 reviews\n",
      "2018-02-19 12:20:38,050 : INFO : read 170000 reviews\n",
      "2018-02-19 12:20:39,899 : INFO : read 180000 reviews\n",
      "2018-02-19 12:20:42,195 : INFO : read 190000 reviews\n",
      "2018-02-19 12:20:45,156 : INFO : read 200000 reviews\n",
      "2018-02-19 12:20:47,450 : INFO : read 210000 reviews\n",
      "2018-02-19 12:20:49,685 : INFO : read 220000 reviews\n",
      "2018-02-19 12:20:51,600 : INFO : read 230000 reviews\n",
      "2018-02-19 12:20:53,690 : INFO : read 240000 reviews\n",
      "2018-02-19 12:20:55,857 : INFO : read 250000 reviews\n",
      "2018-02-19 12:20:57,398 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset takes about 10 minutes so please be patient while running your code on this dataset.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-19 12:21:02,991 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-02-19 12:21:02,992 : INFO : collecting all words and their counts\n",
      "2018-02-19 12:21:02,993 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-19 12:21:03,294 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2018-02-19 12:21:03,559 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2018-02-19 12:21:03,913 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2018-02-19 12:21:04,276 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2018-02-19 12:21:04,661 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2018-02-19 12:21:05,046 : INFO : PROGRESS: at sentence #60000, processed 11013723 words, keeping 76781 word types\n",
      "2018-02-19 12:21:05,329 : INFO : PROGRESS: at sentence #70000, processed 12637525 words, keeping 83194 word types\n",
      "2018-02-19 12:21:05,561 : INFO : PROGRESS: at sentence #80000, processed 14099751 words, keeping 88454 word types\n",
      "2018-02-19 12:21:05,914 : INFO : PROGRESS: at sentence #90000, processed 15662149 words, keeping 93352 word types\n",
      "2018-02-19 12:21:06,333 : INFO : PROGRESS: at sentence #100000, processed 17164487 words, keeping 97881 word types\n",
      "2018-02-19 12:21:06,739 : INFO : PROGRESS: at sentence #110000, processed 18652292 words, keeping 102127 word types\n",
      "2018-02-19 12:21:07,004 : INFO : PROGRESS: at sentence #120000, processed 20152529 words, keeping 105918 word types\n",
      "2018-02-19 12:21:07,256 : INFO : PROGRESS: at sentence #130000, processed 21684330 words, keeping 110099 word types\n",
      "2018-02-19 12:21:07,544 : INFO : PROGRESS: at sentence #140000, processed 23330206 words, keeping 114103 word types\n",
      "2018-02-19 12:21:07,797 : INFO : PROGRESS: at sentence #150000, processed 24838754 words, keeping 118169 word types\n",
      "2018-02-19 12:21:08,046 : INFO : PROGRESS: at sentence #160000, processed 26390910 words, keeping 118665 word types\n",
      "2018-02-19 12:21:08,299 : INFO : PROGRESS: at sentence #170000, processed 27913916 words, keeping 123350 word types\n",
      "2018-02-19 12:21:08,560 : INFO : PROGRESS: at sentence #180000, processed 29535612 words, keeping 126742 word types\n",
      "2018-02-19 12:21:08,937 : INFO : PROGRESS: at sentence #190000, processed 31096459 words, keeping 129841 word types\n",
      "2018-02-19 12:21:09,354 : INFO : PROGRESS: at sentence #200000, processed 32805271 words, keeping 133249 word types\n",
      "2018-02-19 12:21:09,628 : INFO : PROGRESS: at sentence #210000, processed 34434198 words, keeping 136358 word types\n",
      "2018-02-19 12:21:09,941 : INFO : PROGRESS: at sentence #220000, processed 36083482 words, keeping 139412 word types\n",
      "2018-02-19 12:21:10,233 : INFO : PROGRESS: at sentence #230000, processed 37571762 words, keeping 142393 word types\n",
      "2018-02-19 12:21:10,497 : INFO : PROGRESS: at sentence #240000, processed 39138190 words, keeping 145226 word types\n",
      "2018-02-19 12:21:10,776 : INFO : PROGRESS: at sentence #250000, processed 40695049 words, keeping 147960 word types\n",
      "2018-02-19 12:21:10,947 : INFO : collected 150053 word types from a corpus of 41519355 raw words and 255404 sentences\n",
      "2018-02-19 12:21:10,948 : INFO : Loading a fresh vocabulary\n",
      "2018-02-19 12:21:11,705 : INFO : min_count=2 retains 70538 unique words (47% of original 150053, drops 79515)\n",
      "2018-02-19 12:21:11,705 : INFO : min_count=2 leaves 41439840 word corpus (99% of original 41519355, drops 79515)\n",
      "2018-02-19 12:21:11,937 : INFO : deleting the raw counts dictionary of 150053 items\n",
      "2018-02-19 12:21:11,937 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2018-02-19 12:21:11,937 : INFO : downsampling leaves estimated 30349255 word corpus (73.2% of prior 41439840)\n",
      "2018-02-19 12:21:11,937 : INFO : estimated required memory for 70538 words and 150 dimensions: 119914600 bytes\n",
      "2018-02-19 12:21:12,184 : INFO : resetting layer weights\n",
      "2018-02-19 12:21:13,106 : INFO : training model with 10 workers on 70538 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-02-19 12:21:14,131 : INFO : PROGRESS: at 0.64% examples, 980165 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:15,145 : INFO : PROGRESS: at 1.54% examples, 1183985 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:21:16,149 : INFO : PROGRESS: at 2.33% examples, 1261528 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:17,159 : INFO : PROGRESS: at 3.13% examples, 1283195 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:18,160 : INFO : PROGRESS: at 3.86% examples, 1298002 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:21:19,168 : INFO : PROGRESS: at 4.62% examples, 1308218 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:20,169 : INFO : PROGRESS: at 5.46% examples, 1307474 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:21,177 : INFO : PROGRESS: at 6.41% examples, 1306949 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:21:22,180 : INFO : PROGRESS: at 7.37% examples, 1315194 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:21:23,180 : INFO : PROGRESS: at 8.37% examples, 1320339 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:21:24,183 : INFO : PROGRESS: at 9.25% examples, 1312181 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:25,205 : INFO : PROGRESS: at 10.09% examples, 1302556 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:21:26,213 : INFO : PROGRESS: at 10.93% examples, 1299557 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:27,222 : INFO : PROGRESS: at 11.88% examples, 1301792 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:21:28,225 : INFO : PROGRESS: at 12.86% examples, 1305992 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:21:29,232 : INFO : PROGRESS: at 13.71% examples, 1304522 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:30,237 : INFO : PROGRESS: at 14.56% examples, 1302057 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:21:31,241 : INFO : PROGRESS: at 15.43% examples, 1303839 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:21:32,246 : INFO : PROGRESS: at 16.19% examples, 1296133 words/s, in_qsize 16, out_qsize 3\n",
      "2018-02-19 12:21:33,264 : INFO : PROGRESS: at 16.92% examples, 1286780 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:34,272 : INFO : PROGRESS: at 17.80% examples, 1284139 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:21:35,275 : INFO : PROGRESS: at 18.71% examples, 1285901 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:36,282 : INFO : PROGRESS: at 19.62% examples, 1286994 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:37,284 : INFO : PROGRESS: at 20.45% examples, 1285641 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:38,317 : INFO : PROGRESS: at 21.33% examples, 1286147 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:21:39,304 : INFO : PROGRESS: at 22.15% examples, 1290422 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:40,323 : INFO : PROGRESS: at 22.90% examples, 1292089 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:41,326 : INFO : PROGRESS: at 23.66% examples, 1294594 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:42,344 : INFO : PROGRESS: at 24.39% examples, 1293959 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:43,347 : INFO : PROGRESS: at 25.19% examples, 1297238 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:44,350 : INFO : PROGRESS: at 26.18% examples, 1299051 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:21:45,353 : INFO : PROGRESS: at 27.16% examples, 1301928 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:21:46,355 : INFO : PROGRESS: at 28.07% examples, 1301108 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:47,358 : INFO : PROGRESS: at 29.10% examples, 1303737 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:48,361 : INFO : PROGRESS: at 30.05% examples, 1305825 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:49,386 : INFO : PROGRESS: at 30.95% examples, 1306887 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:21:50,387 : INFO : PROGRESS: at 31.84% examples, 1305703 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:51,389 : INFO : PROGRESS: at 32.71% examples, 1303920 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:21:52,391 : INFO : PROGRESS: at 33.66% examples, 1305590 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-19 12:21:53,393 : INFO : PROGRESS: at 34.60% examples, 1308178 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:54,395 : INFO : PROGRESS: at 35.50% examples, 1310248 words/s, in_qsize 20, out_qsize 3\n",
      "2018-02-19 12:21:55,413 : INFO : PROGRESS: at 36.41% examples, 1312006 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:21:56,431 : INFO : PROGRESS: at 37.24% examples, 1310555 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:21:57,434 : INFO : PROGRESS: at 38.19% examples, 1310632 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:58,437 : INFO : PROGRESS: at 39.05% examples, 1309285 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:21:59,440 : INFO : PROGRESS: at 39.87% examples, 1306645 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:00,454 : INFO : PROGRESS: at 40.68% examples, 1305190 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:01,456 : INFO : PROGRESS: at 41.58% examples, 1306585 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:02,461 : INFO : PROGRESS: at 42.34% examples, 1308039 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:03,464 : INFO : PROGRESS: at 43.02% examples, 1305409 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:04,460 : INFO : PROGRESS: at 43.73% examples, 1304509 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:05,462 : INFO : PROGRESS: at 44.52% examples, 1307047 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:06,465 : INFO : PROGRESS: at 45.38% examples, 1308669 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:07,468 : INFO : PROGRESS: at 46.41% examples, 1310475 words/s, in_qsize 16, out_qsize 3\n",
      "2018-02-19 12:22:08,486 : INFO : PROGRESS: at 47.39% examples, 1312195 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:09,489 : INFO : PROGRESS: at 48.42% examples, 1313890 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:10,492 : INFO : PROGRESS: at 49.41% examples, 1315374 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:11,495 : INFO : PROGRESS: at 50.40% examples, 1317152 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:12,498 : INFO : PROGRESS: at 51.36% examples, 1318672 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:13,501 : INFO : PROGRESS: at 52.33% examples, 1320183 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:14,504 : INFO : PROGRESS: at 53.33% examples, 1321953 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:15,522 : INFO : PROGRESS: at 54.27% examples, 1323296 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:16,524 : INFO : PROGRESS: at 55.23% examples, 1324630 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:17,549 : INFO : PROGRESS: at 56.14% examples, 1326098 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:18,552 : INFO : PROGRESS: at 57.07% examples, 1327377 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:19,555 : INFO : PROGRESS: at 58.08% examples, 1328588 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:20,558 : INFO : PROGRESS: at 59.05% examples, 1329747 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:21,576 : INFO : PROGRESS: at 60.01% examples, 1330516 words/s, in_qsize 19, out_qsize 2\n",
      "2018-02-19 12:22:22,563 : INFO : PROGRESS: at 60.93% examples, 1331724 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:23,581 : INFO : PROGRESS: at 61.81% examples, 1332414 words/s, in_qsize 20, out_qsize 1\n",
      "2018-02-19 12:22:24,584 : INFO : PROGRESS: at 62.51% examples, 1332421 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:22:25,586 : INFO : PROGRESS: at 63.17% examples, 1329205 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:22:26,573 : INFO : PROGRESS: at 63.87% examples, 1328981 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:27,607 : INFO : PROGRESS: at 64.67% examples, 1329982 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:28,594 : INFO : PROGRESS: at 65.58% examples, 1330399 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:29,597 : INFO : PROGRESS: at 66.58% examples, 1331071 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:22:30,600 : INFO : PROGRESS: at 67.54% examples, 1332018 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:31,603 : INFO : PROGRESS: at 68.57% examples, 1333162 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:32,606 : INFO : PROGRESS: at 69.59% examples, 1334280 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:33,608 : INFO : PROGRESS: at 70.53% examples, 1334897 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:34,627 : INFO : PROGRESS: at 71.53% examples, 1336089 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:35,629 : INFO : PROGRESS: at 72.48% examples, 1336725 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:22:36,632 : INFO : PROGRESS: at 73.50% examples, 1338055 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:37,635 : INFO : PROGRESS: at 74.45% examples, 1339086 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:38,637 : INFO : PROGRESS: at 75.37% examples, 1339645 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:39,656 : INFO : PROGRESS: at 76.29% examples, 1340623 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:40,658 : INFO : PROGRESS: at 77.22% examples, 1341361 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:22:41,661 : INFO : PROGRESS: at 78.23% examples, 1342114 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:42,664 : INFO : PROGRESS: at 79.21% examples, 1343153 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:43,667 : INFO : PROGRESS: at 80.18% examples, 1344063 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:44,685 : INFO : PROGRESS: at 81.13% examples, 1344886 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:45,687 : INFO : PROGRESS: at 81.97% examples, 1345443 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:22:46,689 : INFO : PROGRESS: at 82.77% examples, 1346437 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:47,692 : INFO : PROGRESS: at 83.57% examples, 1347048 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:48,695 : INFO : PROGRESS: at 84.36% examples, 1347705 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:49,714 : INFO : PROGRESS: at 85.18% examples, 1348360 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:50,717 : INFO : PROGRESS: at 86.23% examples, 1349138 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:51,738 : INFO : PROGRESS: at 87.21% examples, 1349698 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:52,743 : INFO : PROGRESS: at 88.24% examples, 1350458 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:53,744 : INFO : PROGRESS: at 89.27% examples, 1351059 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:54,746 : INFO : PROGRESS: at 90.23% examples, 1351481 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:22:55,752 : INFO : PROGRESS: at 91.20% examples, 1352168 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:22:56,755 : INFO : PROGRESS: at 92.12% examples, 1351908 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:57,757 : INFO : PROGRESS: at 92.97% examples, 1350034 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:58,760 : INFO : PROGRESS: at 93.81% examples, 1349281 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:22:59,778 : INFO : PROGRESS: at 94.73% examples, 1349352 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:00,781 : INFO : PROGRESS: at 95.58% examples, 1349261 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:01,783 : INFO : PROGRESS: at 96.42% examples, 1348521 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:02,802 : INFO : PROGRESS: at 97.28% examples, 1348136 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:03,789 : INFO : PROGRESS: at 98.24% examples, 1348085 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:04,792 : INFO : PROGRESS: at 99.17% examples, 1348257 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:05,594 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-02-19 12:23:05,594 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-02-19 12:23:05,609 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-19 12:23:05,625 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-19 12:23:05,625 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-19 12:23:05,625 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-19 12:23:05,625 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-19 12:23:05,625 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-19 12:23:05,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-19 12:23:05,641 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-19 12:23:05,641 : INFO : training on 207596775 raw words (151753870 effective words) took 112.5s, 1348765 effective words/s\n",
      "2018-02-19 12:23:05,641 : INFO : training model with 10 workers on 70538 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-02-19 12:23:06,658 : INFO : PROGRESS: at 0.38% examples, 1170736 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:23:07,660 : INFO : PROGRESS: at 0.82% examples, 1259487 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:23:08,663 : INFO : PROGRESS: at 1.20% examples, 1308754 words/s, in_qsize 16, out_qsize 3\n",
      "2018-02-19 12:23:09,666 : INFO : PROGRESS: at 1.59% examples, 1314934 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:10,684 : INFO : PROGRESS: at 1.95% examples, 1318617 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:11,689 : INFO : PROGRESS: at 2.31% examples, 1306892 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:12,691 : INFO : PROGRESS: at 2.77% examples, 1320644 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:13,699 : INFO : PROGRESS: at 3.27% examples, 1328877 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:14,701 : INFO : PROGRESS: at 3.75% examples, 1336919 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:15,703 : INFO : PROGRESS: at 4.22% examples, 1329388 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:16,706 : INFO : PROGRESS: at 4.72% examples, 1336861 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:17,708 : INFO : PROGRESS: at 5.21% examples, 1343020 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:23:18,715 : INFO : PROGRESS: at 5.57% examples, 1322950 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:19,724 : INFO : PROGRESS: at 6.03% examples, 1322463 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:20,730 : INFO : PROGRESS: at 6.53% examples, 1326792 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:21,736 : INFO : PROGRESS: at 7.00% examples, 1332711 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:22,737 : INFO : PROGRESS: at 7.48% examples, 1337719 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:23,738 : INFO : PROGRESS: at 7.93% examples, 1341732 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:24,743 : INFO : PROGRESS: at 8.39% examples, 1345782 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:25,745 : INFO : PROGRESS: at 8.89% examples, 1350012 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:26,757 : INFO : PROGRESS: at 9.38% examples, 1352680 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:27,758 : INFO : PROGRESS: at 9.87% examples, 1356299 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:28,765 : INFO : PROGRESS: at 10.35% examples, 1359603 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:29,767 : INFO : PROGRESS: at 10.81% examples, 1362505 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:30,773 : INFO : PROGRESS: at 11.20% examples, 1364179 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:31,773 : INFO : PROGRESS: at 11.61% examples, 1366072 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:32,783 : INFO : PROGRESS: at 11.99% examples, 1368159 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:33,786 : INFO : PROGRESS: at 12.39% examples, 1370078 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:23:34,790 : INFO : PROGRESS: at 12.88% examples, 1371444 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:35,790 : INFO : PROGRESS: at 13.38% examples, 1373011 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:36,797 : INFO : PROGRESS: at 13.87% examples, 1373308 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:37,797 : INFO : PROGRESS: at 14.39% examples, 1375252 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:38,806 : INFO : PROGRESS: at 14.89% examples, 1376386 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:39,808 : INFO : PROGRESS: at 15.35% examples, 1376050 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:40,816 : INFO : PROGRESS: at 15.84% examples, 1377153 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:41,816 : INFO : PROGRESS: at 16.33% examples, 1377495 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:42,829 : INFO : PROGRESS: at 16.82% examples, 1378196 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:43,837 : INFO : PROGRESS: at 17.30% examples, 1379427 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:44,839 : INFO : PROGRESS: at 17.76% examples, 1380468 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:45,845 : INFO : PROGRESS: at 18.23% examples, 1381566 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:46,853 : INFO : PROGRESS: at 18.71% examples, 1382705 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:47,861 : INFO : PROGRESS: at 19.22% examples, 1383750 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:48,867 : INFO : PROGRESS: at 19.63% examples, 1380136 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:49,871 : INFO : PROGRESS: at 20.07% examples, 1377610 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:50,889 : INFO : PROGRESS: at 20.50% examples, 1375833 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:51,890 : INFO : PROGRESS: at 20.90% examples, 1373554 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:52,904 : INFO : PROGRESS: at 21.25% examples, 1371680 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:23:53,917 : INFO : PROGRESS: at 21.63% examples, 1369518 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:23:54,919 : INFO : PROGRESS: at 21.92% examples, 1363812 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:55,930 : INFO : PROGRESS: at 22.27% examples, 1360769 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:56,926 : INFO : PROGRESS: at 22.61% examples, 1356829 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:23:57,939 : INFO : PROGRESS: at 23.09% examples, 1355934 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:58,937 : INFO : PROGRESS: at 23.55% examples, 1355001 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:23:59,950 : INFO : PROGRESS: at 23.94% examples, 1350080 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:00,966 : INFO : PROGRESS: at 24.26% examples, 1341417 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:01,977 : INFO : PROGRESS: at 24.60% examples, 1333746 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:24:02,998 : INFO : PROGRESS: at 24.97% examples, 1328970 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:24:04,006 : INFO : PROGRESS: at 25.32% examples, 1324233 words/s, in_qsize 18, out_qsize 2\n",
      "2018-02-19 12:24:05,018 : INFO : PROGRESS: at 25.69% examples, 1319341 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:24:06,020 : INFO : PROGRESS: at 26.11% examples, 1317786 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:07,030 : INFO : PROGRESS: at 26.44% examples, 1310699 words/s, in_qsize 20, out_qsize 4\n",
      "2018-02-19 12:24:08,036 : INFO : PROGRESS: at 26.84% examples, 1308550 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:09,037 : INFO : PROGRESS: at 27.30% examples, 1309909 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:10,039 : INFO : PROGRESS: at 27.74% examples, 1310902 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:24:11,060 : INFO : PROGRESS: at 28.18% examples, 1310789 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:12,061 : INFO : PROGRESS: at 28.60% examples, 1310735 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:13,061 : INFO : PROGRESS: at 29.08% examples, 1310892 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:24:14,062 : INFO : PROGRESS: at 29.56% examples, 1312084 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:24:15,068 : INFO : PROGRESS: at 30.00% examples, 1311400 words/s, in_qsize 20, out_qsize 2\n",
      "2018-02-19 12:24:16,086 : INFO : PROGRESS: at 30.45% examples, 1312144 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:17,105 : INFO : PROGRESS: at 30.89% examples, 1313101 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:18,107 : INFO : PROGRESS: at 31.26% examples, 1314016 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:24:19,121 : INFO : PROGRESS: at 31.65% examples, 1314212 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-19 12:24:20,124 : INFO : PROGRESS: at 31.90% examples, 1308491 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:21,147 : INFO : PROGRESS: at 32.26% examples, 1308056 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:22,155 : INFO : PROGRESS: at 32.57% examples, 1304716 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:23,173 : INFO : PROGRESS: at 33.02% examples, 1303619 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:24,184 : INFO : PROGRESS: at 33.51% examples, 1304852 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:25,188 : INFO : PROGRESS: at 34.03% examples, 1306426 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:26,198 : INFO : PROGRESS: at 34.55% examples, 1307658 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:27,207 : INFO : PROGRESS: at 35.03% examples, 1308784 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:28,218 : INFO : PROGRESS: at 35.51% examples, 1309971 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:29,226 : INFO : PROGRESS: at 35.90% examples, 1307719 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:30,228 : INFO : PROGRESS: at 36.23% examples, 1303456 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:31,247 : INFO : PROGRESS: at 36.57% examples, 1299086 words/s, in_qsize 16, out_qsize 3\n",
      "2018-02-19 12:24:32,250 : INFO : PROGRESS: at 36.97% examples, 1297697 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:24:33,252 : INFO : PROGRESS: at 37.32% examples, 1294895 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:34,254 : INFO : PROGRESS: at 37.71% examples, 1293730 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:24:35,262 : INFO : PROGRESS: at 38.09% examples, 1292662 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:36,276 : INFO : PROGRESS: at 38.56% examples, 1294066 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:37,279 : INFO : PROGRESS: at 39.07% examples, 1295432 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:38,291 : INFO : PROGRESS: at 39.49% examples, 1294293 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:39,293 : INFO : PROGRESS: at 39.85% examples, 1291762 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:24:40,296 : INFO : PROGRESS: at 40.32% examples, 1292974 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:41,298 : INFO : PROGRESS: at 40.77% examples, 1293958 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:42,318 : INFO : PROGRESS: at 41.16% examples, 1294945 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:24:43,309 : INFO : PROGRESS: at 41.57% examples, 1296109 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:44,311 : INFO : PROGRESS: at 41.94% examples, 1296951 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:45,330 : INFO : PROGRESS: at 42.33% examples, 1298000 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:46,348 : INFO : PROGRESS: at 42.75% examples, 1297593 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:47,350 : INFO : PROGRESS: at 43.25% examples, 1298444 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:48,358 : INFO : PROGRESS: at 43.75% examples, 1299621 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:49,367 : INFO : PROGRESS: at 44.23% examples, 1299690 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:50,361 : INFO : PROGRESS: at 44.62% examples, 1297657 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:24:51,379 : INFO : PROGRESS: at 44.96% examples, 1294642 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:24:52,381 : INFO : PROGRESS: at 45.29% examples, 1291732 words/s, in_qsize 20, out_qsize 1\n",
      "2018-02-19 12:24:53,383 : INFO : PROGRESS: at 45.65% examples, 1289371 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:24:54,386 : INFO : PROGRESS: at 46.11% examples, 1289601 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:24:55,389 : INFO : PROGRESS: at 46.60% examples, 1290660 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:56,391 : INFO : PROGRESS: at 47.07% examples, 1291764 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:57,410 : INFO : PROGRESS: at 47.55% examples, 1292822 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:58,413 : INFO : PROGRESS: at 48.00% examples, 1293857 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:24:59,416 : INFO : PROGRESS: at 48.46% examples, 1294831 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:00,418 : INFO : PROGRESS: at 48.96% examples, 1295912 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:01,421 : INFO : PROGRESS: at 49.45% examples, 1296932 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:02,424 : INFO : PROGRESS: at 49.94% examples, 1298048 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:03,442 : INFO : PROGRESS: at 50.41% examples, 1299032 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:04,451 : INFO : PROGRESS: at 50.87% examples, 1300123 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:05,461 : INFO : PROGRESS: at 51.25% examples, 1301198 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:06,464 : INFO : PROGRESS: at 51.68% examples, 1302378 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:07,467 : INFO : PROGRESS: at 52.06% examples, 1303504 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:08,471 : INFO : PROGRESS: at 52.46% examples, 1304485 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:09,480 : INFO : PROGRESS: at 52.98% examples, 1305325 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:10,488 : INFO : PROGRESS: at 53.47% examples, 1306191 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:11,489 : INFO : PROGRESS: at 53.97% examples, 1307070 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:12,494 : INFO : PROGRESS: at 54.49% examples, 1307796 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:13,495 : INFO : PROGRESS: at 54.98% examples, 1308660 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:14,505 : INFO : PROGRESS: at 55.46% examples, 1309414 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:15,510 : INFO : PROGRESS: at 55.96% examples, 1310399 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:16,516 : INFO : PROGRESS: at 56.48% examples, 1311213 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:17,529 : INFO : PROGRESS: at 56.95% examples, 1312132 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:18,537 : INFO : PROGRESS: at 57.44% examples, 1313038 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:19,544 : INFO : PROGRESS: at 57.89% examples, 1313796 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:25:20,556 : INFO : PROGRESS: at 58.37% examples, 1314681 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:21,560 : INFO : PROGRESS: at 58.86% examples, 1315620 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:22,566 : INFO : PROGRESS: at 59.35% examples, 1316285 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:25:23,573 : INFO : PROGRESS: at 59.84% examples, 1316971 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:25:24,585 : INFO : PROGRESS: at 60.32% examples, 1317774 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:25,600 : INFO : PROGRESS: at 60.79% examples, 1318599 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:26,608 : INFO : PROGRESS: at 61.18% examples, 1319256 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:27,608 : INFO : PROGRESS: at 61.60% examples, 1320110 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:28,616 : INFO : PROGRESS: at 61.95% examples, 1320103 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:29,617 : INFO : PROGRESS: at 62.32% examples, 1320096 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:30,627 : INFO : PROGRESS: at 62.74% examples, 1319745 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:31,630 : INFO : PROGRESS: at 63.23% examples, 1319859 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:32,641 : INFO : PROGRESS: at 63.71% examples, 1320468 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:33,643 : INFO : PROGRESS: at 64.23% examples, 1321164 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:34,643 : INFO : PROGRESS: at 64.73% examples, 1321826 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:35,647 : INFO : PROGRESS: at 65.22% examples, 1322244 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:36,648 : INFO : PROGRESS: at 65.71% examples, 1322994 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:37,654 : INFO : PROGRESS: at 66.20% examples, 1323664 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:38,659 : INFO : PROGRESS: at 66.69% examples, 1324277 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-19 12:25:39,661 : INFO : PROGRESS: at 67.16% examples, 1324734 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:40,671 : INFO : PROGRESS: at 67.62% examples, 1325068 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:41,673 : INFO : PROGRESS: at 68.08% examples, 1325738 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:25:42,674 : INFO : PROGRESS: at 68.53% examples, 1326172 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:43,685 : INFO : PROGRESS: at 69.04% examples, 1326775 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:44,690 : INFO : PROGRESS: at 69.53% examples, 1327330 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:45,691 : INFO : PROGRESS: at 70.03% examples, 1327990 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:25:46,697 : INFO : PROGRESS: at 70.50% examples, 1328739 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:47,707 : INFO : PROGRESS: at 70.94% examples, 1329192 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:48,702 : INFO : PROGRESS: at 71.30% examples, 1329160 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:49,718 : INFO : PROGRESS: at 71.69% examples, 1329208 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:50,706 : INFO : PROGRESS: at 72.01% examples, 1328458 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:51,709 : INFO : PROGRESS: at 72.39% examples, 1328693 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:52,711 : INFO : PROGRESS: at 72.83% examples, 1328372 words/s, in_qsize 16, out_qsize 3\n",
      "2018-02-19 12:25:53,714 : INFO : PROGRESS: at 73.32% examples, 1328477 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:54,732 : INFO : PROGRESS: at 73.80% examples, 1328887 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:25:55,735 : INFO : PROGRESS: at 74.32% examples, 1329354 words/s, in_qsize 20, out_qsize 1\n",
      "2018-02-19 12:25:56,738 : INFO : PROGRESS: at 74.82% examples, 1329747 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:25:57,756 : INFO : PROGRESS: at 75.29% examples, 1330073 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:25:58,775 : INFO : PROGRESS: at 75.79% examples, 1330602 words/s, in_qsize 20, out_qsize 0\n",
      "2018-02-19 12:25:59,784 : INFO : PROGRESS: at 76.27% examples, 1331001 words/s, in_qsize 16, out_qsize 3\n",
      "2018-02-19 12:26:00,793 : INFO : PROGRESS: at 76.78% examples, 1331541 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:01,794 : INFO : PROGRESS: at 77.25% examples, 1332124 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:02,795 : INFO : PROGRESS: at 77.71% examples, 1332559 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:03,796 : INFO : PROGRESS: at 78.17% examples, 1332986 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:04,796 : INFO : PROGRESS: at 78.63% examples, 1333411 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:05,801 : INFO : PROGRESS: at 79.15% examples, 1333949 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:06,805 : INFO : PROGRESS: at 79.63% examples, 1334411 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:07,811 : INFO : PROGRESS: at 80.12% examples, 1334989 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:08,811 : INFO : PROGRESS: at 80.59% examples, 1335488 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:09,816 : INFO : PROGRESS: at 81.02% examples, 1335950 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:10,818 : INFO : PROGRESS: at 81.41% examples, 1336419 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:11,829 : INFO : PROGRESS: at 81.81% examples, 1336748 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:12,835 : INFO : PROGRESS: at 82.22% examples, 1337334 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:13,838 : INFO : PROGRESS: at 82.64% examples, 1337745 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:14,846 : INFO : PROGRESS: at 83.16% examples, 1338130 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:15,868 : INFO : PROGRESS: at 83.66% examples, 1338563 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:16,878 : INFO : PROGRESS: at 84.18% examples, 1339067 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:17,889 : INFO : PROGRESS: at 84.69% examples, 1339477 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:18,895 : INFO : PROGRESS: at 85.18% examples, 1339880 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:19,908 : INFO : PROGRESS: at 85.67% examples, 1340327 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:20,909 : INFO : PROGRESS: at 86.15% examples, 1340569 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:21,911 : INFO : PROGRESS: at 86.60% examples, 1340199 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:22,914 : INFO : PROGRESS: at 87.06% examples, 1340441 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:23,917 : INFO : PROGRESS: at 87.50% examples, 1340164 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:24,927 : INFO : PROGRESS: at 87.94% examples, 1340460 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:25,934 : INFO : PROGRESS: at 88.37% examples, 1340186 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:26,944 : INFO : PROGRESS: at 88.76% examples, 1339150 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:27,947 : INFO : PROGRESS: at 89.12% examples, 1337504 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:28,957 : INFO : PROGRESS: at 89.46% examples, 1335795 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:29,963 : INFO : PROGRESS: at 89.90% examples, 1335459 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:30,970 : INFO : PROGRESS: at 90.35% examples, 1335649 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:31,984 : INFO : PROGRESS: at 90.79% examples, 1335717 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:32,999 : INFO : PROGRESS: at 91.17% examples, 1335780 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:34,003 : INFO : PROGRESS: at 91.58% examples, 1336010 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:35,014 : INFO : PROGRESS: at 91.95% examples, 1336186 words/s, in_qsize 20, out_qsize 1\n",
      "2018-02-19 12:26:36,019 : INFO : PROGRESS: at 92.34% examples, 1336555 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:37,022 : INFO : PROGRESS: at 92.81% examples, 1336800 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:38,027 : INFO : PROGRESS: at 93.31% examples, 1336991 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:39,028 : INFO : PROGRESS: at 93.77% examples, 1337012 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:40,031 : INFO : PROGRESS: at 94.27% examples, 1337273 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:41,046 : INFO : PROGRESS: at 94.78% examples, 1337569 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:42,048 : INFO : PROGRESS: at 95.25% examples, 1337748 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:43,055 : INFO : PROGRESS: at 95.74% examples, 1338037 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:44,060 : INFO : PROGRESS: at 96.21% examples, 1338214 words/s, in_qsize 18, out_qsize 1\n",
      "2018-02-19 12:26:45,068 : INFO : PROGRESS: at 96.69% examples, 1338318 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:46,073 : INFO : PROGRESS: at 97.15% examples, 1338554 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:47,083 : INFO : PROGRESS: at 97.63% examples, 1338955 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:48,090 : INFO : PROGRESS: at 98.09% examples, 1339332 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:49,100 : INFO : PROGRESS: at 98.55% examples, 1339652 words/s, in_qsize 17, out_qsize 2\n",
      "2018-02-19 12:26:50,107 : INFO : PROGRESS: at 99.07% examples, 1340044 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:51,108 : INFO : PROGRESS: at 99.56% examples, 1340402 words/s, in_qsize 19, out_qsize 0\n",
      "2018-02-19 12:26:51,983 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-02-19 12:26:51,984 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-02-19 12:26:51,986 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-19 12:26:51,986 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-19 12:26:51,989 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-19 12:26:51,997 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-19 12:26:52,000 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-19 12:26:52,001 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-19 12:26:52,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-19 12:26:52,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-19 12:26:52,011 : INFO : training on 415193550 raw words (303488696 effective words) took 226.4s, 1340731 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "303488696"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PUT GENSIM CODE HERE\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `dirty`. All we need to do here is to call the `most_similar` function and provide the word `dirty` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 20:08:25,026 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.858426570892334),\n",
       " ('stained', 0.7780173420906067),\n",
       " ('unclean', 0.7708290815353394),\n",
       " ('grubby', 0.764405369758606),\n",
       " ('smelly', 0.7589772343635559),\n",
       " ('dusty', 0.7520752549171448),\n",
       " ('dingy', 0.7471007108688354),\n",
       " ('gross', 0.7170611023902893),\n",
       " ('grimy', 0.711336076259613),\n",
       " ('mouldy', 0.7099933624267578)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dirty\"\n",
    "model.most_similar(w1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9137427806854248),\n",
       " ('friendly', 0.8328721523284912),\n",
       " ('cordial', 0.7980687022209167),\n",
       " ('professional', 0.7894892692565918),\n",
       " ('attentive', 0.7774360775947571),\n",
       " ('personable', 0.7533614039421082)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.most_similar(w1, topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('germany', 0.666722297668457),\n",
       " ('canada', 0.6492294669151306),\n",
       " ('spain', 0.6457390785217285),\n",
       " ('england', 0.6394527554512024),\n",
       " ('hawaii', 0.618595540523529),\n",
       " ('barcelona', 0.6143802404403687)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.most_similar(w1, topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrified', 0.813444972038269),\n",
       " ('amazed', 0.7931602001190186),\n",
       " ('stunned', 0.7805903553962708),\n",
       " ('appalled', 0.7623282670974731),\n",
       " ('astonished', 0.7543476819992065),\n",
       " ('dismayed', 0.7507925033569336)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "model.most_similar(w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blanket', 0.6916123032569885),\n",
       " ('duvet', 0.686753511428833),\n",
       " ('mattress', 0.6822925209999084),\n",
       " ('quilt', 0.6781111359596252),\n",
       " ('matress', 0.6581498384475708),\n",
       " ('sheets', 0.6393461227416992),\n",
       " ('pillows', 0.631115734577179),\n",
       " ('pillowcase', 0.623105525970459),\n",
       " ('foam', 0.6123258471488953),\n",
       " ('comforter', 0.5886681079864502)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.most_similar(w1, w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3489625593721133"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.similarity('london', 'paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two identical words\n",
    "model.similarity('paris', 'paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0969603706452369"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.similarity('bed', 'france')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "list = [\"cat\",\"dog\",\"france\"]\n",
    "model.doesnt_match(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shower'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "list = [\"bed\",\"pillow\",\"duvet\",\"shower\"]\n",
    "model.doesnt_match(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary. \n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
