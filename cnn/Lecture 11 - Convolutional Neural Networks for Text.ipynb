{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Text\n",
    "(adapted from Debajyoti Datta)\n",
    "\n",
    "**CNNs have become widely popular in text classification systems. CNNs help in reduction in computation by exploiting local correlation of the input data.**\n",
    "\n",
    "\n",
    "\n",
    "Let's start with inputs:\n",
    "\n",
    "![png](./assets/images/conv.001.png)\n",
    "\n",
    "So the green boxes represent the words or the characters depending on your approach. If you are using character based convolutional neural network then it is characters whereas if you are using words as a unit then it is the word based convolution. And the corresponding blue rows represent the representation of the words or the characters. In the case of character based convolutions, since the number of characters was around 70, including punctuations, numbers, and alphabets, this is generally the one-hot representation. In the case of words, the blue boxes generally represent dense vectors. These dense vectors can be pre-trained word embeddings or word vectors trained during training. \n",
    "\n",
    "<!-- <img src=\"conv/conv.002.png\",width=550,height=550> -->\n",
    "\n",
    "![png](./assets/images/conv.002.png)\n",
    "\n",
    "Now as you can see, filters (also known as kernels) can be of any length. Here the length refers to the number of rows of the filter. In the case of images, the width and length of the kernel can be any size. But the width of the kernel in case of character and word representations is the dimension of the entire word embedding or the entire character representation. Thus the only dimension that matters in the case of convolutions in NLP tasks, is the length of the filter or the size of the filter.\n",
    "\n",
    "Now the filters need to convolve with the input and produce the output. Convolve is a fancy term for multiplication with corresponding cells and adding up the sum. This part is slightly tricky to understand and varies based on things like stride (How much the filter moves every stage?) and the length of the filter. The output of the convolution operation is directly dependent on these two aspects. This will become clearer in the following image.\n",
    "\n",
    "<!-- <img src=\"conv/conv.003.png\",width=300,height=300>\n",
    "<img src=\"conv/conv.004.png\",width=300,height=300>\n",
    "<img src=\"conv/conv.005.png\",width=300,height=300>\n",
    "<img src=\"conv/conv.006.png\",width=300,height=300> -->\n",
    "\n",
    "![png](./assets/images/conv.003.png)\n",
    "![png](./assets/images/conv.004.png)\n",
    "![png](./assets/images/conv.005.png)\n",
    "![png](./assets/images/conv.006.png)\n",
    "\n",
    "Now, this is where all the interesting bit happens! The convolution is just the multiplication of the weights in the filters and the corresponding representation of the words or characters. Each of the output of the multiplication is then just summed up and it produces one output, shown with the arrow. Thus if the filter would have moved with a stride of 2, then the number of filter outputs would have been different. If the filter length was different, the convolved output would be different. Convince yourself that the filter of the length 4, when convolved, will just produce 2 outputs.\n",
    "\n",
    "Like multiple filter lengths, there can be multiple filters of the same length. So there can be a 100 filters of length 2, a hundred filters of length 4 and so on.\n",
    "\n",
    "Each of these will then produce multiple outputs!\n",
    "\n",
    "<!-- <img src=\"conv/conv2.006.png\",width=400,height=400> -->\n",
    "\n",
    "![png](./assets/images/conv2.006.png)\n",
    "\n",
    "The final stage comprises of max pooling then concatenation and softmax regularization.\n",
    "\n",
    "<!-- <img src=\"conv/conv2.007.png\",width=300,height=300> -->\n",
    "\n",
    "![png](./assets/images/conv2.007.png)\n",
    "\n",
    "The entire process was very nicely illustrated by Zhang et al, in the paper \"A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional\n",
    "Neural Networks for Sentence Classification\", for words. The above is just replicating the same for characters and how it would appear at every step!\n",
    "\n",
    "<!-- <img src=\"conv/Zhang.png\",width=400,height=400> -->\n",
    "\n",
    "![png](./assets/images/Zhang.png)\n",
    "\n",
    "The task we are trying to accomplish here is to classify text. Specifically, the input to the convolutional network can be words or characters like we discussed before. Here, from the sequence of words, in a sentence or from the sequence of characters in a sentence we would want to classify the category of the sentence, like positive or negative and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Service was very prompt.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Would not go back.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The cashier had no care what so ever on what I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I tried the Cape Cod ravoli, chicken,with cran...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I was disgusted because I was pretty sure that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I was shocked because no signs indicate cash o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Highly recommended.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Waitress was a little slow in service.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This place is not worth your time, let alone V...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>did not like at all.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The Burrittos Blah!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The food, amazing.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Service is also cute.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I could care less... The interior is just beau...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>So they performed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>That's right....the red velvet cake.....ohhh t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>- They never brought a salad we asked for.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>This hole in the wall has great Mexican street...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Took an hour to get our food only 4 tables in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The worst was the salmon sashimi.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>I immediately said I wanted to talk to the man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>The ambiance isn't much better.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Unfortunately, it only set us up for disapppoi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>The food wasn't good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>Your servers suck, wait, correction, our serve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>What happened next was pretty....off putting.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>too bad cause I know it's family owned, I real...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Overpriced for what you are getting.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>I vomited in the bathroom mid lunch.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>I kept looking at the time and it had soon bec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>I have been to very few places to eat that und...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>We started with the tuna sashimi which was bro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Food was below average.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>It sure does beat the nachos at the movies but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>All in all, Ha Long Bay was a bit of a flop.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>The problem I have is that they charge $11.99 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>Shrimp- When I unwrapped it (I live only 1/2 a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>It lacked flavor, seemed undercooked, and dry.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>It really is impressive that the place hasn't ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>I would avoid this place if you are staying in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>The refried beans that came with my meal were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Spend your money and time some place else.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>A lady at the table next to us found a live gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>the presentation of the food was awful.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>I can't tell you how disappointed I was.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  label\n",
       "0                             Wow... Loved this place.      1\n",
       "1                                   Crust is not good.      0\n",
       "2            Not tasty and the texture was just nasty.      0\n",
       "3    Stopped by during the late May bank holiday of...      1\n",
       "4    The selection on the menu was great and so wer...      1\n",
       "5       Now I am getting angry and I want my damn pho.      0\n",
       "6                Honeslty it didn't taste THAT fresh.)      0\n",
       "7    The potatoes were like rubber and you could te...      0\n",
       "8                            The fries were great too.      1\n",
       "9                                       A great touch.      1\n",
       "10                            Service was very prompt.      1\n",
       "11                                  Would not go back.      0\n",
       "12   The cashier had no care what so ever on what I...      0\n",
       "13   I tried the Cape Cod ravoli, chicken,with cran...      1\n",
       "14   I was disgusted because I was pretty sure that...      0\n",
       "15   I was shocked because no signs indicate cash o...      0\n",
       "16                                 Highly recommended.      1\n",
       "17              Waitress was a little slow in service.      0\n",
       "18   This place is not worth your time, let alone V...      0\n",
       "19                                did not like at all.      0\n",
       "20                                 The Burrittos Blah!      0\n",
       "21                                  The food, amazing.      1\n",
       "22                               Service is also cute.      1\n",
       "23   I could care less... The interior is just beau...      1\n",
       "24                                  So they performed.      1\n",
       "25   That's right....the red velvet cake.....ohhh t...      1\n",
       "26          - They never brought a salad we asked for.      0\n",
       "27   This hole in the wall has great Mexican street...      1\n",
       "28   Took an hour to get our food only 4 tables in ...      0\n",
       "29                   The worst was the salmon sashimi.      0\n",
       "..                                                 ...    ...\n",
       "970  I immediately said I wanted to talk to the man...      0\n",
       "971                    The ambiance isn't much better.      0\n",
       "972  Unfortunately, it only set us up for disapppoi...      0\n",
       "973                              The food wasn't good.      0\n",
       "974  Your servers suck, wait, correction, our serve...      0\n",
       "975      What happened next was pretty....off putting.      0\n",
       "976  too bad cause I know it's family owned, I real...      0\n",
       "977               Overpriced for what you are getting.      0\n",
       "978               I vomited in the bathroom mid lunch.      0\n",
       "979  I kept looking at the time and it had soon bec...      0\n",
       "980  I have been to very few places to eat that und...      0\n",
       "981  We started with the tuna sashimi which was bro...      0\n",
       "982                            Food was below average.      0\n",
       "983  It sure does beat the nachos at the movies but...      0\n",
       "984       All in all, Ha Long Bay was a bit of a flop.      0\n",
       "985  The problem I have is that they charge $11.99 ...      0\n",
       "986  Shrimp- When I unwrapped it (I live only 1/2 a...      0\n",
       "987     It lacked flavor, seemed undercooked, and dry.      0\n",
       "988  It really is impressive that the place hasn't ...      0\n",
       "989  I would avoid this place if you are staying in...      0\n",
       "990  The refried beans that came with my meal were ...      0\n",
       "991         Spend your money and time some place else.      0\n",
       "992  A lady at the table next to us found a live gr...      0\n",
       "993            the presentation of the food was awful.      0\n",
       "994           I can't tell you how disappointed I was.      0\n",
       "995  I think food should have flavor and texture an...      0\n",
       "996                           Appetite instantly gone.      0\n",
       "997  Overall I was not impressed and would not go b...      0\n",
       "998  The whole experience was underwhelming, and I ...      0\n",
       "999  Then, as if I hadn't wasted enough of my life ...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "data = pd.read_csv(\"./yelp_labelled.txt\", delimiter='\\t', header=None, names=['review','label'], encoding='utf-8')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character based CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "for sentences, sentiment in zip(data.review, data.label):\n",
    "    sentences_cleaned = [sent.lower() for sent in sentences]\n",
    "    docs.append(sentences_cleaned)\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "len(docs), len(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 1024 \n",
    "nb_filter = 256\n",
    "dense_outputs = 1024\n",
    "filter_kernels = [7, 7, 3, 3, 3, 3]\n",
    "n_out = 2\n",
    "batch_size = 80\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''\n",
    "for doc in docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = set(txt)\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def vectorize_sentences(data, char_indices):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [char_indices[w] for w in sentences]\n",
    "        x2 = np.eye(len(char_indices))[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "train_data = vectorize_sentences(docs,char_indices)\n",
    "train_data.shape\n",
    "y_train = to_categorical(sentiments)\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')\n",
    "\n",
    "conv = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[0],\n",
    "                     border_mode='valid', activation='relu',\n",
    "                     input_shape=(maxlen, vocab_size))(inputs)\n",
    "conv = MaxPooling1D(pool_length=3)(conv)\n",
    "\n",
    "conv1 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[1],\n",
    "                      border_mode='valid', activation='relu')(conv)\n",
    "conv1 = MaxPooling1D(pool_length=3)(conv1)\n",
    "\n",
    "conv2 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[2],\n",
    "                      border_mode='valid', activation='relu')(conv1)\n",
    "\n",
    "conv3 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[3],\n",
    "                      border_mode='valid', activation='relu')(conv2)\n",
    "\n",
    "conv4 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[4],\n",
    "                      border_mode='valid', activation='relu')(conv3)\n",
    "\n",
    "conv5 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[5],\n",
    "                      border_mode='valid', activation='relu')(conv4)\n",
    "conv5 = MaxPooling1D(pool_length=3)(conv5)\n",
    "conv5 = Flatten()(conv5)\n",
    "\n",
    "z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv5))\n",
    "z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))\n",
    "\n",
    "pred = Dense(n_out, activation='softmax', name='output')(z)\n",
    "\n",
    "model = Model(input=inputs, output=pred)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(train_data, y_train, batch_size=32,\n",
    "#            nb_epoch=120, validation_split=0.2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import model_to_dot\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(model_to_dot(model, show_shapes=True).create(prog='dot', format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word based CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part, we will use convolutions on words.\n",
    "\n",
    "Now the input data needs to be in the form:\n",
    "\n",
    "Number_of_reviews X maxlen\n",
    "\n",
    "Now the yelp data is really small, for these to be very effective. But hopefully, this will be sufficient for you to do experiments with your own data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./yelp_labelled.txt\",\n",
    "                   delimiter='\\t', header=None, names=['review','label'],encoding='utf-8')\n",
    "# z = list(nlp.pipe(data['review'], n_threads=20, batch_size=20000))\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from spacy.en import English\n",
    "from collections import Counter\n",
    "\n",
    "raw_text = 'Hello, world. Here are two sentences.'\n",
    "nlp = English()\n",
    "\n",
    "def tokenizeSentences(sent):\n",
    "    doc = nlp(sent)\n",
    "    sentences = [sent.string.strip() for sent in doc]\n",
    "    return sentences\n",
    "\n",
    "Xs = []    \n",
    "for texts in data['review']:\n",
    "    Xs.append(tokenizeSentences(texts))\n",
    "\n",
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(reduce(lambda x, y: x | y, (set(words) for words in Xs)))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, you can go ahead with this vocab, but ideally, we would want to get rid of the very infrequent words. So cases where the tokenizer failed, or the emoticons and so on. This is because if someone used the word 'amaaazzziiinggg' or 'Wooooooow' to describe the movie we do not want to create two different tokens for the word 'amazing'. Also, you can define custom rules in Spacy to take these into account. Secondly, another option that is often followed in information retrieval approaches is to get rid of the most frequent words. This is because words like 'the', 'an' and so on occur very frequently and do not add to the meaning. We will ignore this part of the yelp dataset since it is already really small but the following code snippet will help you get rid of the most frequent and the least frequent words in case you desire so.\n",
    "\n",
    "So let's build a function to get words that have at least appeared more than once in our vocab. The reason for doing this, instead of selecting the most frequent 500 words, is that there will be a lot of words that get eliminated arbitrarily as soon as we reach the 500-word limit. (A lot of words have very similar counts!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def word_freq(Xs, num):\n",
    "    all_words = [words.lower() for sentences in Xs for words in sentences]\n",
    "    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=operator.itemgetter(1))\n",
    "    final_vocab = [k for k,v in sorted_vocab if v>num]\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(final_vocab))\n",
    "    return final_vocab, word_idx\n",
    "\n",
    "final_vocab, word_idx = word_freq(Xs,2)\n",
    "vocab_len = len(final_vocab) # Finally we have 598 words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The vectorize function will vectorize the words we have. Now there are three possible scenarios that can occur. Because we didn't specifically lower case all the words:\n",
    "    the word is in the dictionary and we found it\n",
    "    the lower case word is in the dictionary and we found it\n",
    "    the word is not in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(data, word_idx, final_vocab, maxlen=40):\n",
    "    X = []\n",
    "    paddingIdx = len(final_vocab)+2\n",
    "    for sentences in data:\n",
    "        x=[]\n",
    "        for word in sentences:\n",
    "            if word in final_vocab:\n",
    "                x.append(word_idx[word])\n",
    "            elif word.lower() in final_vocab:\n",
    "                x.append(word_idx[word.lower()])\n",
    "            else:\n",
    "                x.append(paddingIdx)\n",
    "        X.append(x)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "train_data = vectorize_sentences(Xs, word_idx, final_vocab)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Flatten, Dropout, Dense, Merge\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
