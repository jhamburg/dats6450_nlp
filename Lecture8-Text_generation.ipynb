{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation using Project Gutenberg\n",
    "\n",
    "In this tutorial, we will download a story from Project Gutenberg and use it to feed a text generation model.\n",
    "\n",
    "About Project Gutenberg:\n",
    "\"Project Gutenberg offers over 56,000 free eBooks: Choose among free epub books, free kindle books, download them or read them online. You will find the world's great literature here, especially older works for which copyright has expired. We digitized and diligently proofread them with the help of thousands of volunteers.\"\n",
    "\n",
    "Navigate to https://www.gutenberg.org/ and pick a book with a plain text file of around 200 kb (Alice in Wonderland is a good option: https://www.gutenberg.org/ebooks/11). Download the text as the basis for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a Small LSTM Recurrent Neural Network\n",
    "In this section we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section we will use this model to generate new sequences of characters.\n",
    "\n",
    "Letâ€™s start off by importing the classes and functions we intend to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"data/dr_hyde_easy.txt\"\n",
    "raw_text = open(filename, encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text = re.sub(r'[^\\w\\s]',' ', raw_text)\n",
    "\n",
    "# Get rid of line breaks and change multiple space to single\n",
    "raw_text = re.sub(r'[\\n]',' ', raw_text)\n",
    "raw_text = re.sub(r' +', ' ', raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  151293\n",
      "Total Vocab:  37\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "In this tutorial we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "\n",
    "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  151193\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the characters between 0 and 1.\n",
    "\n",
    "The problem is really a single character classification problem with n character classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
    "\n",
    "We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.7911\n",
      "Epoch 00001: loss improved from inf to 2.79100, saving model to weights-improvement-01-2.7910.hdf5\n",
      "151193/151193 [==============================] - 215s 1ms/step - loss: 2.7910\n",
      "Epoch 2/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.6379\n",
      "Epoch 00002: loss improved from 2.79100 to 2.63793, saving model to weights-improvement-02-2.6379.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.6379\n",
      "Epoch 3/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.5624\n",
      "Epoch 00003: loss improved from 2.63793 to 2.56246, saving model to weights-improvement-03-2.5625.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.5625\n",
      "Epoch 4/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.5021\n",
      "Epoch 00004: loss improved from 2.56246 to 2.50216, saving model to weights-improvement-04-2.5022.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.5022\n",
      "Epoch 5/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.4444\n",
      "Epoch 00005: loss improved from 2.50216 to 2.44435, saving model to weights-improvement-05-2.4444.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.4444\n",
      "Epoch 6/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.3940\n",
      "Epoch 00006: loss improved from 2.44435 to 2.39395, saving model to weights-improvement-06-2.3940.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.3940\n",
      "Epoch 7/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.3495\n",
      "Epoch 00007: loss improved from 2.39395 to 2.34958, saving model to weights-improvement-07-2.3496.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.3496\n",
      "Epoch 8/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.3072\n",
      "Epoch 00008: loss improved from 2.34958 to 2.30721, saving model to weights-improvement-08-2.3072.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.3072\n",
      "Epoch 9/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.2684\n",
      "Epoch 00009: loss improved from 2.30721 to 2.26834, saving model to weights-improvement-09-2.2683.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.2683\n",
      "Epoch 10/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.2305\n",
      "Epoch 00010: loss improved from 2.26834 to 2.23054, saving model to weights-improvement-10-2.2305.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.2305\n",
      "Epoch 11/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.1963\n",
      "Epoch 00011: loss improved from 2.23054 to 2.19625, saving model to weights-improvement-11-2.1962.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.1962\n",
      "Epoch 12/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.1608\n",
      "Epoch 00012: loss improved from 2.19625 to 2.16081, saving model to weights-improvement-12-2.1608.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.1608\n",
      "Epoch 13/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.1303\n",
      "Epoch 00013: loss improved from 2.16081 to 2.13031, saving model to weights-improvement-13-2.1303.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.1303\n",
      "Epoch 14/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0996\n",
      "Epoch 00014: loss improved from 2.13031 to 2.09956, saving model to weights-improvement-14-2.0996.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.0996\n",
      "Epoch 15/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0698\n",
      "Epoch 00015: loss improved from 2.09956 to 2.06980, saving model to weights-improvement-15-2.0698.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.0698\n",
      "Epoch 16/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0398\n",
      "Epoch 00016: loss improved from 2.06980 to 2.03976, saving model to weights-improvement-16-2.0398.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.0398\n",
      "Epoch 17/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0123\n",
      "Epoch 00017: loss improved from 2.03976 to 2.01234, saving model to weights-improvement-17-2.0123.hdf5\n",
      "151193/151193 [==============================] - 211s 1ms/step - loss: 2.0123\n",
      "Epoch 18/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.9854\n",
      "Epoch 00018: loss improved from 2.01234 to 1.98540, saving model to weights-improvement-18-1.9854.hdf5\n",
      "151193/151193 [==============================] - 209s 1ms/step - loss: 1.9854\n",
      "Epoch 19/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.9612\n",
      "Epoch 00019: loss improved from 1.98540 to 1.96114, saving model to weights-improvement-19-1.9611.hdf5\n",
      "151193/151193 [==============================] - 216s 1ms/step - loss: 1.9611\n",
      "Epoch 20/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.9344\n",
      "Epoch 00020: loss improved from 1.96114 to 1.93435, saving model to weights-improvement-20-1.9344.hdf5\n",
      "151193/151193 [==============================] - 216s 1ms/step - loss: 1.9344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa89163668>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the example, you should have a number of weight checkpoint files in the local directory.\n",
    "\n",
    "You can delete them all except the one with the smallest loss value, eg `weights-improvement-19-1.9435.hdf5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text with an LSTM Network\n",
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-1.9344.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ion and must flee before daylight from a house that was no longer mine and hurrying back to my cabin \"\n",
      "et io the mawe of the sare thi fane oo the fand of the saae whth a streng of toeer and the lawter sooe a creat dare oo the sereo of the fouse oh the saalen the lawter whsh a stidn and the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawy\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the results shows good language generation? What is done poorly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was able to create something like words in the beginning, however, afterwards it just picked up the same pattern over and over again.  One could assume that it did so because that pattern is common throughout the book or it isn't able to distinguish other patterns after those letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Try a bigger model\n",
    "\n",
    "We will keep the number of memory units the same at 256, but add a second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.6894\n",
      "Epoch 00001: loss improved from inf to 2.68936, saving model to weights-improvement-mod2-01-2.6894.hdf5\n",
      "151193/151193 [==============================] - 485s 3ms/step - loss: 2.6894\n",
      "Epoch 2/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.4266\n",
      "Epoch 00002: loss improved from 2.68936 to 2.42657, saving model to weights-improvement-mod2-02-2.4266.hdf5\n",
      "151193/151193 [==============================] - 494s 3ms/step - loss: 2.4266\n",
      "Epoch 3/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.2216\n",
      "Epoch 00003: loss improved from 2.42657 to 2.22155, saving model to weights-improvement-mod2-03-2.2215.hdf5\n",
      "151193/151193 [==============================] - 487s 3ms/step - loss: 2.2215\n",
      "Epoch 4/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0699\n",
      "Epoch 00004: loss improved from 2.22155 to 2.06995, saving model to weights-improvement-mod2-04-2.0699.hdf5\n",
      "151193/151193 [==============================] - 482s 3ms/step - loss: 2.0699\n",
      "Epoch 5/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.9652\n",
      "Epoch 00005: loss improved from 2.06995 to 1.96525, saving model to weights-improvement-mod2-05-1.9652.hdf5\n",
      "151193/151193 [==============================] - 484s 3ms/step - loss: 1.9652\n",
      "Epoch 6/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.8812\n",
      "Epoch 00006: loss improved from 1.96525 to 1.88113, saving model to weights-improvement-mod2-06-1.8811.hdf5\n",
      "151193/151193 [==============================] - 484s 3ms/step - loss: 1.8811\n",
      "Epoch 7/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.8125\n",
      "Epoch 00007: loss improved from 1.88113 to 1.81244, saving model to weights-improvement-mod2-07-1.8124.hdf5\n",
      "151193/151193 [==============================] - 493s 3ms/step - loss: 1.8124\n",
      "Epoch 8/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.7521\n",
      "Epoch 00008: loss improved from 1.81244 to 1.75216, saving model to weights-improvement-mod2-08-1.7522.hdf5\n",
      "151193/151193 [==============================] - 486s 3ms/step - loss: 1.7522\n",
      "Epoch 9/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.6995\n",
      "Epoch 00009: loss improved from 1.75216 to 1.69947, saving model to weights-improvement-mod2-09-1.6995.hdf5\n",
      "151193/151193 [==============================] - 487s 3ms/step - loss: 1.6995\n",
      "Epoch 10/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.6519\n",
      "Epoch 00010: loss improved from 1.69947 to 1.65197, saving model to weights-improvement-mod2-10-1.6520.hdf5\n",
      "151193/151193 [==============================] - 500s 3ms/step - loss: 1.6520\n",
      "Epoch 11/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.6046\n",
      "Epoch 00011: loss improved from 1.65197 to 1.60455, saving model to weights-improvement-mod2-11-1.6046.hdf5\n",
      "151193/151193 [==============================] - 468s 3ms/step - loss: 1.6046\n",
      "Epoch 12/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.5648\n",
      "Epoch 00012: loss improved from 1.60455 to 1.56476, saving model to weights-improvement-mod2-12-1.5648.hdf5\n",
      "151193/151193 [==============================] - 465s 3ms/step - loss: 1.5648\n",
      "Epoch 13/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.5239\n",
      "Epoch 00013: loss improved from 1.56476 to 1.52396, saving model to weights-improvement-mod2-13-1.5240.hdf5\n",
      "151193/151193 [==============================] - 460s 3ms/step - loss: 1.5240\n",
      "Epoch 14/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.4826\n",
      "Epoch 00014: loss improved from 1.52396 to 1.48261, saving model to weights-improvement-mod2-14-1.4826.hdf5\n",
      "151193/151193 [==============================] - 466s 3ms/step - loss: 1.4826\n",
      "Epoch 15/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00015: loss improved from 1.48261 to 1.44319, saving model to weights-improvement-mod2-15-1.4432.hdf5\n",
      "151193/151193 [==============================] - 462s 3ms/step - loss: 1.4432\n",
      "Epoch 16/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.4043\n",
      "Epoch 00016: loss improved from 1.44319 to 1.40436, saving model to weights-improvement-mod2-16-1.4044.hdf5\n",
      "151193/151193 [==============================] - 461s 3ms/step - loss: 1.4044\n",
      "Epoch 17/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.3651\n",
      "Epoch 00017: loss improved from 1.40436 to 1.36516, saving model to weights-improvement-mod2-17-1.3652.hdf5\n",
      "151193/151193 [==============================] - 460s 3ms/step - loss: 1.3652\n",
      "Epoch 18/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.3241\n",
      "Epoch 00018: loss improved from 1.36516 to 1.32418, saving model to weights-improvement-mod2-18-1.3242.hdf5\n",
      "151193/151193 [==============================] - 459s 3ms/step - loss: 1.3242\n",
      "Epoch 19/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.2851\n",
      "Epoch 00019: loss improved from 1.32418 to 1.28501, saving model to weights-improvement-mod2-19-1.2850.hdf5\n",
      "151193/151193 [==============================] - 461s 3ms/step - loss: 1.2850\n",
      "Epoch 20/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.2442\n",
      "Epoch 00020: loss improved from 1.28501 to 1.24420, saving model to weights-improvement-mod2-20-1.2442.hdf5\n",
      "151193/151193 [==============================] - 460s 3ms/step - loss: 1.2442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aba1f0b978>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), \n",
    "                return_sequences=True))\n",
    "model2.add(LSTM(256))\n",
    "model2.add(Dense(y.shape[1], activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-mod2-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model2.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-mod2-20-1.2442.hdf5\"\n",
    "model2.load_weights(filename)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" th which i listened to the civilities of my unhappy victim i declare at least before god no man mora \"\n",
      "ly plt reterted the lawyer i shought it was not tn me a changed renlsee to the project gutenberg tm trademark and distribution of the project gutenberg tm micense and distribution or any pther with the perains of the fyl tere of the fyn tere aut the ward of the lawy moment i was still and i cegind the servant metter i shank you sile that i cannot sead the lawyer i have have his friends were the strange cloter was not my hiad to my consenoent for my sesvrn i dould have teen and i have have eone to my heart that i cannot sead the lawyer i have have his friends were the strange cloter was not my hiad to my consenoent for my sesvrn i dould have teen and i have have eone to my heart that i cannot sead the lawyer i have have his friends were the strange cloter was not my hiad to my consenoent for my sesvrn i dould have teen and i have have eone to my heart that i cannot sead the lawyer i have have his friends were the strange cloter was not my hiad to my consenoent for my sesvrn i dould have\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model2.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, what about the results shows good language generation? What is done poorly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the model is able to better predict some words, such as **lawyer** and **friends**.  However, it seems to replace some letters such as **s** for **r** and **s** for **t** in words such as *sead* that should be *read* and *shank* instead of *thank*.  However, there still seems to be repitition in the model in what it predicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a word based model\n",
    "\n",
    "Instead of generating language character by character, let's build an word generating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  25927\n",
      "Total Vocab:  3931\n",
      "Total Patterns:  25907\n"
     ]
    }
   ],
   "source": [
    "word_text = raw_text.split(' ')\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "words = sorted(list(set(word_text)))\n",
    "word_to_int = dict((c, i) for i, c in enumerate(words))\n",
    "\n",
    "n_words = len(word_text)\n",
    "n_word_vocab = len(words)\n",
    "print(\"Total Characters: \", n_words)\n",
    "print(\"Total Vocab: \", n_word_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "word_seq_length = 20\n",
    "dataX_word = []\n",
    "dataY_word = []\n",
    "for i in range(0, n_words - word_seq_length, 1):\n",
    "    seq_in = word_text[i:i + word_seq_length]\n",
    "    seq_out = word_text[i + word_seq_length]\n",
    "    dataX_word.append([word_to_int[word] for word in seq_in])\n",
    "    dataY_word.append(word_to_int[seq_out])\n",
    "n_word_patterns = len(dataX_word)\n",
    "print(\"Total Patterns: \", n_word_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X_word = numpy.reshape(dataX_word, (n_word_patterns, word_seq_length, 1))\n",
    "# normalize\n",
    "X_word = X_word / float(n_word_vocab)\n",
    "# one hot encode the output variable\n",
    "y_word = np_utils.to_categorical(dataY_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.7273\n",
      "Epoch 00001: loss improved from inf to 6.72783, saving model to weights-improvement-wordmod-01-6.7278.hdf5\n",
      "25907/25907 [==============================] - 20s 758us/step - loss: 6.7278\n",
      "Epoch 2/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4670\n",
      "Epoch 00002: loss improved from 6.72783 to 6.46721, saving model to weights-improvement-wordmod-02-6.4672.hdf5\n",
      "25907/25907 [==============================] - 18s 709us/step - loss: 6.4672\n",
      "Epoch 3/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4352\n",
      "Epoch 00003: loss improved from 6.46721 to 6.43536, saving model to weights-improvement-wordmod-03-6.4354.hdf5\n",
      "25907/25907 [==============================] - 19s 730us/step - loss: 6.4354\n",
      "Epoch 4/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4309\n",
      "Epoch 00004: loss improved from 6.43536 to 6.43092, saving model to weights-improvement-wordmod-04-6.4309.hdf5\n",
      "25907/25907 [==============================] - 19s 728us/step - loss: 6.4309\n",
      "Epoch 5/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4275\n",
      "Epoch 00005: loss improved from 6.43092 to 6.42703, saving model to weights-improvement-wordmod-05-6.4270.hdf5\n",
      "25907/25907 [==============================] - 19s 732us/step - loss: 6.4270\n",
      "Epoch 6/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4286\n",
      "Epoch 00006: loss did not improve\n",
      "25907/25907 [==============================] - 19s 731us/step - loss: 6.4280\n",
      "Epoch 7/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4289\n",
      "Epoch 00007: loss did not improve\n",
      "25907/25907 [==============================] - 19s 730us/step - loss: 6.4284\n",
      "Epoch 8/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4316- ETA: 0s - loss: \n",
      "Epoch 00008: loss did not improve\n",
      "25907/25907 [==============================] - 19s 730us/step - loss: 6.4325\n",
      "Epoch 9/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4298\n",
      "Epoch 00009: loss did not improve\n",
      "25907/25907 [==============================] - 19s 726us/step - loss: 6.4296\n",
      "Epoch 10/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4245\n",
      "Epoch 00010: loss improved from 6.42703 to 6.42424, saving model to weights-improvement-wordmod-10-6.4242.hdf5\n",
      "25907/25907 [==============================] - 19s 731us/step - loss: 6.4242\n",
      "Epoch 11/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4245\n",
      "Epoch 00011: loss did not improve\n",
      "25907/25907 [==============================] - 19s 722us/step - loss: 6.4250\n",
      "Epoch 12/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.4118\n",
      "Epoch 00012: loss improved from 6.42424 to 6.41176, saving model to weights-improvement-wordmod-12-6.4118.hdf5\n",
      "25907/25907 [==============================] - 19s 733us/step - loss: 6.4118\n",
      "Epoch 13/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.3988\n",
      "Epoch 00013: loss improved from 6.41176 to 6.39843, saving model to weights-improvement-wordmod-13-6.3984.hdf5\n",
      "25907/25907 [==============================] - 19s 730us/step - loss: 6.3984\n",
      "Epoch 14/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.3870\n",
      "Epoch 00014: loss improved from 6.39843 to 6.38752, saving model to weights-improvement-wordmod-14-6.3875.hdf5\n",
      "25907/25907 [==============================] - 19s 740us/step - loss: 6.3875\n",
      "Epoch 15/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.3720\n",
      "Epoch 00015: loss improved from 6.38752 to 6.37195, saving model to weights-improvement-wordmod-15-6.3719.hdf5\n",
      "25907/25907 [==============================] - 19s 729us/step - loss: 6.3719\n",
      "Epoch 16/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.3584\n",
      "Epoch 00016: loss improved from 6.37195 to 6.35921, saving model to weights-improvement-wordmod-16-6.3592.hdf5\n",
      "25907/25907 [==============================] - 19s 732us/step - loss: 6.3592\n",
      "Epoch 17/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.3350\n",
      "Epoch 00017: loss improved from 6.35921 to 6.33635, saving model to weights-improvement-wordmod-17-6.3364.hdf5\n",
      "25907/25907 [==============================] - 19s 747us/step - loss: 6.3364\n",
      "Epoch 18/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.3113\n",
      "Epoch 00018: loss improved from 6.33635 to 6.31174, saving model to weights-improvement-wordmod-18-6.3117.hdf5\n",
      "25907/25907 [==============================] - 19s 739us/step - loss: 6.3117\n",
      "Epoch 19/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.2999\n",
      "Epoch 00019: loss improved from 6.31174 to 6.30048, saving model to weights-improvement-wordmod-19-6.3005.hdf5\n",
      "25907/25907 [==============================] - 19s 728us/step - loss: 6.3005\n",
      "Epoch 20/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.2847\n",
      "Epoch 00020: loss improved from 6.30048 to 6.28531, saving model to weights-improvement-wordmod-20-6.2853.hdf5\n",
      "25907/25907 [==============================] - 18s 706us/step - loss: 6.2853\n",
      "Epoch 21/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.2639\n",
      "Epoch 00021: loss improved from 6.28531 to 6.26291, saving model to weights-improvement-wordmod-21-6.2629.hdf5\n",
      "25907/25907 [==============================] - 19s 716us/step - loss: 6.2629\n",
      "Epoch 22/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.2456\n",
      "Epoch 00022: loss improved from 6.26291 to 6.24517, saving model to weights-improvement-wordmod-22-6.2452.hdf5\n",
      "25907/25907 [==============================] - 18s 713us/step - loss: 6.2452\n",
      "Epoch 23/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.2291\n",
      "Epoch 00023: loss improved from 6.24517 to 6.22869, saving model to weights-improvement-wordmod-23-6.2287.hdf5\n",
      "25907/25907 [==============================] - 18s 712us/step - loss: 6.2287\n",
      "Epoch 24/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.1992\n",
      "Epoch 00024: loss improved from 6.22869 to 6.19907, saving model to weights-improvement-wordmod-24-6.1991.hdf5\n",
      "25907/25907 [==============================] - 18s 713us/step - loss: 6.1991\n",
      "Epoch 25/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.1744\n",
      "Epoch 00025: loss improved from 6.19907 to 6.17536, saving model to weights-improvement-wordmod-25-6.1754.hdf5\n",
      "25907/25907 [==============================] - 18s 708us/step - loss: 6.1754\n",
      "Epoch 26/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.1540\n",
      "Epoch 00026: loss improved from 6.17536 to 6.15302, saving model to weights-improvement-wordmod-26-6.1530.hdf5\n",
      "25907/25907 [==============================] - 18s 713us/step - loss: 6.1530\n",
      "Epoch 27/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.1258\n",
      "Epoch 00027: loss improved from 6.15302 to 6.12606, saving model to weights-improvement-wordmod-27-6.1261.hdf5\n",
      "25907/25907 [==============================] - 18s 712us/step - loss: 6.1261\n",
      "Epoch 28/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.0962\n",
      "Epoch 00028: loss improved from 6.12606 to 6.09701, saving model to weights-improvement-wordmod-28-6.0970.hdf5\n",
      "25907/25907 [==============================] - 18s 712us/step - loss: 6.0970\n",
      "Epoch 29/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.0700\n",
      "Epoch 00029: loss improved from 6.09701 to 6.06887, saving model to weights-improvement-wordmod-29-6.0689.hdf5\n",
      "25907/25907 [==============================] - 18s 708us/step - loss: 6.0689\n",
      "Epoch 30/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.0438\n",
      "Epoch 00030: loss improved from 6.06887 to 6.04403, saving model to weights-improvement-wordmod-30-6.0440.hdf5\n",
      "25907/25907 [==============================] - 18s 713us/step - loss: 6.0440\n",
      "Epoch 31/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 6.0168\n",
      "Epoch 00031: loss improved from 6.04403 to 6.01632, saving model to weights-improvement-wordmod-31-6.0163.hdf5\n",
      "25907/25907 [==============================] - 19s 728us/step - loss: 6.0163\n",
      "Epoch 32/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.9849\n",
      "Epoch 00032: loss improved from 6.01632 to 5.98456, saving model to weights-improvement-wordmod-32-5.9846.hdf5\n",
      "25907/25907 [==============================] - 20s 762us/step - loss: 5.9846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.9531\n",
      "Epoch 00033: loss improved from 5.98456 to 5.95358, saving model to weights-improvement-wordmod-33-5.9536.hdf5\n",
      "25907/25907 [==============================] - 18s 708us/step - loss: 5.9536\n",
      "Epoch 34/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.9174\n",
      "Epoch 00034: loss improved from 5.95358 to 5.91670, saving model to weights-improvement-wordmod-34-5.9167.hdf5\n",
      "25907/25907 [==============================] - 19s 735us/step - loss: 5.9167\n",
      "Epoch 35/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.8784\n",
      "Epoch 00035: loss improved from 5.91670 to 5.87818, saving model to weights-improvement-wordmod-35-5.8782.hdf5\n",
      "25907/25907 [==============================] - 19s 725us/step - loss: 5.8782\n",
      "Epoch 36/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.8431\n",
      "Epoch 00036: loss improved from 5.87818 to 5.84298, saving model to weights-improvement-wordmod-36-5.8430.hdf5\n",
      "25907/25907 [==============================] - 19s 728us/step - loss: 5.8430\n",
      "Epoch 37/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.8076\n",
      "Epoch 00037: loss improved from 5.84298 to 5.80794, saving model to weights-improvement-wordmod-37-5.8079.hdf5\n",
      "25907/25907 [==============================] - 19s 734us/step - loss: 5.8079\n",
      "Epoch 38/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.7664\n",
      "Epoch 00038: loss improved from 5.80794 to 5.76665, saving model to weights-improvement-wordmod-38-5.7666.hdf5\n",
      "25907/25907 [==============================] - 19s 729us/step - loss: 5.7666\n",
      "Epoch 39/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.7104\n",
      "Epoch 00039: loss improved from 5.76665 to 5.71070, saving model to weights-improvement-wordmod-39-5.7107.hdf5\n",
      "25907/25907 [==============================] - 19s 727us/step - loss: 5.7107\n",
      "Epoch 40/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.6615\n",
      "Epoch 00040: loss improved from 5.71070 to 5.66172, saving model to weights-improvement-wordmod-40-5.6617.hdf5\n",
      "25907/25907 [==============================] - 19s 729us/step - loss: 5.6617\n",
      "Epoch 41/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.6021\n",
      "Epoch 00041: loss improved from 5.66172 to 5.60239, saving model to weights-improvement-wordmod-41-5.6024.hdf5\n",
      "25907/25907 [==============================] - 19s 727us/step - loss: 5.6024\n",
      "Epoch 42/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.5439\n",
      "Epoch 00042: loss improved from 5.60239 to 5.54384, saving model to weights-improvement-wordmod-42-5.5438.hdf5\n",
      "25907/25907 [==============================] - 19s 735us/step - loss: 5.5438\n",
      "Epoch 43/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.4885\n",
      "Epoch 00043: loss improved from 5.54384 to 5.48842, saving model to weights-improvement-wordmod-43-5.4884.hdf5\n",
      "25907/25907 [==============================] - 19s 729us/step - loss: 5.4884\n",
      "Epoch 44/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.4203\n",
      "Epoch 00044: loss improved from 5.48842 to 5.41995, saving model to weights-improvement-wordmod-44-5.4199.hdf5\n",
      "25907/25907 [==============================] - 19s 730us/step - loss: 5.4199\n",
      "Epoch 45/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.3578\n",
      "Epoch 00045: loss improved from 5.41995 to 5.35864, saving model to weights-improvement-wordmod-45-5.3586.hdf5\n",
      "25907/25907 [==============================] - 19s 734us/step - loss: 5.3586\n",
      "Epoch 46/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.3008\n",
      "Epoch 00046: loss improved from 5.35864 to 5.30140, saving model to weights-improvement-wordmod-46-5.3014.hdf5\n",
      "25907/25907 [==============================] - 19s 729us/step - loss: 5.3014\n",
      "Epoch 47/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.2366\n",
      "Epoch 00047: loss improved from 5.30140 to 5.23752, saving model to weights-improvement-wordmod-47-5.2375.hdf5\n",
      "25907/25907 [==============================] - 19s 726us/step - loss: 5.2375\n",
      "Epoch 48/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.1846\n",
      "Epoch 00048: loss improved from 5.23752 to 5.18442, saving model to weights-improvement-wordmod-48-5.1844.hdf5\n",
      "25907/25907 [==============================] - 19s 721us/step - loss: 5.1844\n",
      "Epoch 49/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.1249\n",
      "Epoch 00049: loss improved from 5.18442 to 5.12484, saving model to weights-improvement-wordmod-49-5.1248.hdf5\n",
      "25907/25907 [==============================] - 19s 736us/step - loss: 5.1248\n",
      "Epoch 50/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.0550\n",
      "Epoch 00050: loss improved from 5.12484 to 5.05548, saving model to weights-improvement-wordmod-50-5.0555.hdf5\n",
      "25907/25907 [==============================] - 19s 726us/step - loss: 5.0555\n",
      "Epoch 51/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 5.0095\n",
      "Epoch 00051: loss improved from 5.05548 to 5.00994, saving model to weights-improvement-wordmod-51-5.0099.hdf5\n",
      "25907/25907 [==============================] - 19s 728us/step - loss: 5.0099\n",
      "Epoch 52/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.9465\n",
      "Epoch 00052: loss improved from 5.00994 to 4.94632, saving model to weights-improvement-wordmod-52-4.9463.hdf5\n",
      "25907/25907 [==============================] - 19s 727us/step - loss: 4.9463\n",
      "Epoch 53/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.8873\n",
      "Epoch 00053: loss improved from 4.94632 to 4.88765, saving model to weights-improvement-wordmod-53-4.8876.hdf5\n",
      "25907/25907 [==============================] - 19s 731us/step - loss: 4.8876\n",
      "Epoch 54/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.8468\n",
      "Epoch 00054: loss improved from 4.88765 to 4.84777, saving model to weights-improvement-wordmod-54-4.8478.hdf5\n",
      "25907/25907 [==============================] - 19s 730us/step - loss: 4.8478\n",
      "Epoch 55/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.7866\n",
      "Epoch 00055: loss improved from 4.84777 to 4.78714, saving model to weights-improvement-wordmod-55-4.7871.hdf5\n",
      "25907/25907 [==============================] - 19s 727us/step - loss: 4.7871\n",
      "Epoch 56/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.7319\n",
      "Epoch 00056: loss improved from 4.78714 to 4.73259, saving model to weights-improvement-wordmod-56-4.7326.hdf5\n",
      "25907/25907 [==============================] - 19s 732us/step - loss: 4.7326\n",
      "Epoch 57/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.6868\n",
      "Epoch 00057: loss improved from 4.73259 to 4.68730, saving model to weights-improvement-wordmod-57-4.6873.hdf5\n",
      "25907/25907 [==============================] - 19s 728us/step - loss: 4.6873\n",
      "Epoch 58/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.6382\n",
      "Epoch 00058: loss improved from 4.68730 to 4.63847, saving model to weights-improvement-wordmod-58-4.6385.hdf5\n",
      "25907/25907 [==============================] - 19s 726us/step - loss: 4.6385\n",
      "Epoch 59/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.5912\n",
      "Epoch 00059: loss improved from 4.63847 to 4.59073, saving model to weights-improvement-wordmod-59-4.5907.hdf5\n",
      "25907/25907 [==============================] - 19s 736us/step - loss: 4.5907\n",
      "Epoch 60/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.5425\n",
      "Epoch 00060: loss improved from 4.59073 to 4.54346, saving model to weights-improvement-wordmod-60-4.5435.hdf5\n",
      "25907/25907 [==============================] - 19s 735us/step - loss: 4.5435\n",
      "Epoch 61/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.4911\n",
      "Epoch 00061: loss improved from 4.54346 to 4.49168, saving model to weights-improvement-wordmod-61-4.4917.hdf5\n",
      "25907/25907 [==============================] - 19s 737us/step - loss: 4.4917\n",
      "Epoch 62/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.4496\n",
      "Epoch 00062: loss improved from 4.49168 to 4.45003, saving model to weights-improvement-wordmod-62-4.4500.hdf5\n",
      "25907/25907 [==============================] - 19s 733us/step - loss: 4.4500\n",
      "Epoch 63/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.4070\n",
      "Epoch 00063: loss improved from 4.45003 to 4.40758, saving model to weights-improvement-wordmod-63-4.4076.hdf5\n",
      "25907/25907 [==============================] - 19s 729us/step - loss: 4.4076\n",
      "Epoch 64/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.3615\n",
      "Epoch 00064: loss improved from 4.40758 to 4.36134, saving model to weights-improvement-wordmod-64-4.3613.hdf5\n",
      "25907/25907 [==============================] - 19s 729us/step - loss: 4.3613\n",
      "Epoch 65/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.3258\n",
      "Epoch 00065: loss improved from 4.36134 to 4.32582, saving model to weights-improvement-wordmod-65-4.3258.hdf5\n",
      "25907/25907 [==============================] - 19s 741us/step - loss: 4.3258\n",
      "Epoch 66/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.2871\n",
      "Epoch 00066: loss improved from 4.32582 to 4.28678, saving model to weights-improvement-wordmod-66-4.2868.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 4.2868\n",
      "Epoch 67/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.2372\n",
      "Epoch 00067: loss improved from 4.28678 to 4.23732, saving model to weights-improvement-wordmod-67-4.2373.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 4.2373\n",
      "Epoch 68/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.2040\n",
      "Epoch 00068: loss improved from 4.23732 to 4.20478, saving model to weights-improvement-wordmod-68-4.2048.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 4.2048\n",
      "Epoch 69/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.1580\n",
      "Epoch 00069: loss improved from 4.20478 to 4.15823, saving model to weights-improvement-wordmod-69-4.1582.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 4.1582\n",
      "Epoch 70/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.1228\n",
      "Epoch 00070: loss improved from 4.15823 to 4.12292, saving model to weights-improvement-wordmod-70-4.1229.hdf5\n",
      "25907/25907 [==============================] - 18s 697us/step - loss: 4.1229\n",
      "Epoch 71/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.0871\n",
      "Epoch 00071: loss improved from 4.12292 to 4.08699, saving model to weights-improvement-wordmod-71-4.0870.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 4.0870\n",
      "Epoch 72/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.0503\n",
      "Epoch 00072: loss improved from 4.08699 to 4.05006, saving model to weights-improvement-wordmod-72-4.0501.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 4.0501\n",
      "Epoch 73/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 4.0134\n",
      "Epoch 00073: loss improved from 4.05006 to 4.01308, saving model to weights-improvement-wordmod-73-4.0131.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 4.0131\n",
      "Epoch 74/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.9841\n",
      "Epoch 00074: loss improved from 4.01308 to 3.98353, saving model to weights-improvement-wordmod-74-3.9835.hdf5\n",
      "25907/25907 [==============================] - 18s 698us/step - loss: 3.9835\n",
      "Epoch 75/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.9427\n",
      "Epoch 00075: loss improved from 3.98353 to 3.94359, saving model to weights-improvement-wordmod-75-3.9436.hdf5\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 3.9436\n",
      "Epoch 76/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.9117\n",
      "Epoch 00076: loss improved from 3.94359 to 3.91221, saving model to weights-improvement-wordmod-76-3.9122.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 3.9122\n",
      "Epoch 77/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.8805\n",
      "Epoch 00077: loss improved from 3.91221 to 3.88090, saving model to weights-improvement-wordmod-77-3.8809.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 3.8809\n",
      "Epoch 78/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.8411\n",
      "Epoch 00078: loss improved from 3.88090 to 3.84146, saving model to weights-improvement-wordmod-78-3.8415.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 3.8415\n",
      "Epoch 79/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.8101\n",
      "Epoch 00079: loss improved from 3.84146 to 3.81030, saving model to weights-improvement-wordmod-79-3.8103.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 3.8103\n",
      "Epoch 80/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.7890\n",
      "Epoch 00080: loss improved from 3.81030 to 3.78901, saving model to weights-improvement-wordmod-80-3.7890.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 3.7890\n",
      "Epoch 81/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.7542\n",
      "Epoch 00081: loss improved from 3.78901 to 3.75518, saving model to weights-improvement-wordmod-81-3.7552.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 3.7552\n",
      "Epoch 82/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.7215\n",
      "Epoch 00082: loss improved from 3.75518 to 3.72227, saving model to weights-improvement-wordmod-82-3.7223.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 3.7223\n",
      "Epoch 83/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.6944\n",
      "Epoch 00083: loss improved from 3.72227 to 3.69423, saving model to weights-improvement-wordmod-83-3.6942.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 3.6942\n",
      "Epoch 84/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.6725\n",
      "Epoch 00084: loss improved from 3.69423 to 3.67266, saving model to weights-improvement-wordmod-84-3.6727.hdf5\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 3.6727\n",
      "Epoch 85/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.6508\n",
      "Epoch 00085: loss improved from 3.67266 to 3.65153, saving model to weights-improvement-wordmod-85-3.6515.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 3.6515\n",
      "Epoch 86/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.6115\n",
      "Epoch 00086: loss improved from 3.65153 to 3.61164, saving model to weights-improvement-wordmod-86-3.6116.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 3.6116\n",
      "Epoch 87/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.5739\n",
      "Epoch 00087: loss improved from 3.61164 to 3.57403, saving model to weights-improvement-wordmod-87-3.5740.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 3.5740\n",
      "Epoch 88/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.5572\n",
      "Epoch 00088: loss improved from 3.57403 to 3.55701, saving model to weights-improvement-wordmod-88-3.5570.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 3.5570\n",
      "Epoch 89/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.5388\n",
      "Epoch 00089: loss improved from 3.55701 to 3.53879, saving model to weights-improvement-wordmod-89-3.5388.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 3.5388\n",
      "Epoch 90/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.5032\n",
      "Epoch 00090: loss improved from 3.53879 to 3.50389, saving model to weights-improvement-wordmod-90-3.5039.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 3.5039\n",
      "Epoch 91/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.4778\n",
      "Epoch 00091: loss improved from 3.50389 to 3.47828, saving model to weights-improvement-wordmod-91-3.4783.hdf5\n",
      "25907/25907 [==============================] - 18s 697us/step - loss: 3.4783\n",
      "Epoch 92/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.4569\n",
      "Epoch 00092: loss improved from 3.47828 to 3.45679, saving model to weights-improvement-wordmod-92-3.4568.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 3.4568\n",
      "Epoch 93/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.4330\n",
      "Epoch 00093: loss improved from 3.45679 to 3.43308, saving model to weights-improvement-wordmod-93-3.4331.hdf5\n",
      "25907/25907 [==============================] - 18s 698us/step - loss: 3.4331\n",
      "Epoch 94/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.4099\n",
      "Epoch 00094: loss improved from 3.43308 to 3.41009, saving model to weights-improvement-wordmod-94-3.4101.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 3.4101\n",
      "Epoch 95/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.3993\n",
      "Epoch 00095: loss improved from 3.41009 to 3.39874, saving model to weights-improvement-wordmod-95-3.3987.hdf5\n",
      "25907/25907 [==============================] - 18s 697us/step - loss: 3.3987\n",
      "Epoch 96/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.3657\n",
      "Epoch 00096: loss improved from 3.39874 to 3.36534, saving model to weights-improvement-wordmod-96-3.3653.hdf5\n",
      "25907/25907 [==============================] - 18s 697us/step - loss: 3.3653\n",
      "Epoch 97/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.3462\n",
      "Epoch 00097: loss improved from 3.36534 to 3.34618, saving model to weights-improvement-wordmod-97-3.3462.hdf5\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 3.3462\n",
      "Epoch 98/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.3349\n",
      "Epoch 00098: loss improved from 3.34618 to 3.33483, saving model to weights-improvement-wordmod-98-3.3348.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 3.3348\n",
      "Epoch 99/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.2970\n",
      "Epoch 00099: loss improved from 3.33483 to 3.29688, saving model to weights-improvement-wordmod-99-3.2969.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 3.2969\n",
      "Epoch 100/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.2805\n",
      "Epoch 00100: loss improved from 3.29688 to 3.28033, saving model to weights-improvement-wordmod-100-3.2803.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 3.2803\n",
      "Epoch 101/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.2667\n",
      "Epoch 00101: loss improved from 3.28033 to 3.26717, saving model to weights-improvement-wordmod-101-3.2672.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 3.2672\n",
      "Epoch 102/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.2555\n",
      "Epoch 00102: loss improved from 3.26717 to 3.25605, saving model to weights-improvement-wordmod-102-3.2561.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 3.2561\n",
      "Epoch 103/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.2228\n",
      "Epoch 00103: loss improved from 3.25605 to 3.22346, saving model to weights-improvement-wordmod-103-3.2235.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 3.2235\n",
      "Epoch 104/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.2003\n",
      "Epoch 00104: loss improved from 3.22346 to 3.20053, saving model to weights-improvement-wordmod-104-3.2005.hdf5\n",
      "25907/25907 [==============================] - 18s 703us/step - loss: 3.2005\n",
      "Epoch 105/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.1866\n",
      "Epoch 00105: loss improved from 3.20053 to 3.18661, saving model to weights-improvement-wordmod-105-3.1866.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 3.1866\n",
      "Epoch 106/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.1683\n",
      "Epoch 00106: loss improved from 3.18661 to 3.16828, saving model to weights-improvement-wordmod-106-3.1683.hdf5\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 3.1683\n",
      "Epoch 107/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.1554\n",
      "Epoch 00107: loss improved from 3.16828 to 3.15566, saving model to weights-improvement-wordmod-107-3.1557.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 3.1557\n",
      "Epoch 108/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.1331\n",
      "Epoch 00108: loss improved from 3.15566 to 3.13329, saving model to weights-improvement-wordmod-108-3.1333.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 3.1333\n",
      "Epoch 109/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.1235\n",
      "Epoch 00109: loss improved from 3.13329 to 3.12411, saving model to weights-improvement-wordmod-109-3.1241.hdf5\n",
      "25907/25907 [==============================] - 18s 696us/step - loss: 3.1241\n",
      "Epoch 110/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.1073\n",
      "Epoch 00110: loss improved from 3.12411 to 3.10738, saving model to weights-improvement-wordmod-110-3.1074.hdf5\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 3.1074\n",
      "Epoch 111/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.0920\n",
      "Epoch 00111: loss improved from 3.10738 to 3.09275, saving model to weights-improvement-wordmod-111-3.0927.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 3.0927\n",
      "Epoch 112/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.0666\n",
      "Epoch 00112: loss improved from 3.09275 to 3.06642, saving model to weights-improvement-wordmod-112-3.0664.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 3.0664\n",
      "Epoch 113/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.0517\n",
      "Epoch 00113: loss improved from 3.06642 to 3.05130, saving model to weights-improvement-wordmod-113-3.0513.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 3.0513\n",
      "Epoch 114/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.0400\n",
      "Epoch 00114: loss improved from 3.05130 to 3.04038, saving model to weights-improvement-wordmod-114-3.0404.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 3.0404\n",
      "Epoch 115/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.0476\n",
      "Epoch 00115: loss did not improve\n",
      "25907/25907 [==============================] - 18s 703us/step - loss: 3.0483\n",
      "Epoch 116/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 3.0114\n",
      "Epoch 00116: loss improved from 3.04038 to 3.01173, saving model to weights-improvement-wordmod-116-3.0117.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 3.0117\n",
      "Epoch 117/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.9846\n",
      "Epoch 00117: loss improved from 3.01173 to 2.98505, saving model to weights-improvement-wordmod-117-2.9850.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.9850\n",
      "Epoch 118/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.9746\n",
      "Epoch 00118: loss improved from 2.98505 to 2.97520, saving model to weights-improvement-wordmod-118-2.9752.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.9752\n",
      "Epoch 119/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.9642\n",
      "Epoch 00119: loss improved from 2.97520 to 2.96472, saving model to weights-improvement-wordmod-119-2.9647.hdf5\n",
      "25907/25907 [==============================] - 18s 682us/step - loss: 2.9647\n",
      "Epoch 120/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.9502\n",
      "Epoch 00120: loss improved from 2.96472 to 2.94965, saving model to weights-improvement-wordmod-120-2.9497.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.9497\n",
      "Epoch 121/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.9828\n",
      "Epoch 00121: loss did not improve\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.9830\n",
      "Epoch 122/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.9357\n",
      "Epoch 00122: loss improved from 2.94965 to 2.93549, saving model to weights-improvement-wordmod-122-2.9355.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.9355\n",
      "Epoch 123/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.9051\n",
      "Epoch 00123: loss improved from 2.93549 to 2.90543, saving model to weights-improvement-wordmod-123-2.9054.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.9054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.8908\n",
      "Epoch 00124: loss improved from 2.90543 to 2.89138, saving model to weights-improvement-wordmod-124-2.8914.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.8914\n",
      "Epoch 125/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.8752\n",
      "Epoch 00125: loss improved from 2.89138 to 2.87512, saving model to weights-improvement-wordmod-125-2.8751.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.8751\n",
      "Epoch 126/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.8664\n",
      "Epoch 00126: loss improved from 2.87512 to 2.86661, saving model to weights-improvement-wordmod-126-2.8666.hdf5\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 2.8666\n",
      "Epoch 127/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.8530\n",
      "Epoch 00127: loss improved from 2.86661 to 2.85341, saving model to weights-improvement-wordmod-127-2.8534.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.8534\n",
      "Epoch 128/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.8447\n",
      "Epoch 00128: loss improved from 2.85341 to 2.84394, saving model to weights-improvement-wordmod-128-2.8439.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.8439\n",
      "Epoch 129/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.8229\n",
      "Epoch 00129: loss improved from 2.84394 to 2.82235, saving model to weights-improvement-wordmod-129-2.8223.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.8223\n",
      "Epoch 130/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.8162\n",
      "Epoch 00130: loss improved from 2.82235 to 2.81611, saving model to weights-improvement-wordmod-130-2.8161.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.8161\n",
      "Epoch 131/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.8010\n",
      "Epoch 00131: loss improved from 2.81611 to 2.80115, saving model to weights-improvement-wordmod-131-2.8011.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.8011\n",
      "Epoch 132/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.7976\n",
      "Epoch 00132: loss improved from 2.80115 to 2.79738, saving model to weights-improvement-wordmod-132-2.7974.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.7974\n",
      "Epoch 133/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.7804\n",
      "Epoch 00133: loss improved from 2.79738 to 2.78013, saving model to weights-improvement-wordmod-133-2.7801.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.7801\n",
      "Epoch 134/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.7834\n",
      "Epoch 00134: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 2.7831\n",
      "Epoch 135/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.7606\n",
      "Epoch 00135: loss improved from 2.78013 to 2.76114, saving model to weights-improvement-wordmod-135-2.7611.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.7611\n",
      "Epoch 136/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.7457\n",
      "Epoch 00136: loss improved from 2.76114 to 2.74490, saving model to weights-improvement-wordmod-136-2.7449.hdf5\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 2.7449\n",
      "Epoch 137/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.7299\n",
      "Epoch 00137: loss improved from 2.74490 to 2.73071, saving model to weights-improvement-wordmod-137-2.7307.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.7307\n",
      "Epoch 138/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.7378\n",
      "Epoch 00138: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.7380\n",
      "Epoch 139/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.7145\n",
      "Epoch 00139: loss improved from 2.73071 to 2.71541, saving model to weights-improvement-wordmod-139-2.7154.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.7154\n",
      "Epoch 140/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6994\n",
      "Epoch 00140: loss improved from 2.71541 to 2.69957, saving model to weights-improvement-wordmod-140-2.6996.hdf5\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.6996\n",
      "Epoch 141/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6863\n",
      "Epoch 00141: loss improved from 2.69957 to 2.68704, saving model to weights-improvement-wordmod-141-2.6870.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.6870\n",
      "Epoch 142/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6842\n",
      "Epoch 00142: loss improved from 2.68704 to 2.68427, saving model to weights-improvement-wordmod-142-2.6843.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.6843\n",
      "Epoch 143/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6888\n",
      "Epoch 00143: loss did not improve\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.6895\n",
      "Epoch 144/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6650\n",
      "Epoch 00144: loss improved from 2.68427 to 2.66510, saving model to weights-improvement-wordmod-144-2.6651.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.6651\n",
      "Epoch 145/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6535\n",
      "Epoch 00145: loss improved from 2.66510 to 2.65388, saving model to weights-improvement-wordmod-145-2.6539.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.6539\n",
      "Epoch 146/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6376\n",
      "Epoch 00146: loss improved from 2.65388 to 2.63696, saving model to weights-improvement-wordmod-146-2.6370.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.6370\n",
      "Epoch 147/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6373\n",
      "Epoch 00147: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.6373\n",
      "Epoch 148/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6272\n",
      "Epoch 00148: loss improved from 2.63696 to 2.62717, saving model to weights-improvement-wordmod-148-2.6272.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.6272\n",
      "Epoch 149/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6172\n",
      "Epoch 00149: loss improved from 2.62717 to 2.61805, saving model to weights-improvement-wordmod-149-2.6180.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.6180\n",
      "Epoch 150/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6050\n",
      "Epoch 00150: loss improved from 2.61805 to 2.60511, saving model to weights-improvement-wordmod-150-2.6051.hdf5\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 2.6051\n",
      "Epoch 151/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.6134\n",
      "Epoch 00151: loss did not improve\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.6131\n",
      "Epoch 152/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5807\n",
      "Epoch 00152: loss improved from 2.60511 to 2.58077, saving model to weights-improvement-wordmod-152-2.5808.hdf5\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 2.5808\n",
      "Epoch 153/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5839\n",
      "Epoch 00153: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.5846\n",
      "Epoch 154/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5655\n",
      "Epoch 00154: loss improved from 2.58077 to 2.56476, saving model to weights-improvement-wordmod-154-2.5648.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.5648\n",
      "Epoch 155/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5645\n",
      "Epoch 00155: loss improved from 2.56476 to 2.56466, saving model to weights-improvement-wordmod-155-2.5647.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.5647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5452\n",
      "Epoch 00156: loss improved from 2.56466 to 2.54499, saving model to weights-improvement-wordmod-156-2.5450.hdf5\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.5450\n",
      "Epoch 157/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5370\n",
      "Epoch 00157: loss improved from 2.54499 to 2.53715, saving model to weights-improvement-wordmod-157-2.5371.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.5371\n",
      "Epoch 158/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5269\n",
      "Epoch 00158: loss improved from 2.53715 to 2.52702, saving model to weights-improvement-wordmod-158-2.5270.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.5270\n",
      "Epoch 159/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5403\n",
      "Epoch 00159: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.5406\n",
      "Epoch 160/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5210\n",
      "Epoch 00160: loss improved from 2.52702 to 2.52137, saving model to weights-improvement-wordmod-160-2.5214.hdf5\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.5214\n",
      "Epoch 161/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5164\n",
      "Epoch 00161: loss improved from 2.52137 to 2.51616, saving model to weights-improvement-wordmod-161-2.5162.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.5162\n",
      "Epoch 162/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5098\n",
      "Epoch 00162: loss improved from 2.51616 to 2.50998, saving model to weights-improvement-wordmod-162-2.5100.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.5100\n",
      "Epoch 163/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4989\n",
      "Epoch 00163: loss improved from 2.50998 to 2.49962, saving model to weights-improvement-wordmod-163-2.4996.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.4996\n",
      "Epoch 164/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.5021\n",
      "Epoch 00164: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.5018\n",
      "Epoch 165/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4946\n",
      "Epoch 00165: loss improved from 2.49962 to 2.49456, saving model to weights-improvement-wordmod-165-2.4946.hdf5\n",
      "25907/25907 [==============================] - 18s 703us/step - loss: 2.4946\n",
      "Epoch 166/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4607\n",
      "Epoch 00166: loss improved from 2.49456 to 2.46231, saving model to weights-improvement-wordmod-166-2.4623.hdf5\n",
      "25907/25907 [==============================] - 18s 697us/step - loss: 2.4623\n",
      "Epoch 167/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4622\n",
      "Epoch 00167: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.4636\n",
      "Epoch 168/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4627\n",
      "Epoch 00168: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 2.4631\n",
      "Epoch 169/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4463\n",
      "Epoch 00169: loss improved from 2.46231 to 2.44677, saving model to weights-improvement-wordmod-169-2.4468.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.4468\n",
      "Epoch 170/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4379\n",
      "Epoch 00170: loss improved from 2.44677 to 2.43862, saving model to weights-improvement-wordmod-170-2.4386.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.4386\n",
      "Epoch 171/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4530\n",
      "Epoch 00171: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.4535\n",
      "Epoch 172/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4347\n",
      "Epoch 00172: loss improved from 2.43862 to 2.43412, saving model to weights-improvement-wordmod-172-2.4341.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.4341\n",
      "Epoch 173/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4284\n",
      "Epoch 00173: loss improved from 2.43412 to 2.42808, saving model to weights-improvement-wordmod-173-2.4281.hdf5\n",
      "25907/25907 [==============================] - 18s 701us/step - loss: 2.4281\n",
      "Epoch 174/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4168\n",
      "Epoch 00174: loss improved from 2.42808 to 2.41727, saving model to weights-improvement-wordmod-174-2.4173.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.4173\n",
      "Epoch 175/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4070\n",
      "Epoch 00175: loss improved from 2.41727 to 2.40655, saving model to weights-improvement-wordmod-175-2.4066.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.4066\n",
      "Epoch 176/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4124\n",
      "Epoch 00176: loss did not improve\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.4129\n",
      "Epoch 177/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.4162\n",
      "Epoch 00177: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.4161\n",
      "Epoch 178/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3760\n",
      "Epoch 00178: loss improved from 2.40655 to 2.37630, saving model to weights-improvement-wordmod-178-2.3763.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.3763\n",
      "Epoch 179/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3707\n",
      "Epoch 00179: loss improved from 2.37630 to 2.37091, saving model to weights-improvement-wordmod-179-2.3709.hdf5\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 2.3709\n",
      "Epoch 180/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3550\n",
      "Epoch 00180: loss improved from 2.37091 to 2.35575, saving model to weights-improvement-wordmod-180-2.3557.hdf5\n",
      "25907/25907 [==============================] - 18s 701us/step - loss: 2.3557\n",
      "Epoch 181/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3716\n",
      "Epoch 00181: loss did not improve\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.3713\n",
      "Epoch 182/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3873\n",
      "Epoch 00182: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.3880\n",
      "Epoch 183/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3688\n",
      "Epoch 00183: loss did not improve\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.3683\n",
      "Epoch 184/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3528\n",
      "Epoch 00184: loss improved from 2.35575 to 2.35307, saving model to weights-improvement-wordmod-184-2.3531.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.3531\n",
      "Epoch 185/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3465\n",
      "Epoch 00185: loss improved from 2.35307 to 2.34606, saving model to weights-improvement-wordmod-185-2.3461.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.3461\n",
      "Epoch 186/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3409\n",
      "Epoch 00186: loss improved from 2.34606 to 2.34152, saving model to weights-improvement-wordmod-186-2.3415.hdf5\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 2.3415\n",
      "Epoch 187/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3258\n",
      "Epoch 00187: loss improved from 2.34152 to 2.32565, saving model to weights-improvement-wordmod-187-2.3257.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.3257\n",
      "Epoch 188/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3309\n",
      "Epoch 00188: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.3303\n",
      "Epoch 189/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3200\n",
      "Epoch 00189: loss improved from 2.32565 to 2.32061, saving model to weights-improvement-wordmod-189-2.3206.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.3206\n",
      "Epoch 190/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3191\n",
      "Epoch 00190: loss improved from 2.32061 to 2.31936, saving model to weights-improvement-wordmod-190-2.3194.hdf5\n",
      "25907/25907 [==============================] - 18s 678us/step - loss: 2.3194\n",
      "Epoch 191/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2967\n",
      "Epoch 00191: loss improved from 2.31936 to 2.29681, saving model to weights-improvement-wordmod-191-2.2968.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.2968\n",
      "Epoch 192/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3113\n",
      "Epoch 00192: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.3114\n",
      "Epoch 193/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.3011\n",
      "Epoch 00193: loss did not improve\n",
      "25907/25907 [==============================] - 18s 682us/step - loss: 2.3009\n",
      "Epoch 194/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2931\n",
      "Epoch 00194: loss improved from 2.29681 to 2.29330, saving model to weights-improvement-wordmod-194-2.2933.hdf5\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 2.2933\n",
      "Epoch 195/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2971\n",
      "Epoch 00195: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.2965\n",
      "Epoch 196/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2720\n",
      "Epoch 00196: loss improved from 2.29330 to 2.27220, saving model to weights-improvement-wordmod-196-2.2722.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.2722\n",
      "Epoch 197/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2725\n",
      "Epoch 00197: loss did not improve\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.2730\n",
      "Epoch 198/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2908\n",
      "Epoch 00198: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.2915\n",
      "Epoch 199/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2583\n",
      "Epoch 00199: loss improved from 2.27220 to 2.25896, saving model to weights-improvement-wordmod-199-2.2590.hdf5\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 2.2590\n",
      "Epoch 200/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2757\n",
      "Epoch 00200: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.2753\n",
      "Epoch 201/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2537\n",
      "Epoch 00201: loss improved from 2.25896 to 2.25360, saving model to weights-improvement-wordmod-201-2.2536.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.2536\n",
      "Epoch 202/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2440\n",
      "Epoch 00202: loss improved from 2.25360 to 2.24445, saving model to weights-improvement-wordmod-202-2.2445.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.2445\n",
      "Epoch 203/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2457\n",
      "Epoch 00203: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.2457\n",
      "Epoch 204/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2618\n",
      "Epoch 00204: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.2610\n",
      "Epoch 205/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2383\n",
      "Epoch 00205: loss improved from 2.24445 to 2.23815, saving model to weights-improvement-wordmod-205-2.2381.hdf5\n",
      "25907/25907 [==============================] - 18s 677us/step - loss: 2.2381\n",
      "Epoch 206/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2225\n",
      "Epoch 00206: loss improved from 2.23815 to 2.22252, saving model to weights-improvement-wordmod-206-2.2225.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.2225\n",
      "Epoch 207/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2201\n",
      "Epoch 00207: loss improved from 2.22252 to 2.22082, saving model to weights-improvement-wordmod-207-2.2208.hdf5\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 2.2208\n",
      "Epoch 208/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2171\n",
      "Epoch 00208: loss improved from 2.22082 to 2.21755, saving model to weights-improvement-wordmod-208-2.2175.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.2175\n",
      "Epoch 209/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2118\n",
      "Epoch 00209: loss improved from 2.21755 to 2.21153, saving model to weights-improvement-wordmod-209-2.2115.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.2115\n",
      "Epoch 210/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2273\n",
      "Epoch 00210: loss did not improve\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.2268\n",
      "Epoch 211/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2252\n",
      "Epoch 00211: loss did not improve\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.2248\n",
      "Epoch 212/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2015\n",
      "Epoch 00212: loss improved from 2.21153 to 2.20184, saving model to weights-improvement-wordmod-212-2.2018.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.2018\n",
      "Epoch 213/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1954\n",
      "Epoch 00213: loss improved from 2.20184 to 2.19545, saving model to weights-improvement-wordmod-213-2.1954.hdf5\n",
      "25907/25907 [==============================] - 18s 679us/step - loss: 2.1954\n",
      "Epoch 214/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1970\n",
      "Epoch 00214: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.1978\n",
      "Epoch 215/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2190\n",
      "Epoch 00215: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.2193\n",
      "Epoch 216/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.2079\n",
      "Epoch 00216: loss did not improve\n",
      "25907/25907 [==============================] - 18s 714us/step - loss: 2.2080\n",
      "Epoch 217/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1791\n",
      "Epoch 00217: loss improved from 2.19545 to 2.17927, saving model to weights-improvement-wordmod-217-2.1793.hdf5\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 2.1793\n",
      "Epoch 218/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1617\n",
      "Epoch 00218: loss improved from 2.17927 to 2.16229, saving model to weights-improvement-wordmod-218-2.1623.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.1623\n",
      "Epoch 219/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1681\n",
      "Epoch 00219: loss did not improve\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.1680\n",
      "Epoch 220/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1648\n",
      "Epoch 00220: loss did not improve\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.1650\n",
      "Epoch 221/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1725\n",
      "Epoch 00221: loss did not improve\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.1719\n",
      "Epoch 222/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1686\n",
      "Epoch 00222: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.1682\n",
      "Epoch 223/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1541\n",
      "Epoch 00223: loss improved from 2.16229 to 2.15408, saving model to weights-improvement-wordmod-223-2.1541.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.1541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1504\n",
      "Epoch 00224: loss improved from 2.15408 to 2.15072, saving model to weights-improvement-wordmod-224-2.1507.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.1507\n",
      "Epoch 225/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1454\n",
      "Epoch 00225: loss improved from 2.15072 to 2.14601, saving model to weights-improvement-wordmod-225-2.1460.hdf5\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 2.1460\n",
      "Epoch 226/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1388\n",
      "Epoch 00226: loss improved from 2.14601 to 2.13874, saving model to weights-improvement-wordmod-226-2.1387.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.1387\n",
      "Epoch 227/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1199\n",
      "Epoch 00227: loss improved from 2.13874 to 2.12024, saving model to weights-improvement-wordmod-227-2.1202.hdf5\n",
      "25907/25907 [==============================] - 18s 681us/step - loss: 2.1202\n",
      "Epoch 228/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1530\n",
      "Epoch 00228: loss did not improve\n",
      "25907/25907 [==============================] - 18s 681us/step - loss: 2.1527\n",
      "Epoch 229/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1242\n",
      "Epoch 00229: loss did not improve\n",
      "25907/25907 [==============================] - 18s 680us/step - loss: 2.1246\n",
      "Epoch 230/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1170\n",
      "Epoch 00230: loss improved from 2.12024 to 2.11762, saving model to weights-improvement-wordmod-230-2.1176.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.1176\n",
      "Epoch 231/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1275\n",
      "Epoch 00231: loss did not improve\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 2.1280\n",
      "Epoch 232/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1401\n",
      "Epoch 00232: loss did not improve\n",
      "25907/25907 [==============================] - 18s 681us/step - loss: 2.1402\n",
      "Epoch 233/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1155\n",
      "Epoch 00233: loss improved from 2.11762 to 2.11551, saving model to weights-improvement-wordmod-233-2.1155.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.1155\n",
      "Epoch 234/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1094\n",
      "Epoch 00234: loss improved from 2.11551 to 2.10898, saving model to weights-improvement-wordmod-234-2.1090.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.1090\n",
      "Epoch 235/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1080\n",
      "Epoch 00235: loss improved from 2.10898 to 2.10805, saving model to weights-improvement-wordmod-235-2.1081.hdf5\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 2.1081\n",
      "Epoch 236/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1160\n",
      "Epoch 00236: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.1157\n",
      "Epoch 237/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1013\n",
      "Epoch 00237: loss improved from 2.10805 to 2.10160, saving model to weights-improvement-wordmod-237-2.1016.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.1016\n",
      "Epoch 238/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0881\n",
      "Epoch 00238: loss improved from 2.10160 to 2.08918, saving model to weights-improvement-wordmod-238-2.0892.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.0892\n",
      "Epoch 239/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0991\n",
      "Epoch 00239: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 2.0998\n",
      "Epoch 240/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.1031\n",
      "Epoch 00240: loss did not improve\n",
      "25907/25907 [==============================] - 18s 697us/step - loss: 2.1033\n",
      "Epoch 241/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0851\n",
      "Epoch 00241: loss improved from 2.08918 to 2.08567, saving model to weights-improvement-wordmod-241-2.0857.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.0857\n",
      "Epoch 242/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0946\n",
      "Epoch 00242: loss did not improve\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.0942\n",
      "Epoch 243/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0978\n",
      "Epoch 00243: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.0984\n",
      "Epoch 244/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0915\n",
      "Epoch 00244: loss did not improve\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 2.0915\n",
      "Epoch 245/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0699\n",
      "Epoch 00245: loss improved from 2.08567 to 2.07050, saving model to weights-improvement-wordmod-245-2.0705.hdf5\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 2.0705\n",
      "Epoch 246/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0486\n",
      "Epoch 00246: loss improved from 2.07050 to 2.04919, saving model to weights-improvement-wordmod-246-2.0492.hdf5\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.0492\n",
      "Epoch 247/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0687\n",
      "Epoch 00247: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 2.0690\n",
      "Epoch 248/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0672\n",
      "Epoch 00248: loss did not improve\n",
      "25907/25907 [==============================] - 18s 680us/step - loss: 2.0665\n",
      "Epoch 249/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0666\n",
      "Epoch 00249: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.0682\n",
      "Epoch 250/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0749\n",
      "Epoch 00250: loss did not improve\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 2.0750\n",
      "Epoch 251/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0604\n",
      "Epoch 00251: loss did not improve\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.0602\n",
      "Epoch 252/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0518\n",
      "Epoch 00252: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.0518\n",
      "Epoch 253/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0560\n",
      "Epoch 00253: loss did not improve\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 2.0565\n",
      "Epoch 254/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0475\n",
      "Epoch 00254: loss improved from 2.04919 to 2.04714, saving model to weights-improvement-wordmod-254-2.0471.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.0471\n",
      "Epoch 255/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0404\n",
      "Epoch 00255: loss improved from 2.04714 to 2.04118, saving model to weights-improvement-wordmod-255-2.0412.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 2.0412\n",
      "Epoch 256/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0629\n",
      "Epoch 00256: loss did not improve\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 2.0625\n",
      "Epoch 257/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0355\n",
      "Epoch 00257: loss improved from 2.04118 to 2.03546, saving model to weights-improvement-wordmod-257-2.0355.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 2.0355\n",
      "Epoch 258/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0399\n",
      "Epoch 00258: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.0398\n",
      "Epoch 259/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0279\n",
      "Epoch 00259: loss improved from 2.03546 to 2.02868, saving model to weights-improvement-wordmod-259-2.0287.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 2.0287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0213\n",
      "Epoch 00260: loss improved from 2.02868 to 2.02144, saving model to weights-improvement-wordmod-260-2.0214.hdf5\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.0214\n",
      "Epoch 261/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0318\n",
      "Epoch 00261: loss did not improve\n",
      "25907/25907 [==============================] - 18s 682us/step - loss: 2.0317\n",
      "Epoch 262/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0294\n",
      "Epoch 00262: loss did not improve\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.0293\n",
      "Epoch 263/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0291\n",
      "Epoch 00263: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.0291\n",
      "Epoch 264/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0482\n",
      "Epoch 00264: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.0480\n",
      "Epoch 265/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0027\n",
      "Epoch 00265: loss improved from 2.02144 to 2.00208, saving model to weights-improvement-wordmod-265-2.0021.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 2.0021\n",
      "Epoch 266/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0187\n",
      "Epoch 00266: loss did not improve\n",
      "25907/25907 [==============================] - 18s 706us/step - loss: 2.0185\n",
      "Epoch 267/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9924\n",
      "Epoch 00267: loss improved from 2.00208 to 1.99270, saving model to weights-improvement-wordmod-267-1.9927.hdf5\n",
      "25907/25907 [==============================] - 18s 699us/step - loss: 1.9927\n",
      "Epoch 268/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0181\n",
      "Epoch 00268: loss did not improve\n",
      "25907/25907 [==============================] - 18s 695us/step - loss: 2.0181\n",
      "Epoch 269/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9967\n",
      "Epoch 00269: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 1.9972\n",
      "Epoch 270/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9927\n",
      "Epoch 00270: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 1.9930\n",
      "Epoch 271/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0205\n",
      "Epoch 00271: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 2.0212\n",
      "Epoch 272/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0395\n",
      "Epoch 00272: loss did not improve\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 2.0388\n",
      "Epoch 273/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9962\n",
      "Epoch 00273: loss did not improve\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 1.9970\n",
      "Epoch 274/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9735\n",
      "Epoch 00274: loss improved from 1.99270 to 1.97481, saving model to weights-improvement-wordmod-274-1.9748.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 1.9748\n",
      "Epoch 275/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9738\n",
      "Epoch 00275: loss improved from 1.97481 to 1.97454, saving model to weights-improvement-wordmod-275-1.9745.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 1.9745\n",
      "Epoch 276/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9797\n",
      "Epoch 00276: loss did not improve\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 1.9800\n",
      "Epoch 277/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 2.0079\n",
      "Epoch 00277: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 2.0085\n",
      "Epoch 278/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9604\n",
      "Epoch 00278: loss improved from 1.97454 to 1.96054, saving model to weights-improvement-wordmod-278-1.9605.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 1.9605\n",
      "Epoch 279/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9700\n",
      "Epoch 00279: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 1.9706\n",
      "Epoch 280/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9563\n",
      "Epoch 00280: loss improved from 1.96054 to 1.95721, saving model to weights-improvement-wordmod-280-1.9572.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.9572\n",
      "Epoch 281/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9631\n",
      "Epoch 00281: loss did not improve\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 1.9633\n",
      "Epoch 282/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9629\n",
      "Epoch 00282: loss did not improve\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 1.9626\n",
      "Epoch 283/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9613\n",
      "Epoch 00283: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 1.9612\n",
      "Epoch 284/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9661\n",
      "Epoch 00284: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.9663\n",
      "Epoch 285/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9467\n",
      "Epoch 00285: loss improved from 1.95721 to 1.94681, saving model to weights-improvement-wordmod-285-1.9468.hdf5\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 1.9468\n",
      "Epoch 286/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9629\n",
      "Epoch 00286: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 1.9629\n",
      "Epoch 287/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9655\n",
      "Epoch 00287: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 1.9652\n",
      "Epoch 288/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9551\n",
      "Epoch 00288: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.9552\n",
      "Epoch 289/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9474\n",
      "Epoch 00289: loss did not improve\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 1.9480\n",
      "Epoch 290/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9773\n",
      "Epoch 00290: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 1.9776\n",
      "Epoch 291/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9546\n",
      "Epoch 00291: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 1.9548\n",
      "Epoch 292/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9494\n",
      "Epoch 00292: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 1.9494\n",
      "Epoch 293/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9336\n",
      "Epoch 00293: loss improved from 1.94681 to 1.93377, saving model to weights-improvement-wordmod-293-1.9338.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 1.9338\n",
      "Epoch 294/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9377\n",
      "Epoch 00294: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 1.9391\n",
      "Epoch 295/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9582\n",
      "Epoch 00295: loss did not improve\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 1.9576\n",
      "Epoch 296/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9197\n",
      "Epoch 00296: loss improved from 1.93377 to 1.91976, saving model to weights-improvement-wordmod-296-1.9198.hdf5\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.9198\n",
      "Epoch 297/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9323\n",
      "Epoch 00297: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 1.9333\n",
      "Epoch 298/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9467\n",
      "Epoch 00298: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 1.9468\n",
      "Epoch 299/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9314\n",
      "Epoch 00299: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.9315\n",
      "Epoch 300/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9207\n",
      "Epoch 00300: loss improved from 1.91976 to 1.91945, saving model to weights-improvement-wordmod-300-1.9195.hdf5\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 1.9195\n",
      "Epoch 301/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9043\n",
      "Epoch 00301: loss improved from 1.91945 to 1.90358, saving model to weights-improvement-wordmod-301-1.9036.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 1.9036\n",
      "Epoch 302/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9238\n",
      "Epoch 00302: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.9240\n",
      "Epoch 303/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9126\n",
      "Epoch 00303: loss did not improve\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 1.9138\n",
      "Epoch 304/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8983\n",
      "Epoch 00304: loss improved from 1.90358 to 1.89823, saving model to weights-improvement-wordmod-304-1.8982.hdf5\n",
      "25907/25907 [==============================] - 18s 696us/step - loss: 1.8982\n",
      "Epoch 305/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9264\n",
      "Epoch 00305: loss did not improve\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 1.9266\n",
      "Epoch 306/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9069\n",
      "Epoch 00306: loss did not improve\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 1.9068\n",
      "Epoch 307/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9135\n",
      "Epoch 00307: loss did not improve\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 1.9132\n",
      "Epoch 308/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8971- ETA:\n",
      "Epoch 00308: loss improved from 1.89823 to 1.89713, saving model to weights-improvement-wordmod-308-1.8971.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 1.8971\n",
      "Epoch 309/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9099\n",
      "Epoch 00309: loss did not improve\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 1.9097\n",
      "Epoch 310/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8789\n",
      "Epoch 00310: loss improved from 1.89713 to 1.87960, saving model to weights-improvement-wordmod-310-1.8796.hdf5\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 1.8796\n",
      "Epoch 311/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9219\n",
      "Epoch 00311: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 1.9223\n",
      "Epoch 312/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9007\n",
      "Epoch 00312: loss did not improve\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 1.9007\n",
      "Epoch 313/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9233\n",
      "Epoch 00313: loss did not improve\n",
      "25907/25907 [==============================] - 18s 698us/step - loss: 1.9234\n",
      "Epoch 314/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9031\n",
      "Epoch 00314: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.9035\n",
      "Epoch 315/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8907\n",
      "Epoch 00315: loss did not improve\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 1.8902\n",
      "Epoch 316/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8714\n",
      "Epoch 00316: loss improved from 1.87960 to 1.87155, saving model to weights-improvement-wordmod-316-1.8715.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 1.8715\n",
      "Epoch 317/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8768\n",
      "Epoch 00317: loss did not improve\n",
      "25907/25907 [==============================] - 18s 707us/step - loss: 1.8775\n",
      "Epoch 318/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8684\n",
      "Epoch 00318: loss improved from 1.87155 to 1.86707, saving model to weights-improvement-wordmod-318-1.8671.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 1.8671\n",
      "Epoch 319/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8861\n",
      "Epoch 00319: loss did not improve\n",
      "25907/25907 [==============================] - 18s 683us/step - loss: 1.8857\n",
      "Epoch 320/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8787\n",
      "Epoch 00320: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 1.8776\n",
      "Epoch 321/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8877\n",
      "Epoch 00321: loss did not improve\n",
      "25907/25907 [==============================] - 18s 682us/step - loss: 1.8878\n",
      "Epoch 322/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8954\n",
      "Epoch 00322: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 1.8952\n",
      "Epoch 323/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8717\n",
      "Epoch 00323: loss did not improve\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 1.8719\n",
      "Epoch 324/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8773\n",
      "Epoch 00324: loss did not improve\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 1.8777\n",
      "Epoch 325/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8631\n",
      "Epoch 00325: loss improved from 1.86707 to 1.86286, saving model to weights-improvement-wordmod-325-1.8629.hdf5\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 1.8629\n",
      "Epoch 326/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8549\n",
      "Epoch 00326: loss improved from 1.86286 to 1.85391, saving model to weights-improvement-wordmod-326-1.8539.hdf5\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 1.8539\n",
      "Epoch 327/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8495\n",
      "Epoch 00327: loss improved from 1.85391 to 1.84983, saving model to weights-improvement-wordmod-327-1.8498.hdf5\n",
      "25907/25907 [==============================] - 18s 703us/step - loss: 1.8498\n",
      "Epoch 328/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8559\n",
      "Epoch 00328: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.8568\n",
      "Epoch 329/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8429\n",
      "Epoch 00329: loss improved from 1.84983 to 1.84281, saving model to weights-improvement-wordmod-329-1.8428.hdf5\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 1.8428\n",
      "Epoch 330/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8667\n",
      "Epoch 00330: loss did not improve\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 1.8673\n",
      "Epoch 331/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8615\n",
      "Epoch 00331: loss did not improve\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 1.8615\n",
      "Epoch 332/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8566\n",
      "Epoch 00332: loss did not improve\n",
      "25907/25907 [==============================] - 18s 690us/step - loss: 1.8571\n",
      "Epoch 333/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8502\n",
      "Epoch 00333: loss did not improve\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 1.8497\n",
      "Epoch 334/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8482\n",
      "Epoch 00334: loss did not improve\n",
      "25907/25907 [==============================] - 18s 693us/step - loss: 1.8484\n",
      "Epoch 335/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8666\n",
      "Epoch 00335: loss did not improve\n",
      "25907/25907 [==============================] - 18s 689us/step - loss: 1.8676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 336/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8624\n",
      "Epoch 00336: loss did not improve\n",
      "25907/25907 [==============================] - 18s 686us/step - loss: 1.8626\n",
      "Epoch 337/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9104\n",
      "Epoch 00337: loss did not improve\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 1.9107\n",
      "Epoch 338/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8494\n",
      "Epoch 00338: loss did not improve\n",
      "25907/25907 [==============================] - 18s 681us/step - loss: 1.8493\n",
      "Epoch 339/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8509\n",
      "Epoch 00339: loss did not improve\n",
      "25907/25907 [==============================] - 18s 692us/step - loss: 1.8515\n",
      "Epoch 340/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8358\n",
      "Epoch 00340: loss improved from 1.84281 to 1.83632, saving model to weights-improvement-wordmod-340-1.8363.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 1.8363\n",
      "Epoch 341/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8371\n",
      "Epoch 00341: loss did not improve\n",
      "25907/25907 [==============================] - 18s 685us/step - loss: 1.8375\n",
      "Epoch 342/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8337\n",
      "Epoch 00342: loss improved from 1.83632 to 1.83469, saving model to weights-improvement-wordmod-342-1.8347.hdf5\n",
      "25907/25907 [==============================] - 18s 694us/step - loss: 1.8347\n",
      "Epoch 343/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.9222\n",
      "Epoch 00343: loss did not improve\n",
      "25907/25907 [==============================] - 18s 697us/step - loss: 1.9227\n",
      "Epoch 344/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8357\n",
      "Epoch 00344: loss did not improve\n",
      "25907/25907 [==============================] - 18s 691us/step - loss: 1.8365\n",
      "Epoch 345/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8638\n",
      "Epoch 00345: loss did not improve\n",
      "25907/25907 [==============================] - 18s 688us/step - loss: 1.8637\n",
      "Epoch 346/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8257\n",
      "Epoch 00346: loss improved from 1.83469 to 1.82561, saving model to weights-improvement-wordmod-346-1.8256.hdf5\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 1.8256\n",
      "Epoch 347/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8178\n",
      "Epoch 00347: loss improved from 1.82561 to 1.81758, saving model to weights-improvement-wordmod-347-1.8176.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 1.8176\n",
      "Epoch 348/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8207\n",
      "Epoch 00348: loss did not improve\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 1.8206\n",
      "Epoch 349/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8085\n",
      "Epoch 00349: loss improved from 1.81758 to 1.80941, saving model to weights-improvement-wordmod-349-1.8094.hdf5\n",
      "25907/25907 [==============================] - 18s 687us/step - loss: 1.8094\n",
      "Epoch 350/350\n",
      "25856/25907 [============================>.] - ETA: 0s - loss: 1.8104\n",
      "Epoch 00350: loss did not improve\n",
      "25907/25907 [==============================] - 18s 684us/step - loss: 1.8108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ad52b57c18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(GRU(200, input_shape=(X_word.shape[1], X_word.shape[2]), \n",
    "                return_sequences=True))\n",
    "model3.add(Dropout(.2))\n",
    "model3.add(GRU(200))\n",
    "model3.add(Dropout(.2))\n",
    "model3.add(Dense(y_word.shape[1], activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-wordmod-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model3.fit(X_word, y_word, epochs=350, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-wordmod-329-1.8428.hdf5\"\n",
    "model3.load_weights(filename)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" the fellow had a key and what s more he has it still i saw him use it not a \"\n",
      "week ago the he with sir very i i was i with stay i the the mr a the the you of in and and he to with have of whom that relate of the the i of a he could so look and the taking of was the above very the an guarantee and tempted in and were it to it and and to looking of the of and part and of he and the know god only been sunday trade of of i undignified get a the will it a to the of the like almost he on \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX_word)-1)\n",
    "pattern = dataX_word[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_word_vocab)\n",
    "    prediction = model3.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different source corpus for training and compare results\n",
    "\n",
    "Pick a very different source corpus, like the King James Bible or something that would differ greatly from the book you initially chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
