{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation using Project Gutenberg\n",
    "\n",
    "In this tutorial, we will download a story from Project Gutenberg and use it to feed a text generation model.\n",
    "\n",
    "About Project Gutenberg:\n",
    "\"Project Gutenberg offers over 56,000 free eBooks: Choose among free epub books, free kindle books, download them or read them online. You will find the world's great literature here, especially older works for which copyright has expired. We digitized and diligently proofread them with the help of thousands of volunteers.\"\n",
    "\n",
    "Navigate to https://www.gutenberg.org/ and pick a book with a plain text file of around 200 kb (Alice in Wonderland is a good option: https://www.gutenberg.org/ebooks/11). Download the text as the basis for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a Small LSTM Recurrent Neural Network\n",
    "In this section we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section we will use this model to generate new sequences of characters.\n",
    "\n",
    "Letâ€™s start off by importing the classes and functions we intend to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "import re\n",
    "filename = \"data/dr_hyde_easy.txt\"\n",
    "raw_text = open(filename, encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text = re.sub(r'[^\\w\\s]',' ', raw_text)\n",
    "\n",
    "# Get rid of line breaks and change multiple space to single\n",
    "raw_text = re.sub(r'[\\n]',' ', raw_text)\n",
    "raw_text = re.sub(r' +', ' ', raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  151293\n",
      "Total Vocab:  37\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "In this tutorial we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "\n",
    "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  151193\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the characters between 0 and 1.\n",
    "\n",
    "The problem is really a single character classification problem with n character classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
    "\n",
    "We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.7911\n",
      "Epoch 00001: loss improved from inf to 2.79100, saving model to weights-improvement-01-2.7910.hdf5\n",
      "151193/151193 [==============================] - 215s 1ms/step - loss: 2.7910\n",
      "Epoch 2/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.6379\n",
      "Epoch 00002: loss improved from 2.79100 to 2.63793, saving model to weights-improvement-02-2.6379.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.6379\n",
      "Epoch 3/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.5624\n",
      "Epoch 00003: loss improved from 2.63793 to 2.56246, saving model to weights-improvement-03-2.5625.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.5625\n",
      "Epoch 4/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.5021\n",
      "Epoch 00004: loss improved from 2.56246 to 2.50216, saving model to weights-improvement-04-2.5022.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.5022\n",
      "Epoch 5/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.4444\n",
      "Epoch 00005: loss improved from 2.50216 to 2.44435, saving model to weights-improvement-05-2.4444.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.4444\n",
      "Epoch 6/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.3940\n",
      "Epoch 00006: loss improved from 2.44435 to 2.39395, saving model to weights-improvement-06-2.3940.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.3940\n",
      "Epoch 7/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.3495\n",
      "Epoch 00007: loss improved from 2.39395 to 2.34958, saving model to weights-improvement-07-2.3496.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.3496\n",
      "Epoch 8/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.3072\n",
      "Epoch 00008: loss improved from 2.34958 to 2.30721, saving model to weights-improvement-08-2.3072.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.3072\n",
      "Epoch 9/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.2684\n",
      "Epoch 00009: loss improved from 2.30721 to 2.26834, saving model to weights-improvement-09-2.2683.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.2683\n",
      "Epoch 10/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.2305\n",
      "Epoch 00010: loss improved from 2.26834 to 2.23054, saving model to weights-improvement-10-2.2305.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.2305\n",
      "Epoch 11/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.1963\n",
      "Epoch 00011: loss improved from 2.23054 to 2.19625, saving model to weights-improvement-11-2.1962.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.1962\n",
      "Epoch 12/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.1608\n",
      "Epoch 00012: loss improved from 2.19625 to 2.16081, saving model to weights-improvement-12-2.1608.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.1608\n",
      "Epoch 13/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.1303\n",
      "Epoch 00013: loss improved from 2.16081 to 2.13031, saving model to weights-improvement-13-2.1303.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.1303\n",
      "Epoch 14/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0996\n",
      "Epoch 00014: loss improved from 2.13031 to 2.09956, saving model to weights-improvement-14-2.0996.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.0996\n",
      "Epoch 15/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0698\n",
      "Epoch 00015: loss improved from 2.09956 to 2.06980, saving model to weights-improvement-15-2.0698.hdf5\n",
      "151193/151193 [==============================] - 214s 1ms/step - loss: 2.0698\n",
      "Epoch 16/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0398\n",
      "Epoch 00016: loss improved from 2.06980 to 2.03976, saving model to weights-improvement-16-2.0398.hdf5\n",
      "151193/151193 [==============================] - 213s 1ms/step - loss: 2.0398\n",
      "Epoch 17/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0123\n",
      "Epoch 00017: loss improved from 2.03976 to 2.01234, saving model to weights-improvement-17-2.0123.hdf5\n",
      "151193/151193 [==============================] - 211s 1ms/step - loss: 2.0123\n",
      "Epoch 18/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.9854\n",
      "Epoch 00018: loss improved from 2.01234 to 1.98540, saving model to weights-improvement-18-1.9854.hdf5\n",
      "151193/151193 [==============================] - 209s 1ms/step - loss: 1.9854\n",
      "Epoch 19/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.9612\n",
      "Epoch 00019: loss improved from 1.98540 to 1.96114, saving model to weights-improvement-19-1.9611.hdf5\n",
      "151193/151193 [==============================] - 216s 1ms/step - loss: 1.9611\n",
      "Epoch 20/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.9344\n",
      "Epoch 00020: loss improved from 1.96114 to 1.93435, saving model to weights-improvement-20-1.9344.hdf5\n",
      "151193/151193 [==============================] - 216s 1ms/step - loss: 1.9344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa89163668>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the example, you should have a number of weight checkpoint files in the local directory.\n",
    "\n",
    "You can delete them all except the one with the smallest loss value, eg `weights-improvement-19-1.9435.hdf5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text with an LSTM Network\n",
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-1.9344.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ion and must flee before daylight from a house that was no longer mine and hurrying back to my cabin \"\n",
      "et io the mawe of the sare thi fane oo the fand of the saae whth a streng of toeer and the lawter sooe a creat dare oo the sereo of the fouse oh the saalen the lawter whsh a stidn and the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawyer i sae io whet ie a mort an wou cane a auaat foro the coor of the sale the sawe oo the coor of the saalen the lawyer saed the lawy\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the results shows good language generation? What is done poorly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was able to create something like words in the beginning, however, afterwards it just picked up the same pattern over and over again.  One could assume that it did so because that pattern is common throughout the book or it isn't able to distinguish other patterns after those letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Try a bigger model\n",
    "\n",
    "We will keep the number of memory units the same at 256, but add a second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.6894\n",
      "Epoch 00001: loss improved from inf to 2.68936, saving model to weights-improvement-mod2-01-2.6894.hdf5\n",
      "151193/151193 [==============================] - 485s 3ms/step - loss: 2.6894\n",
      "Epoch 2/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.4266\n",
      "Epoch 00002: loss improved from 2.68936 to 2.42657, saving model to weights-improvement-mod2-02-2.4266.hdf5\n",
      "151193/151193 [==============================] - 494s 3ms/step - loss: 2.4266\n",
      "Epoch 3/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.2216\n",
      "Epoch 00003: loss improved from 2.42657 to 2.22155, saving model to weights-improvement-mod2-03-2.2215.hdf5\n",
      "151193/151193 [==============================] - 487s 3ms/step - loss: 2.2215\n",
      "Epoch 4/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 2.0699\n",
      "Epoch 00004: loss improved from 2.22155 to 2.06995, saving model to weights-improvement-mod2-04-2.0699.hdf5\n",
      "151193/151193 [==============================] - 482s 3ms/step - loss: 2.0699\n",
      "Epoch 5/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.9652\n",
      "Epoch 00005: loss improved from 2.06995 to 1.96525, saving model to weights-improvement-mod2-05-1.9652.hdf5\n",
      "151193/151193 [==============================] - 484s 3ms/step - loss: 1.9652\n",
      "Epoch 6/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.8812\n",
      "Epoch 00006: loss improved from 1.96525 to 1.88113, saving model to weights-improvement-mod2-06-1.8811.hdf5\n",
      "151193/151193 [==============================] - 484s 3ms/step - loss: 1.8811\n",
      "Epoch 7/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.8125\n",
      "Epoch 00007: loss improved from 1.88113 to 1.81244, saving model to weights-improvement-mod2-07-1.8124.hdf5\n",
      "151193/151193 [==============================] - 493s 3ms/step - loss: 1.8124\n",
      "Epoch 8/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.7521\n",
      "Epoch 00008: loss improved from 1.81244 to 1.75216, saving model to weights-improvement-mod2-08-1.7522.hdf5\n",
      "151193/151193 [==============================] - 486s 3ms/step - loss: 1.7522\n",
      "Epoch 9/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.6995\n",
      "Epoch 00009: loss improved from 1.75216 to 1.69947, saving model to weights-improvement-mod2-09-1.6995.hdf5\n",
      "151193/151193 [==============================] - 487s 3ms/step - loss: 1.6995\n",
      "Epoch 10/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.6519\n",
      "Epoch 00010: loss improved from 1.69947 to 1.65197, saving model to weights-improvement-mod2-10-1.6520.hdf5\n",
      "151193/151193 [==============================] - 500s 3ms/step - loss: 1.6520\n",
      "Epoch 11/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.6046\n",
      "Epoch 00011: loss improved from 1.65197 to 1.60455, saving model to weights-improvement-mod2-11-1.6046.hdf5\n",
      "151193/151193 [==============================] - 468s 3ms/step - loss: 1.6046\n",
      "Epoch 12/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.5648\n",
      "Epoch 00012: loss improved from 1.60455 to 1.56476, saving model to weights-improvement-mod2-12-1.5648.hdf5\n",
      "151193/151193 [==============================] - 465s 3ms/step - loss: 1.5648\n",
      "Epoch 13/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.5239\n",
      "Epoch 00013: loss improved from 1.56476 to 1.52396, saving model to weights-improvement-mod2-13-1.5240.hdf5\n",
      "151193/151193 [==============================] - 460s 3ms/step - loss: 1.5240\n",
      "Epoch 14/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.4826\n",
      "Epoch 00014: loss improved from 1.52396 to 1.48261, saving model to weights-improvement-mod2-14-1.4826.hdf5\n",
      "151193/151193 [==============================] - 466s 3ms/step - loss: 1.4826\n",
      "Epoch 15/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00015: loss improved from 1.48261 to 1.44319, saving model to weights-improvement-mod2-15-1.4432.hdf5\n",
      "151193/151193 [==============================] - 462s 3ms/step - loss: 1.4432\n",
      "Epoch 16/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.4043\n",
      "Epoch 00016: loss improved from 1.44319 to 1.40436, saving model to weights-improvement-mod2-16-1.4044.hdf5\n",
      "151193/151193 [==============================] - 461s 3ms/step - loss: 1.4044\n",
      "Epoch 17/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.3651\n",
      "Epoch 00017: loss improved from 1.40436 to 1.36516, saving model to weights-improvement-mod2-17-1.3652.hdf5\n",
      "151193/151193 [==============================] - 460s 3ms/step - loss: 1.3652\n",
      "Epoch 18/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.3241\n",
      "Epoch 00018: loss improved from 1.36516 to 1.32418, saving model to weights-improvement-mod2-18-1.3242.hdf5\n",
      "151193/151193 [==============================] - 459s 3ms/step - loss: 1.3242\n",
      "Epoch 19/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.2851\n",
      "Epoch 00019: loss improved from 1.32418 to 1.28501, saving model to weights-improvement-mod2-19-1.2850.hdf5\n",
      "151193/151193 [==============================] - 461s 3ms/step - loss: 1.2850\n",
      "Epoch 20/20\n",
      "151168/151193 [============================>.] - ETA: 0s - loss: 1.2442\n",
      "Epoch 00020: loss improved from 1.28501 to 1.24420, saving model to weights-improvement-mod2-20-1.2442.hdf5\n",
      "151193/151193 [==============================] - 460s 3ms/step - loss: 1.2442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aba1f0b978>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), \n",
    "                return_sequences=True))\n",
    "model2.add(LSTM(256))\n",
    "model2.add(Dense(y.shape[1], activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-mod2-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model2.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-mod2-20-1.2442.hdf5\"\n",
    "model2.load_weights(filename)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" th which i listened to the civilities of my unhappy victim i declare at least before god no man mora \"\n",
      "ly plt reterted the lawyer i shought it was not tn me a changed renlsee to the project gutenberg tm trademark and distribution of the project gutenberg tm micense and distribution or any pther with the perains of the fyl tere of the fyn tere aut the ward of the lawy moment i was still and i cegind the servant metter i shank you sile that i cannot sead the lawyer i have have his friends were the strange cloter was not my hiad to my consenoent for my sesvrn i dould have teen and i have have eone to my heart that i cannot sead the lawyer i have have his friends were the strange cloter was not my hiad to my consenoent for my sesvrn i dould have teen and i have have eone to my heart that i cannot sead the lawyer i have have his friends were the strange cloter was not my hiad to my consenoent for my sesvrn i dould have teen and i have have eone to my heart that i cannot sead the lawyer i have have his friends were the strange cloter was not my hiad to my consenoent for my sesvrn i dould have\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model2.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, what about the results shows good language generation? What is done poorly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the model is able to better predict some words, such as **lawyer** and **friends**.  However, it seems to replace some letters such as **s** for **r** and **s** for **t** in words such as *sead* that should be *read* and *shank* instead of *thank*.  However, there still seems to be repitition in the model in what it predicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a word based model\n",
    "\n",
    "Instead of generating language character by character, let's build an word generating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  29027\n",
      "Total Vocab:  4327\n",
      "Total Patterns:  28927\n"
     ]
    }
   ],
   "source": [
    "word_text = raw_text.split(' ')\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "words = sorted(list(set(word_text)))\n",
    "word_to_int = dict((c, i) for i, c in enumerate(words))\n",
    "\n",
    "n_words = len(word_text)\n",
    "n_word_vocab = len(words)\n",
    "print(\"Total Characters: \", n_words)\n",
    "print(\"Total Vocab: \", n_word_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "word_seq_length = 100\n",
    "dataX_word = []\n",
    "dataY_word = []\n",
    "for i in range(0, n_words - word_seq_length, 1):\n",
    "    seq_in = word_text[i:i + word_seq_length]\n",
    "    seq_out = word_text[i + word_seq_length]\n",
    "    dataX_word.append([word_to_int[word] for word in seq_in])\n",
    "    dataY_word.append(word_to_int[seq_out])\n",
    "n_word_patterns = len(dataX_word)\n",
    "print(\"Total Patterns: \", n_word_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X_word = numpy.reshape(dataX_word, (n_word_patterns, word_seq_length, 1))\n",
    "# normalize\n",
    "X_word = X_word / float(n_word_vocab)\n",
    "# one hot encode the output variable\n",
    "y_word = np_utils.to_categorical(dataY_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.7547\n",
      "Epoch 00001: loss improved from inf to 6.75371, saving model to weights-improvement-wordmod-01-6.7537.hdf5\n",
      "28927/28927 [==============================] - 122s 4ms/step - loss: 6.7537\n",
      "Epoch 2/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4458\n",
      "Epoch 00002: loss improved from 6.75371 to 6.44474, saving model to weights-improvement-wordmod-02-6.4447.hdf5\n",
      "28927/28927 [==============================] - 120s 4ms/step - loss: 6.4447\n",
      "Epoch 3/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4385\n",
      "Epoch 00003: loss improved from 6.44474 to 6.43840, saving model to weights-improvement-wordmod-03-6.4384.hdf5\n",
      "28927/28927 [==============================] - 108s 4ms/step - loss: 6.4384\n",
      "Epoch 4/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4400\n",
      "Epoch 00004: loss improved from 6.43840 to 6.43824, saving model to weights-improvement-wordmod-04-6.4382.hdf5\n",
      "28927/28927 [==============================] - 108s 4ms/step - loss: 6.4382\n",
      "Epoch 5/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4358\n",
      "Epoch 00005: loss improved from 6.43824 to 6.43629, saving model to weights-improvement-wordmod-05-6.4363.hdf5\n",
      "28927/28927 [==============================] - 108s 4ms/step - loss: 6.4363\n",
      "Epoch 6/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4375\n",
      "Epoch 00006: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4379\n",
      "Epoch 7/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4391\n",
      "Epoch 00007: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4387\n",
      "Epoch 8/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4362\n",
      "Epoch 00008: loss did not improve\n",
      "28927/28927 [==============================] - 108s 4ms/step - loss: 6.4379\n",
      "Epoch 9/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4397\n",
      "Epoch 00009: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4391\n",
      "Epoch 10/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4392\n",
      "Epoch 00010: loss did not improve\n",
      "28927/28927 [==============================] - 108s 4ms/step - loss: 6.4402\n",
      "Epoch 11/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4377\n",
      "Epoch 00011: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4381\n",
      "Epoch 12/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4390\n",
      "Epoch 00012: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4396\n",
      "Epoch 13/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4388\n",
      "Epoch 00013: loss did not improve\n",
      "28927/28927 [==============================] - 108s 4ms/step - loss: 6.4378\n",
      "Epoch 14/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4389\n",
      "Epoch 00014: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4396\n",
      "Epoch 15/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4397\n",
      "Epoch 00015: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4390\n",
      "Epoch 16/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4378\n",
      "Epoch 00016: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4372\n",
      "Epoch 17/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4339\n",
      "Epoch 00017: loss improved from 6.43629 to 6.43503, saving model to weights-improvement-wordmod-17-6.4350.hdf5\n",
      "28927/28927 [==============================] - 108s 4ms/step - loss: 6.4350\n",
      "Epoch 18/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4361\n",
      "Epoch 00018: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4365\n",
      "Epoch 19/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4397\n",
      "Epoch 00019: loss did not improve\n",
      "28927/28927 [==============================] - 108s 4ms/step - loss: 6.4392\n",
      "Epoch 20/20\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.4357\n",
      "Epoch 00020: loss did not improve\n",
      "28927/28927 [==============================] - 107s 4ms/step - loss: 6.4375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aba5d10d68>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(LSTM(100, input_shape=(X_word.shape[1], X_word.shape[2]), \n",
    "                return_sequences=True))\n",
    "model3.add(LSTM(100, return_sequences=True))\n",
    "model3.add(LSTM(100))\n",
    "model3.add(Dense(y_word.shape[1], activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-wordmod-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model3.fit(X_word, y_word, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-wordmod-17-6.4350.hdf5\"\n",
    "model3.load_weights(filename)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" my life my honour my reason are all at your mercy if you fail me to night i am lost you might suppose after this preface that i am going to ask you for something dishonourable to grant judge for yourself i want you to postpone all other engagements for to night ay even if you were summoned to the bedside of an emperor to take a cab unless your carriage should be actually at the door and with this letter in your hand for consultation to drive straight to my house poole my butler has his orders you will \"\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX_word)-1)\n",
    "pattern = dataX_word[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_word_vocab)\n",
    "    prediction = model3.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.7759\n",
      "Epoch 00001: loss improved from inf to 6.77577, saving model to weights-improvement-wordmod2-01-6.7758.hdf5\n",
      "28927/28927 [==============================] - 157s 5ms/step - loss: 6.7758\n",
      "Epoch 2/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3898\n",
      "Epoch 00002: loss improved from 6.77577 to 6.38962, saving model to weights-improvement-wordmod2-02-6.3896.hdf5\n",
      "28927/28927 [==============================] - 155s 5ms/step - loss: 6.3896\n",
      "Epoch 3/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3805\n",
      "Epoch 00003: loss improved from 6.38962 to 6.38053, saving model to weights-improvement-wordmod2-03-6.3805.hdf5\n",
      "28927/28927 [==============================] - 155s 5ms/step - loss: 6.3805\n",
      "Epoch 4/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3811\n",
      "Epoch 00004: loss improved from 6.38053 to 6.37867, saving model to weights-improvement-wordmod2-04-6.3787.hdf5\n",
      "28927/28927 [==============================] - 155s 5ms/step - loss: 6.3787\n",
      "Epoch 5/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3796\n",
      "Epoch 00005: loss improved from 6.37867 to 6.37842, saving model to weights-improvement-wordmod2-05-6.3784.hdf5\n",
      "28927/28927 [==============================] - 155s 5ms/step - loss: 6.3784\n",
      "Epoch 6/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3772\n",
      "Epoch 00006: loss did not improve\n",
      "28927/28927 [==============================] - 154s 5ms/step - loss: 6.3786\n",
      "Epoch 7/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3786\n",
      "Epoch 00007: loss improved from 6.37842 to 6.37810, saving model to weights-improvement-wordmod2-07-6.3781.hdf5\n",
      "28927/28927 [==============================] - 154s 5ms/step - loss: 6.3781\n",
      "Epoch 8/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3797\n",
      "Epoch 00008: loss did not improve\n",
      "28927/28927 [==============================] - 154s 5ms/step - loss: 6.3787\n",
      "Epoch 9/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3773\n",
      "Epoch 00009: loss did not improve\n",
      "28927/28927 [==============================] - 154s 5ms/step - loss: 6.3783\n",
      "Epoch 10/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.3778\n",
      "Epoch 00010: loss did not improve\n",
      "28927/28927 [==============================] - 154s 5ms/step - loss: 6.3787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abb0cae0b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(LSTM(50, input_shape=(X_word.shape[1], X_word.shape[2]), \n",
    "                return_sequences=True))\n",
    "model4.add(LSTM(50, return_sequences=True))\n",
    "model4.add(LSTM(50, return_sequences=True))\n",
    "model4.add(LSTM(50, return_sequences=True))\n",
    "model4.add(LSTM(50))\n",
    "model4.add(Dense(y_word.shape[1], activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-wordmod2-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model4.fit(X_word, y_word, epochs=10, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" of hyde it took on this occasion a double dose to recall me to myself and alas six hours after as i sat looking sadly in the fire the pangs returned and the drug had to be re administered in short from that day forth it seemed only by a great effort as of gymnastics and only under the immediate stimulation of the drug that i was able to wear the countenance of jekyll at all hours of the day and night i would be taken with the premonitory shudder above all if i slept or even dozed for a \"\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-wordmod2-07-6.3781.hdf5\"\n",
    "model4.load_weights(filename)\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "int_to_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX_word)-1)\n",
    "pattern = dataX_word[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_word_vocab)\n",
    "    prediction = model4.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.8752\n",
      "Epoch 00001: loss improved from inf to 6.87377, saving model to weights-improvement-wordmod3-01-6.8738.hdf5\n",
      "28927/28927 [==============================] - 182s 6ms/step - loss: 6.8738\n",
      "Epoch 2/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.5987\n",
      "Epoch 00002: loss improved from 6.87377 to 6.59828, saving model to weights-improvement-wordmod3-02-6.5983.hdf5\n",
      "28927/28927 [==============================] - 179s 6ms/step - loss: 6.5983\n",
      "Epoch 3/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.6010\n",
      "Epoch 00003: loss did not improve\n",
      "28927/28927 [==============================] - 180s 6ms/step - loss: 6.6015\n",
      "Epoch 4/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.6098\n",
      "Epoch 00004: loss did not improve\n",
      "28927/28927 [==============================] - 179s 6ms/step - loss: 6.6100\n",
      "Epoch 5/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.6103\n",
      "Epoch 00005: loss did not improve\n",
      "28927/28927 [==============================] - 178s 6ms/step - loss: 6.6085\n",
      "Epoch 6/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.6059\n",
      "Epoch 00006: loss did not improve\n",
      "28927/28927 [==============================] - 178s 6ms/step - loss: 6.6057\n",
      "Epoch 7/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.6078\n",
      "Epoch 00007: loss did not improve\n",
      "28927/28927 [==============================] - 178s 6ms/step - loss: 6.6084\n",
      "Epoch 8/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.6086\n",
      "Epoch 00008: loss did not improve\n",
      "28927/28927 [==============================] - 178s 6ms/step - loss: 6.6092\n",
      "Epoch 9/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.6062\n",
      "Epoch 00009: loss did not improve\n",
      "28927/28927 [==============================] - 178s 6ms/step - loss: 6.6084\n",
      "Epoch 10/10\n",
      "28800/28927 [============================>.] - ETA: 0s - loss: 6.6055\n",
      "Epoch 00010: loss did not improve\n",
      "28927/28927 [==============================] - 178s 6ms/step - loss: 6.6064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abba7fbf98>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(LSTM(300, input_shape=(X_word.shape[1], X_word.shape[2]), \n",
    "                return_sequences=True))\n",
    "model5.add(LSTM(300, return_sequences=True))\n",
    "model5.add(LSTM(300))\n",
    "model5.add(Dense(y_word.shape[1], activation='softmax'))\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-wordmod3-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model5.fit(X_word, y_word, epochs=10, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" copyright holder the work can be copied and distributed to anyone in the united states without paying any fees or charges if you are redistributing or providing access to a work with the phrase project gutenberg associated with or appearing on the work you must comply either with the requirements of paragraphs 1 e 1 through 1 e 7 or obtain permission for the use of the work and the project gutenberg tm trademark as set forth in paragraphs 1 e 8 or 1 e 9 1 e 3 if an individual project gutenberg tm electronic work is posted with \"\n",
      "and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-wordmod3-02-6.5983.hdf5\"\n",
    "model5.load_weights(filename)\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "int_to_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX_word)-1)\n",
    "pattern = dataX_word[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_word_vocab)\n",
    "    prediction = model5.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different source corpus for training and compare results\n",
    "\n",
    "Pick a very different source corpus, like the King James Bible or something that would differ greatly from the book you initially chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
