{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.loads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
