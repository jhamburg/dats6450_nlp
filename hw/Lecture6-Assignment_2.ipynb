{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Movie Classification, the sequel\n",
    "![](https://images-na.ssl-images-amazon.com/images/S/sgp-catalog-images/region_US/paramount-01376-Full-Image_GalleryBackground-en-US-1484000188762._RI_SX940_.jpg)\n",
    "\n",
    "\n",
    "#### In this assignment, we will learn a little more about word2vec and then use the resulting vectors to make some predictions.\n",
    "\n",
    "We will be working with a movie synopsis dataset, found here: http://www.cs.cmu.edu/~ark/personas/\n",
    "\n",
    "The overall goal should sound a little familiar - based on the movie synopses, we will classify movie genre. Some of your favorites should be in this dataset, and hopefully, based on the genre specific terminology of the movie synopses, we will be able to figure out which movies are which type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: clean your dataset!\n",
    "\n",
    "For your input data:\n",
    "\n",
    "1. Find the top 10 movie genres\n",
    "2. Remove any synopses that don't fit into these genres\n",
    "3. Take the top 10,000 reviews in terms of \"Movie box office revenue\"\n",
    "\n",
    "Congrats, you've got a dataset! For each movie, some of them may have multiple classifications. To deal with this, you'll have to look at the Reuters dataset classification code that we used previously and possibly this example: https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n",
    "\n",
    "We want to use categorical cross-entropy as our loss function (or a one vs. all classifier in the case of SVM) because our data will potentially have multiple classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "import word2vec\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '../data/MovieSummaries/'\n",
    "movieHeader = ['wikiID', 'freebaseID', 'name', 'releaseDate', 'revenue',\n",
    "               'runtime', 'languages', 'countries', 'genres']\n",
    "movieDat = pd.read_csv(dataPath + 'movie.metadata.tsv', delimiter = '\\t',\n",
    "                      header = None, names = movieHeader)\n",
    "synopsisDat = pd.read_csv(dataPath + 'plot_summaries.txt', delimiter = '\\t',\n",
    "                      header = None, names = ['wikiID', 'synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find top genres, will split the genres into their own columns\n",
    "# since one move can be multiple genres and then sum those columns\n",
    "# to find the max 10\n",
    "\n",
    "### Step 1 -- Convert Genres into a list of genres\n",
    "def cleanGenres(genreDat):\n",
    "    clean = [re.findall(r'\"\\S+\": \"(.+)\"', x) for x in genreDat.split(',')]\n",
    "    return [item for sublist in clean for item in sublist]\n",
    "    \n",
    "movieDat['genres_clean'] = movieDat.genres.apply(cleanGenres)\n",
    "\n",
    "### Step 2 --- \"One Hot Encode\" that list and then join back\n",
    "mlb = MultiLabelBinarizer()\n",
    "movieDat = movieDat.join(pd.DataFrame(mlb.fit_transform(movieDat['genres_clean']),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=movieDat.index))\n",
    "\n",
    "### Step 3 --- Find genres with the largest sums\n",
    "idCols = movieHeader.copy()\n",
    "idCols.extend(['genres_clean'])\n",
    "genreCols = movieDat.columns.difference(idCols)\n",
    "topGenres = movieDat.loc[:, genreCols].sum().nlargest(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filter dataset to those that contain these genres and take top 10,000 highest grossing\n",
    "movieDat['topGenresOnly'] = movieDat['genres_clean'].apply(lambda x: set(x).intersection(topGenres))\n",
    "containsGenreBool = movieDat['genres_clean'].apply(any)\n",
    "movies = movieDat.loc[containsGenreBool].sort_values('revenue', ascending=False)\n",
    "finalDat = movies.merge(synopsisDat, how = 'inner', on='wikiID').iloc[:10000]\n",
    "\n",
    "# Split into X and y and preprocess\n",
    "X = np.array(finalDat['synopsis'].apply(gensim.utils.simple_preprocess))\n",
    "y = np.array(finalDat['topGenresOnly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split the data\n",
    "\n",
    "Make a dataset of 70% train and 30% test. Sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb2 = MultiLabelBinarizer()\n",
    "train_labels = mlb2.fit_transform(y_train)\n",
    "test_labels = mlb2.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: Build a model using ONLY word2vec\n",
    "\n",
    "Woah what? I don't think that's recommended...\n",
    "\n",
    "In fact it's a commonly accepted practice. What you will want to do is average the word vectors that will be input for a given synopsis (https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) and then input that averaged vector as your feature space into a model. For this example, use a Support Vector Machine classifier. For your first time doing this, train a model in Gensim and use the output vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec (X_train, size=100, window=5, min_count=5, workers=10)\n",
    "model.train(X_train,total_examples=len(X_train),epochs=10)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert X_train and X_test to matrix of values by\n",
    "# substituting each word to get its mean embedded score\n",
    "def convert_word_mat_to_num(word_mat, w2v):\n",
    "    dim = len(next(iter(w2v.values())))\n",
    "    return np.array([np.mean([w2v[w] for w in words if w in w2v]\n",
    "                             or [np.zeros(dim)], axis=0)\n",
    "                     for words in word_mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMod(mod, embedding):\n",
    "    clf = OneVsRestClassifier(mod(random_state=10))\n",
    "    train_x = convert_word_mat_to_num(X_train, embedding)\n",
    "    clf.fit(train_x, train_labels)\n",
    "    \n",
    "    test_x = convert_word_mat_to_num(X_test, embedding)\n",
    "    preds = clf.predict(test_x)\n",
    "    acc = metrics.accuracy_score(test_labels, preds)\n",
    "    return (clf, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_w2v = runMod(SVC, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Do the same thing but with pretrained embeddings\n",
    "\n",
    "Now pull down the Google News word embeddings and do the same thing. Compare the results. Why was one better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_goog = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "w2v_goog = {w: vec for w, vec in zip(model_goog.wv.index2word, model_goog.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_w2v = runMod(SVC, w2v_goog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Build a neural net model using word2vec embeddings (both pretrained and within an Embedding layer from Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Change the architecture of your model and compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: For each model, do an error evaluation\n",
    "\n",
    "You now have a bunch of classifiers. For each classifier, pick 2 good classifications and 2 bad classifications. Print the expected and predicted label, and also print the movie synopsis. From these results, can you spot some systematic errors from your models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
