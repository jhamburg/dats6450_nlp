{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Movie Classification, the sequel\n",
    "![](https://images-na.ssl-images-amazon.com/images/S/sgp-catalog-images/region_US/paramount-01376-Full-Image_GalleryBackground-en-US-1484000188762._RI_SX940_.jpg)\n",
    "\n",
    "\n",
    "#### In this assignment, we will learn a little more about word2vec and then use the resulting vectors to make some predictions.\n",
    "\n",
    "We will be working with a movie synopsis dataset, found here: http://www.cs.cmu.edu/~ark/personas/\n",
    "\n",
    "The overall goal should sound a little familiar - based on the movie synopses, we will classify movie genre. Some of your favorites should be in this dataset, and hopefully, based on the genre specific terminology of the movie synopses, we will be able to figure out which movies are which type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: clean your dataset!\n",
    "\n",
    "For your input data:\n",
    "\n",
    "1. Find the top 10 movie genres\n",
    "2. Remove any synopses that don't fit into these genres\n",
    "3. Take the top 10,000 reviews in terms of \"Movie box office revenue\"\n",
    "\n",
    "Congrats, you've got a dataset! For each movie, some of them may have multiple classifications. To deal with this, you'll have to look at the Reuters dataset classification code that we used previously and possibly this example: https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n",
    "\n",
    "We want to use categorical cross-entropy as our loss function (or a one vs. all classifier in the case of SVM) because our data will potentially have multiple classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "import word2vec\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '../data/MovieSummaries/'\n",
    "movieHeader = ['wikiID', 'freebaseID', 'name', 'releaseDate', 'revenue',\n",
    "               'runtime', 'languages', 'countries', 'genres']\n",
    "movieDat = pd.read_csv(dataPath + 'movie.metadata.tsv', delimiter = '\\t',\n",
    "                      header = None, names = movieHeader)\n",
    "synopsisDat = pd.read_csv(dataPath + 'plot_summaries.txt', delimiter = '\\t',\n",
    "                      header = None, names = ['wikiID', 'synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find top genres, will split the genres into their own columns\n",
    "# since one move can be multiple genres and then sum those columns\n",
    "# to find the max 10\n",
    "\n",
    "### Step 1 -- Convert Genres into a list of genres\n",
    "def cleanGenres(genreDat):\n",
    "    clean = [re.findall(r'\"\\S+\": \"(.+)\"', x) for x in genreDat.split(',')]\n",
    "    return [item for sublist in clean for item in sublist]\n",
    "    \n",
    "movieDat['genres_clean'] = movieDat.genres.apply(cleanGenres)\n",
    "\n",
    "### Step 2 --- \"One Hot Encode\" that list and then join back\n",
    "mlb = MultiLabelBinarizer()\n",
    "movieDat = movieDat.join(pd.DataFrame(mlb.fit_transform(movieDat['genres_clean']),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=movieDat.index))\n",
    "\n",
    "### Step 3 --- Find genres with the largest sums\n",
    "idCols = movieHeader.copy()\n",
    "idCols.extend(['genres_clean'])\n",
    "genreCols = movieDat.columns.difference(idCols)\n",
    "topGenres = movieDat.loc[:, genreCols].sum().nlargest(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filter dataset to those that contain these genres and take top 10,000 highest grossing\n",
    "movieDat['topGenresOnly'] = movieDat['genres_clean'].apply(lambda x: set(x).intersection(topGenres))\n",
    "containsGenreBool = movieDat['genres_clean'].apply(any)\n",
    "movies = movieDat.loc[containsGenreBool].sort_values('revenue', ascending=False)\n",
    "finalDat = movies.merge(synopsisDat, how = 'inner', on='wikiID').iloc[:10000]\n",
    "\n",
    "# Split into X and y and preprocess\n",
    "X = np.array(finalDat['synopsis'].apply(gensim.utils.simple_preprocess))\n",
    "y = np.array(finalDat['topGenresOnly'])\n",
    "\n",
    "all_words = set(w for words in X for w in words)\n",
    "vocab_size = len(all_words)\n",
    "embed_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split the data\n",
    "\n",
    "Make a dataset of 70% train and 30% test. Sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb2 = MultiLabelBinarizer()\n",
    "train_labels = mlb2.fit_transform(y_train)\n",
    "test_labels = mlb2.transform(y_test)\n",
    "\n",
    "# x_train = pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "# x_test = pad_sequences(X_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: Build a model using ONLY word2vec\n",
    "\n",
    "Woah what? I don't think that's recommended...\n",
    "\n",
    "In fact it's a commonly accepted practice. What you will want to do is average the word vectors that will be input for a given synopsis (https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) and then input that averaged vector as your feature space into a model. For this example, use a Support Vector Machine classifier. For your first time doing this, train a model in Gensim and use the output vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec (X_train, size=embed_size, \n",
    "                                window=5, min_count=5, workers=-1)\n",
    "model.train(X_train,total_examples=len(X_train),epochs=10)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert X_train and X_test to matrix of values by\n",
    "# substituting each word to get its mean embedded score\n",
    "\n",
    "def convert_word_mat_to_mean_embed(word_mat, w2v):\n",
    "    dim = len(next(iter(w2v.values())))\n",
    "    return np.array([np.mean([w2v[w] for w in words if w in w2v]\n",
    "                             or [np.zeros(dim)], axis=0)\n",
    "                     for words in word_mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMod(mod, embedding):\n",
    "    clf = OneVsRestClassifier(mod(random_state=10))\n",
    "    train_x = convert_word_mat_to_mean_embed(X_train, embedding)\n",
    "    clf.fit(train_x, train_labels)\n",
    "    \n",
    "    test_x = convert_word_mat_to_mean_embed(X_test, embedding)\n",
    "    preds = clf.predict(test_x)\n",
    "    acc = metrics.accuracy_score(test_labels, preds)\n",
    "    return (clf, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_w2v = runMod(LinearSVC, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Do the same thing but with pretrained embeddings\n",
    "\n",
    "Now pull down the Google News word embeddings and do the same thing. Compare the results. Why was one better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_goog = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "w2v_goog = {w: vec for w, vec in zip(model_goog.wv.index2word, model_goog.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_goog = runMod(LinearSVC, w2v_goog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of W2V: 0.107\n",
      "Accuracy of Google: W2V: 0.20066666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of W2V: {}'.format(svc_w2v[1]))\n",
    "print('Accuracy of Google: W2V: {}'.format(svc_goog[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google Word2Vec model is slightly better given that the \n",
    "model was trained on a much bigger corpus, that possibly includes \n",
    "orders of words in the test dataset that doesn't exist in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Build a neural net model using word2vec embeddings (both pretrained and within an Embedding layer from Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_word_dict(model):\n",
    "    mapdict = {}\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        word = model.wv.index2word[i]\n",
    "        mapdict[word] = i\n",
    "    return mapdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed_matrix(model, all_words=all_words):\n",
    "    \"Create a weight matrix for words\"\n",
    "    vocab_size = len(all_words)\n",
    "    embedding_matrix = np.zeros((vocab_size, model.vector_size))\n",
    "    n = 0\n",
    "    word_list = list(all_words)\n",
    "    for i in range(vocab_size):\n",
    "        word = word_list[i]\n",
    "        if word in model.wv.vocab:\n",
    "            embedding_vector = model.wv[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[n] = embedding_vector\n",
    "                n += 1\n",
    "\n",
    "    return embedding_matrix[:n, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X datasets from words to numbers\n",
    "def convert_word_to_num(word_mat, model, max_len = None):\n",
    "    new_mat = []\n",
    "    for review in word_mat:\n",
    "        tmp = []\n",
    "        for w in review:\n",
    "            if w in model.wv.vocab:\n",
    "                tmp.append(model.wv.vocab[w].index + 1)\n",
    "            else:\n",
    "                tmp.append(0)\n",
    "        new_mat.append(tmp)\n",
    "\n",
    "    return pad_sequences(new_mat, padding='post', maxlen=max_len)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_keras_model1(gensim_model, batch_size, \n",
    "                     create_emebed_mat = False):\n",
    "    x_train = convert_word_to_num(X_train, gensim_model)\n",
    "    x_test = convert_word_to_num(X_test, gensim_model, \n",
    "                                 max_len=x_train.shape[1])\n",
    "    \n",
    "    # For embedding layer\n",
    "    if create_emebed_mat:\n",
    "        embed_matrix = create_embed_matrix(gensim_model)\n",
    "        vocab_size = embed_matrix.shape[0]\n",
    "        e = Embedding(vocab_size, gensim_model.vector_size,\n",
    "                      weights=[embed_matrix],\n",
    "                     input_length=x_train.shape[1], trainable=False)\n",
    "    else:\n",
    "        e = gensim_model.wv.get_keras_embedding()\n",
    "        e.input_length = x_train.shape[1]    \n",
    "    \n",
    "    # define model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # e = Embedding(vocab_size, embed_size, weights=[embed_matrix], \n",
    "    #               input_length=maxlen, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    # fit the model\n",
    "    model.fit(x_train, train_labels,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, test_labels))\n",
    "\n",
    "    score, acc = model.evaluate(x_test, test_labels,\n",
    "                                batch_size=batch_size)\n",
    "    return (score, acc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 3469, 200)         5205400   \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 693800)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                6938010   \n",
      "=================================================================\n",
      "Total params: 12,143,410\n",
      "Trainable params: 6,938,010\n",
      "Non-trainable params: 5,205,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 14s 2ms/step - loss: 3.9507 - acc: 0.2547 - val_loss: 3.9078 - val_acc: 0.2457\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 14s 2ms/step - loss: 3.8563 - acc: 0.2787 - val_loss: 3.9237 - val_acc: 0.3203\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 14s 2ms/step - loss: 3.7755 - acc: 0.3003 - val_loss: 3.9905 - val_acc: 0.2717\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 14s 2ms/step - loss: 3.7161 - acc: 0.3256 - val_loss: 3.9924 - val_acc: 0.2457\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 14s 2ms/step - loss: 3.6363 - acc: 0.3433 - val_loss: 3.9266 - val_acc: 0.2523\n",
      "3000/3000 [==============================] - 2s 557us/step\n"
     ]
    }
   ],
   "source": [
    "keras_w2v = run_keras_model1(model, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3469, 300)         13400100  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1040700)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10407010  \n",
      "=================================================================\n",
      "Total params: 23,807,110\n",
      "Trainable params: 10,407,010\n",
      "Non-trainable params: 13,400,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 20s 3ms/step - loss: 27.0113 - acc: 0.0141 - val_loss: 26.9602 - val_acc: 0.0110\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 20s 3ms/step - loss: 27.1682 - acc: 0.0111 - val_loss: 26.9602 - val_acc: 0.0110\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 19s 3ms/step - loss: 27.1682 - acc: 0.0111 - val_loss: 26.9602 - val_acc: 0.0110\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 19s 3ms/step - loss: 27.1682 - acc: 0.0111 - val_loss: 26.9602 - val_acc: 0.0110\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 20s 3ms/step - loss: 27.1682 - acc: 0.0111 - val_loss: 26.9602 - val_acc: 0.0110\n",
      "3000/3000 [==============================] - 2s 767us/step\n"
     ]
    }
   ],
   "source": [
    "keras_goog = run_keras_model1(model_goog, 32, create_emebed_mat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Change the architecture of your model and compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_keras_model2(gensim_model, batch_size, \n",
    "                     create_emebed_mat = False):\n",
    "    x_train = convert_word_to_num(X_train, gensim_model)\n",
    "    x_test = convert_word_to_num(X_test, gensim_model, \n",
    "                                 max_len=x_train.shape[1])\n",
    "    \n",
    "    # For embedding layer\n",
    "    if create_emebed_mat:\n",
    "        embed_matrix = create_embed_matrix(gensim_model)\n",
    "        vocab_size = embed_matrix.shape[0]\n",
    "        e = Embedding(vocab_size, gensim_model.vector_size,\n",
    "                      weights=[embed_matrix],\n",
    "                     input_length=x_train.shape[1], trainable=False)\n",
    "    else:\n",
    "        e = gensim_model.wv.get_keras_embedding()\n",
    "        e.input_length = x_train.shape[1]    \n",
    "    \n",
    "    # define model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # e = Embedding(vocab_size, embed_size, weights=[embed_matrix], \n",
    "    #               input_length=maxlen, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(LSTM(gensim_model.vector_size, \n",
    "                   dropout=.3, recurrent_dropout=.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    # fit the model\n",
    "    model.fit(x_train, train_labels,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, test_labels))\n",
    "\n",
    "    score, acc = model.evaluate(x_test, test_labels,\n",
    "                                batch_size=batch_size)\n",
    "    return (score, acc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 3469, 200)         5205400   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 5,536,760\n",
      "Trainable params: 331,360\n",
      "Non-trainable params: 5,205,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      " 128/7000 [..............................] - ETA: 39:38 - loss: 4.4869 - acc: 0.1719"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ea8925153607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras_w2v_mod2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_keras_model2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-fc85336762fd>\u001b[0m in \u001b[0;36mrun_keras_model2\u001b[1;34m(gensim_model, batch_size, create_emebed_mat)\u001b[0m\n\u001b[0;32m     34\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m               validation_data=(x_test, test_labels))\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     score, acc = model.evaluate(x_test, test_labels,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "keras_w2v_mod2 = run_keras_model2(model, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_goog_mod2 = run_keras_model2(model_goog, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: For each model, do an error evaluation\n",
    "\n",
    "You now have a bunch of classifiers. For each classifier, pick 2 good classifications and 2 bad classifications. Print the expected and predicted label, and also print the movie synopsis. From these results, can you spot some systematic errors from your models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
