{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import calendar\n",
    "\n",
    "# Prepocessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Concatenate, TimeDistributed\n",
    "from keras.layers import Dense, Bidirectional, Dropout, Flatten, merge \n",
    "from keras.layers import Conv1D, Conv2D, MaxPool1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '../data'\n",
    "reutersFile = 'news_reuters.csv'\n",
    "stockFile = 'stockReturns.json'\n",
    "\n",
    "rawX = pd.read_csv(os.path.join(dataPath, reutersFile), header=None, \n",
    "                   names=['ticker', 'company', 'pub_date', 'headline', 'first_sent', 'category'])\n",
    "rawY = pd.read_json(os.path.join(dataPath, stockFile))\n",
    "# rawY = json.load(os.path.join(dataPath, stockFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_y_data(data, tickerType='mid'):\n",
    "    \"\"\"Convert stock data into binary postive/negative\"\"\"\n",
    "    tmp = data[tickerType].apply(pd.Series)\n",
    "    tmp = tmp.stack().rename('price', inplace=True).reset_index()\n",
    "    tmp['y'] = np.where(tmp['price'] >= 0, 1, 0)\n",
    "    tmp.rename(columns={'level_0': 'ticker', 'level_1': 'pub_date'}, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def clean_and_merge_data(X, Y):\n",
    "    \"\"\"Filter X to only those tickers with stock data\"\"\"\n",
    "    y_tickers = set(Y['ticker'])\n",
    "    X = X.loc[X['ticker'].isin(y_tickers)]\n",
    "    # Make sure data types are the same for merge    \n",
    "    Y['pub_date'] = Y['pub_date'].astype(rawX['pub_date'].dtype)\n",
    "    Y['ticker'] = Y['ticker'].astype(rawX['ticker'].dtype)\n",
    "    return X.merge(Y, on=['ticker', 'pub_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanY = reformat_y_data(rawY, 'short')\n",
    "\n",
    "merged = clean_and_merge_data(rawX, cleanY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up text columns and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    \"\"\"Clean up text data by:\n",
    "    \n",
    "    1. Replacing double spaces into a single space\n",
    "    2. Replace U.S. to United States so U won't get deleted with next \n",
    "       replacement\n",
    "    3. Remove all capitalized words at the beginning of the \n",
    "       sentence, since those are mostly places (aka NEW YORK)\n",
    "    4. Remove unnecessary punctuation (hyphens and asterisks)\n",
    "    5. Remove dates\n",
    "    \"\"\"\n",
    "    monthStrings = list(calendar.month_name)[1:] + list(calendar.month_abbr)[1:]\n",
    "    monthPattern = '|'.join(monthStrings)\n",
    "    \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    sent = re.sub(r'U.S.', 'United States', sent)\n",
    "    sent = re.sub(r'^(\\W?[A-Z\\s\\d]+\\b-?)', '', sent)\n",
    "    sent = re.sub(r'^ ?\\W ', '', sent)\n",
    "    sent = re.sub(r'({}) \\d+'.format(monthPattern), '', sent)\n",
    "    \n",
    "    # replace double spaces one more time after previous cleaning \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    return sent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(col):\n",
    "    \"\"\"Tokenize string into a sequence of words\"\"\"\n",
    "    return [text_to_word_sequence(text, lower=False) for text in col]\n",
    "\n",
    "def filt_to_one(x, random_state=10):\n",
    "    \"\"\"Filter dataset so that there is only one observation per day.\n",
    "    \n",
    "    If there is more than one record, will use the topStory record\n",
    "    if one exists.  If one doesn't or there are 2 topStory records\n",
    "    then it will randomly select one of the observations.\n",
    "    \"\"\"\n",
    "    if x.shape[0] > 1:\n",
    "        if 'topStory' in x['category'].unique():\n",
    "            x = x.loc[x['category'] == 'topStory']\n",
    "        if x.shape[0] > 1:\n",
    "            x = x.sample(n=1, random_state=random_state)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up text\n",
    "merged['headline'] = merged.headline.apply(clean_text)\n",
    "merged['first_sent'] = merged.first_sent.apply(clean_text)\n",
    "\n",
    "# Turn sentences into tokens\n",
    "merged['headline_token'] = tokenize_sent(merged.headline)\n",
    "merged['first_sent_token'] = tokenize_sent(merged.first_sent)\n",
    "\n",
    "# Get one record per company/day\n",
    "finalData = merged.groupby(by=['ticker', 'pub_date']).apply(filt_to_one)\n",
    "\n",
    "# Combine Headline and First Sentence into one text \n",
    "finalData['final_text'] = finalData['headline_token'] + finalData.first_sent_token\n",
    "\n",
    "# Remove observations with missing stock price\n",
    "finalData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "train, test = train_test_split(finalData, test_size = .2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Lexicon and Transform Data to Integers for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lexiconTransformer():\n",
    "    \"\"\"Create a lexicon and transform sentences and\n",
    "       to indexes for use in the model.\"\"\"\n",
    "    \n",
    "    def __init__(self, words_min_freq = 1, unknown_word_token = u'<UNK>',\n",
    "                 savePath='models', saveName='stock_word_lexicon'):\n",
    "        self.words_min_freq = words_min_freq\n",
    "        self.words_lexicon = None\n",
    "        self.unknown_word_token = unknown_word_token\n",
    "        self.indx_to_words_dict = None\n",
    "        self.savePath = savePath\n",
    "        self.saveName = saveName + '.pkl'\n",
    "    \n",
    "    def fit(self, sents):\n",
    "        \"\"\"Create lexicon based on sentences\"\"\"\n",
    "        self.make_words_lexicon(sents)        \n",
    "        self.make_lexicon_reverse()\n",
    "        self.save_lexicon()\n",
    "                \n",
    "    def transform(self, sents):\n",
    "        sents_indxs = self.tokens_to_idxs(sents, self.words_lexicon)\n",
    "        return sents_indxs\n",
    "\n",
    "    def fit_transform(self, sents):\n",
    "        self.fit(sents)\n",
    "        return self.transform(sents)\n",
    "        \n",
    "    def make_words_lexicon(self, sents_token):\n",
    "        \"\"\"Wrapper for words lexicon\"\"\"\n",
    "        self.words_lexicon = self.make_lexicon(sents_token, self.words_min_freq,\n",
    "                                               self.unknown_word_token)\n",
    "\n",
    "    def make_lexicon(self, token_seqs, min_freq=1, unknown = u'<UNK>'):\n",
    "        \"\"\"Create lexicon from input based on a frequency\n",
    "\n",
    "            Parameters:\n",
    "            \n",
    "            token_seqs\n",
    "            ----------\n",
    "               A list of a list of input tokens that will be used to create the lexicon\n",
    "            \n",
    "            min_freq\n",
    "            --------\n",
    "               Number of times the token needs to be in the corpus to be included in the\n",
    "               lexicon.  Otherwise, will be replaced with the \"unknown\" entry\n",
    "            \n",
    "            unknown\n",
    "            -------\n",
    "               The word in the lexicon that should be used for tokens not existing in lexicon.\n",
    "               This can be a value that already exists in input list.  For instance, in \n",
    "               Named Entity Recognition, a value of \"other\" or \"O\" may already be a tag \n",
    "               and so having \"other\" and \"unknown\" are the same thing!\n",
    "        \"\"\"\n",
    "        # Count how often each word appears in the text.\n",
    "        token_counts = {}\n",
    "        for seq in token_seqs:\n",
    "            for token in seq:\n",
    "                if token in token_counts:\n",
    "                    token_counts[token] += 1\n",
    "                else:\n",
    "                    token_counts[token] = 1\n",
    "\n",
    "        # Then, assign each word to a numerical index. \n",
    "        # Filter words that occur less than min_freq times.\n",
    "        lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "        \n",
    "        # Have to delete unknown value from token list so not a gap in lexicon values when\n",
    "        # turning it into a lexicon (aka, if unknown == OTHER and that is the 7th value, \n",
    "        # then 7 won't exist in the lexicon which may cause issues)\n",
    "        if unknown in lexicon:\n",
    "            lexicon.remove(unknown)\n",
    "\n",
    "        # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "        lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "        \n",
    "        lexicon[unknown] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "        lexicon_size = len(lexicon)\n",
    "        return lexicon\n",
    "    \n",
    "    def save_lexicon(self):\n",
    "        \"Save lexicons by pickling them\"\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "        with open(os.path.join(self.savePath, self.saveName), 'wb') as f:\n",
    "            pickle.dump(self.words_lexicon, f)\n",
    "                        \n",
    "    def load_lexicon(self):\n",
    "        with open(os.path.join(self.savePath, self.saveName), 'rb') as f:\n",
    "            self.words_lexicon = pickle.load(f)\n",
    "                    \n",
    "        self.make_lexicon_reverse()\n",
    "        \n",
    "    def make_lexicon_reverse(self):\n",
    "        self.indx_to_words_dict = self.get_lexicon_lookup(self.words_lexicon)\n",
    "    \n",
    "    def get_lexicon_lookup(self, lexicon):\n",
    "        '''Make a dictionary where the string representation of \n",
    "           a lexicon item can be retrieved from its numerical index'''\n",
    "        lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "        return lexicon_lookup\n",
    "    \n",
    "    def tokens_to_idxs(self, token_seqs, lexicon):\n",
    "        \"\"\"Transform tokens to numeric indexes or <UNK> if doesn't exist\"\"\"\n",
    "        idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in token_seq] for token_seq in token_seqs]\n",
    "        return idx_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = lexiconTransformer(words_min_freq=2)\n",
    "\n",
    "lexicon.fit(train['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train['finalText_indx'] = lexicon.transform(train['final_text'])\n",
    "test['finalText_indx'] = lexicon.transform(test['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_seq_len(sents):\n",
    "    return max([len(idx_seq) for idx_seq in sents])\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of longest sequence\n",
    "max_seq_len = get_max_seq_len(train['finalText_indx'])\n",
    "\n",
    "#Add one to max length for offsetting sequence by 1\n",
    "train_padded_words = pad_idx_seqs(train['finalText_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "test_padded_words = pad_idx_seqs(test['finalText_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "train_y = to_categorical(train['y'])\n",
    "test_y = to_categorical(test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 2\n",
    "nb_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed_matrix(model, lexicon, embed_size):\n",
    "    \"Create a weight matrix for words\"\n",
    "    vocab_size = len(lexicon)\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_size))\n",
    "    n = 0\n",
    "    word_list = list(lexicon)\n",
    "    for i in range(vocab_size):\n",
    "        word = word_list[i]\n",
    "        if word in model.wv.vocab:\n",
    "            embedding_vector = model.wv[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[n] = embedding_vector[:embed_size]\n",
    "                n += 1\n",
    "\n",
    "    return embedding_matrix[:n, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "word_embed_len = 200\n",
    "word_embed_matrix = create_embed_matrix(w2v, lexicon.words_lexicon, \n",
    "                                   word_embed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(seq_input_len, embed_matrix, \n",
    "                     n_RNN_nodes, n_dense_nodes, \n",
    "                     recurrent_dropout=0.2, \n",
    "                     drop_out=.2, n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(seq_input_len,), name='word_input_layer')\n",
    "        \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    hidden_layer1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n",
    "                                      recurrent_dropout=recurrent_dropout, \n",
    "                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n",
    "    \n",
    "    hidden_layer2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      dropout=drop_out, name='hidden_layer2'))(hidden_layer1)\n",
    "\n",
    "    dense_layer = Dense(units=n_dense_nodes, activation='relu', name='dense_layer')(hidden_layer2)\n",
    "\n",
    "    drop_out3 = Dropout(drop_out)(dense_layer)\n",
    "\n",
    "    output_layer = Dense(units=n_out, activation='softmax',\n",
    "                         name='output_layer')(drop_out3)\n",
    "\n",
    "    model = Model(inputs=[word_input], outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = create_rnn_model(seq_input_len=train_padded_words.shape[-1],\n",
    "                             embed_matrix=word_embed_matrix, \n",
    "                             recurrent_dropout=.4, drop_out=.5,\n",
    "                             n_RNN_nodes=500, n_dense_nodes=500, n_out=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(model, x_train, y_train, x_test, y_test, \n",
    "                         modelSaveName, modelSavePath='models',\n",
    "                         batch_size=128, epochs=3, validation_split=.1):\n",
    "    \"\"\"Train model, save weights, and predict data\"\"\"\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    filepath = os.path.join(modelSavePath, modelSaveName + '.hdf5')\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(x=x_train, y=y_train, batch_size=batch_size, \n",
    "              epochs=epochs, validation_split=validation_split, \n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    score, acc, rec, prec = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    return (model, acc, rec, prec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input_layer (InputLayer (None, 102)               0         \n",
      "_________________________________________________________________\n",
      "word_embedding_layer (Embedd (None, 102, 200)          2356000   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 102, 1000)         2804000   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 1000)              6004000   \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 11,665,502\n",
      "Trainable params: 11,665,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 62s 8ms/step - loss: 0.6994 - acc: 0.5026 - recall: 0.5026 - precision: 0.5026 - val_loss: 0.6975 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00001: saving model to models/rnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.6952 - acc: 0.5210 - recall: 0.5210 - precision: 0.5210 - val_loss: 0.7110 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00002: saving model to models/rnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.6904 - acc: 0.5448 - recall: 0.5448 - precision: 0.5448 - val_loss: 0.6977 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00003: saving model to models/rnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.6634 - acc: 0.5999 - recall: 0.5999 - precision: 0.5999 - val_loss: 0.7040 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00004: saving model to models/rnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.6020 - acc: 0.6762 - recall: 0.6762 - precision: 0.6762 - val_loss: 0.7272 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00005: saving model to models/rnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.5294 - acc: 0.7365 - recall: 0.7365 - precision: 0.7365 - val_loss: 0.8485 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00006: saving model to models/rnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.4364 - acc: 0.7994 - recall: 0.7994 - precision: 0.7994 - val_loss: 0.9269 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00007: saving model to models/rnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.3792 - acc: 0.8287 - recall: 0.8287 - precision: 0.8287 - val_loss: 0.9821 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00008: saving model to models/rnn_model.hdf5\n",
      "Epoch 9/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.3073 - acc: 0.8705 - recall: 0.8705 - precision: 0.8705 - val_loss: 1.2328 - val_acc: 0.5351 - val_recall: 0.5351 - val_precision: 0.5351\n",
      "\n",
      "Epoch 00009: saving model to models/rnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.2629 - acc: 0.8909 - recall: 0.8909 - precision: 0.8909 - val_loss: 1.2489 - val_acc: 0.5329 - val_recall: 0.5329 - val_precision: 0.5329\n",
      "\n",
      "Epoch 00010: saving model to models/rnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.2352 - acc: 0.9057 - recall: 0.9057 - precision: 0.9057 - val_loss: 1.4071 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00011: saving model to models/rnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.2052 - acc: 0.9155 - recall: 0.9155 - precision: 0.9155 - val_loss: 1.5470 - val_acc: 0.5285 - val_recall: 0.5285 - val_precision: 0.5285\n",
      "\n",
      "Epoch 00012: saving model to models/rnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1779 - acc: 0.9298 - recall: 0.9298 - precision: 0.9298 - val_loss: 1.5011 - val_acc: 0.5241 - val_recall: 0.5241 - val_precision: 0.5241\n",
      "\n",
      "Epoch 00013: saving model to models/rnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1547 - acc: 0.9360 - recall: 0.9360 - precision: 0.9360 - val_loss: 1.9286 - val_acc: 0.5044 - val_recall: 0.5044 - val_precision: 0.5044\n",
      "\n",
      "Epoch 00014: saving model to models/rnn_model.hdf5\n",
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1589 - acc: 0.9372 - recall: 0.9372 - precision: 0.9372 - val_loss: 1.6641 - val_acc: 0.5384 - val_recall: 0.5384 - val_precision: 0.5384\n",
      "\n",
      "Epoch 00015: saving model to models/rnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1337 - acc: 0.9485 - recall: 0.9485 - precision: 0.9485 - val_loss: 2.0149 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00016: saving model to models/rnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1275 - acc: 0.9495 - recall: 0.9495 - precision: 0.9495 - val_loss: 1.6864 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00017: saving model to models/rnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1217 - acc: 0.9494 - recall: 0.9494 - precision: 0.9494 - val_loss: 1.7823 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00018: saving model to models/rnn_model.hdf5\n",
      "Epoch 19/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1185 - acc: 0.9522 - recall: 0.9522 - precision: 0.9522 - val_loss: 2.0152 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00019: saving model to models/rnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1027 - acc: 0.9587 - recall: 0.9587 - precision: 0.9587 - val_loss: 1.9572 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00020: saving model to models/rnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.1012 - acc: 0.9557 - recall: 0.9557 - precision: 0.9557 - val_loss: 2.0952 - val_acc: 0.5274 - val_recall: 0.5274 - val_precision: 0.5274\n",
      "\n",
      "Epoch 00021: saving model to models/rnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0912 - acc: 0.9634 - recall: 0.9634 - precision: 0.9634 - val_loss: 1.9485 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00022: saving model to models/rnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0892 - acc: 0.9618 - recall: 0.9618 - precision: 0.9618 - val_loss: 2.2483 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00023: saving model to models/rnn_model.hdf5\n",
      "Epoch 24/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0845 - acc: 0.9628 - recall: 0.9628 - precision: 0.9628 - val_loss: 2.2554 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00024: saving model to models/rnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0798 - acc: 0.9655 - recall: 0.9655 - precision: 0.9655 - val_loss: 2.5355 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00025: saving model to models/rnn_model.hdf5\n",
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0755 - acc: 0.9670 - recall: 0.9670 - precision: 0.9670 - val_loss: 2.4073 - val_acc: 0.5274 - val_recall: 0.5274 - val_precision: 0.5274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: saving model to models/rnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0785 - acc: 0.9632 - recall: 0.9632 - precision: 0.9632 - val_loss: 2.6411 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00027: saving model to models/rnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0729 - acc: 0.9663 - recall: 0.9663 - precision: 0.9663 - val_loss: 2.2929 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00028: saving model to models/rnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0682 - acc: 0.9673 - recall: 0.9673 - precision: 0.9673 - val_loss: 2.8222 - val_acc: 0.5340 - val_recall: 0.5340 - val_precision: 0.5340\n",
      "\n",
      "Epoch 00029: saving model to models/rnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0664 - acc: 0.9683 - recall: 0.9683 - precision: 0.9683 - val_loss: 2.8011 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00030: saving model to models/rnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0616 - acc: 0.9682 - recall: 0.9682 - precision: 0.9682 - val_loss: 3.0225 - val_acc: 0.5274 - val_recall: 0.5274 - val_precision: 0.5274\n",
      "\n",
      "Epoch 00031: saving model to models/rnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0670 - acc: 0.9665 - recall: 0.9665 - precision: 0.9665 - val_loss: 2.8879 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00032: saving model to models/rnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0623 - acc: 0.9668 - recall: 0.9668 - precision: 0.9668 - val_loss: 3.0470 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00033: saving model to models/rnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0561 - acc: 0.9712 - recall: 0.9712 - precision: 0.9712 - val_loss: 3.6154 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00034: saving model to models/rnn_model.hdf5\n",
      "Epoch 35/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0595 - acc: 0.9702 - recall: 0.9702 - precision: 0.9702 - val_loss: 3.3959 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00035: saving model to models/rnn_model.hdf5\n",
      "Epoch 36/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0560 - acc: 0.9710 - recall: 0.9710 - precision: 0.9710 - val_loss: 3.2664 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00036: saving model to models/rnn_model.hdf5\n",
      "Epoch 37/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0579 - acc: 0.9704 - recall: 0.9704 - precision: 0.9704 - val_loss: 2.9882 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00037: saving model to models/rnn_model.hdf5\n",
      "Epoch 38/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0550 - acc: 0.9711 - recall: 0.9711 - precision: 0.9711 - val_loss: 3.4713 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00038: saving model to models/rnn_model.hdf5\n",
      "Epoch 39/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0532 - acc: 0.9716 - recall: 0.9716 - precision: 0.9716 - val_loss: 3.5460 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00039: saving model to models/rnn_model.hdf5\n",
      "Epoch 40/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0506 - acc: 0.9720 - recall: 0.9720 - precision: 0.9720 - val_loss: 3.4744 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00040: saving model to models/rnn_model.hdf5\n",
      "Epoch 41/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0494 - acc: 0.9726 - recall: 0.9726 - precision: 0.9726 - val_loss: 3.1607 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00041: saving model to models/rnn_model.hdf5\n",
      "Epoch 42/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0513 - acc: 0.9718 - recall: 0.9718 - precision: 0.9718 - val_loss: 4.0909 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00042: saving model to models/rnn_model.hdf5\n",
      "Epoch 43/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0505 - acc: 0.9705 - recall: 0.9705 - precision: 0.9705 - val_loss: 3.5233 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00043: saving model to models/rnn_model.hdf5\n",
      "Epoch 44/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0517 - acc: 0.9743 - recall: 0.9743 - precision: 0.9743 - val_loss: 4.0674 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00044: saving model to models/rnn_model.hdf5\n",
      "Epoch 45/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0502 - acc: 0.9734 - recall: 0.9734 - precision: 0.9734 - val_loss: 3.6478 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00045: saving model to models/rnn_model.hdf5\n",
      "Epoch 46/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0495 - acc: 0.9699 - recall: 0.9699 - precision: 0.9699 - val_loss: 3.8573 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00046: saving model to models/rnn_model.hdf5\n",
      "Epoch 47/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0467 - acc: 0.9724 - recall: 0.9724 - precision: 0.9724 - val_loss: 4.1325 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00047: saving model to models/rnn_model.hdf5\n",
      "Epoch 48/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0441 - acc: 0.9739 - recall: 0.9739 - precision: 0.9739 - val_loss: 4.5634 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00048: saving model to models/rnn_model.hdf5\n",
      "Epoch 49/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0527 - acc: 0.9718 - recall: 0.9718 - precision: 0.9718 - val_loss: 3.5749 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00049: saving model to models/rnn_model.hdf5\n",
      "Epoch 50/50\n",
      "8200/8200 [==============================] - 58s 7ms/step - loss: 0.0578 - acc: 0.9699 - recall: 0.9699 - precision: 0.9699 - val_loss: 3.8950 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00050: saving model to models/rnn_model.hdf5\n",
      "2278/2278 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "rnn_res = train_and_test_model(rnn_model, train_padded_words, train_y, \n",
    "                               test_padded_words, test_y, 'rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.engine.training.Model at 0x7f2777c9c5f8>,\n",
       " 0.5267778764805111,\n",
       " 0.5267778764805111,\n",
       " 0.5267778764805111)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sentences(data, lexicon, maxlen=200):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in sentences]\n",
    "        x2 = np.eye(len(char_indices) + 1)[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "def create_cnn_model(char_maxlen, vocab_size,\n",
    "                     nb_filter=100, filter_kernels = [4] * 4,\n",
    "                     pool_size=3, n_dense_nodes=100,\n",
    "                     drop_out=.2, n_out=2):\n",
    "\n",
    "    inputs = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "\n",
    "    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                  padding='valid', activation='relu',\n",
    "                  input_shape=(char_maxlen, vocab_size))(inputs)\n",
    "    \n",
    "    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n",
    "\n",
    "    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                          padding='valid', activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n",
    "\n",
    "    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                          padding='valid', activation='relu')(maxpool2)\n",
    "\n",
    "    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                          padding='valid', activation='relu')(conv3)\n",
    "\n",
    "    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n",
    "    flatten = Flatten()(maxpool3)\n",
    "\n",
    "    dense_layer = Dense(n_dense_nodes, activation='relu')(flatten)\n",
    "    dropout = Dropout(drop_out)(dense_layer)\n",
    "\n",
    "    output_layer = Dense(n_out, activation='softmax', name='output')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_maxlen = 1024 \n",
    "nb_filter = 128\n",
    "dense_outputs = 1024\n",
    "filter_kernels = [7, 5, 5, 3]\n",
    "pool_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 86\n"
     ]
    }
   ],
   "source": [
    "# Turn all tokens into one string and then all obs \n",
    "# into one overall string\n",
    "trainTokensAsString = train.final_text.apply(lambda x: ' '.join(x))\n",
    "testTokensAsString = test.final_text.apply(lambda x: ' '.join(x))\n",
    "oneTxt = ' '.join(trainTokensAsString)\n",
    "\n",
    "# Get info about characters\n",
    "chars = set(oneTxt)\n",
    "vocab_size = len(chars) + 1\n",
    "print('total chars:', vocab_size)\n",
    "char_indices = dict((c, i + 2) for i, c in enumerate(chars))\n",
    "indices_char = dict((i + 2, c) for i, c in enumerate(chars))\n",
    "\n",
    "char_indices['<UNK>'] = 1\n",
    "indices_char[1] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCharData = vectorize_sentences(trainTokensAsString, char_indices, char_maxlen)\n",
    "testCharData = vectorize_sentences(testTokensAsString, char_indices, char_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = create_cnn_model(char_maxlen=char_maxlen, \n",
    "                             vocab_size=vocab_size,\n",
    "                             nb_filter=nb_filter, \n",
    "                             filter_kernels=filter_kernels,\n",
    "                             pool_size=pool_size, \n",
    "                             n_dense_nodes=dense_outputs,\n",
    "                             drop_out=.5, \n",
    "                             n_out=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_input_layer (InputLayer (None, 1024, 86)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1018, 128)         77184     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 203, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 199, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 33, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              787456    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 1,080,066\n",
      "Trainable params: 1,080,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6946 - acc: 0.4940 - recall: 0.4935 - precision: 0.4939 - val_loss: 0.6945 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00001: saving model to models/cnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 7s 850us/step - loss: 0.6934 - acc: 0.5034 - recall: 0.5034 - precision: 0.5034 - val_loss: 0.6931 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00002: saving model to models/cnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 7s 853us/step - loss: 0.6935 - acc: 0.4955 - recall: 0.4955 - precision: 0.4955 - val_loss: 0.6931 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00003: saving model to models/cnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 7s 863us/step - loss: 0.6933 - acc: 0.4941 - recall: 0.4941 - precision: 0.4941 - val_loss: 0.6930 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00004: saving model to models/cnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 7s 861us/step - loss: 0.6933 - acc: 0.4923 - recall: 0.4923 - precision: 0.4923 - val_loss: 0.6932 - val_acc: 0.4825 - val_recall: 0.4825 - val_precision: 0.4825\n",
      "\n",
      "Epoch 00005: saving model to models/cnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 7s 860us/step - loss: 0.6934 - acc: 0.4978 - recall: 0.4978 - precision: 0.4978 - val_loss: 0.6933 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00006: saving model to models/cnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 7s 849us/step - loss: 0.6931 - acc: 0.4963 - recall: 0.4963 - precision: 0.4963 - val_loss: 0.6941 - val_acc: 0.5033 - val_recall: 0.5033 - val_precision: 0.5033\n",
      "\n",
      "Epoch 00007: saving model to models/cnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 7s 854us/step - loss: 0.6887 - acc: 0.5313 - recall: 0.5313 - precision: 0.5313 - val_loss: 0.6940 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00008: saving model to models/cnn_model.hdf5\n",
      "Epoch 9/50\n",
      "8200/8200 [==============================] - 7s 859us/step - loss: 0.6607 - acc: 0.5843 - recall: 0.5843 - precision: 0.5843 - val_loss: 0.7145 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00009: saving model to models/cnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 7s 863us/step - loss: 0.5845 - acc: 0.6755 - recall: 0.6755 - precision: 0.6755 - val_loss: 0.7746 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00010: saving model to models/cnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 7s 857us/step - loss: 0.5071 - acc: 0.7309 - recall: 0.7309 - precision: 0.7309 - val_loss: 0.8867 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00011: saving model to models/cnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 7s 858us/step - loss: 0.4275 - acc: 0.7802 - recall: 0.7802 - precision: 0.7802 - val_loss: 0.9770 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00012: saving model to models/cnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 7s 860us/step - loss: 0.3568 - acc: 0.8130 - recall: 0.8130 - precision: 0.8130 - val_loss: 1.1436 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00013: saving model to models/cnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 7s 856us/step - loss: 0.3228 - acc: 0.8328 - recall: 0.8328 - precision: 0.8328 - val_loss: 1.3252 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00014: saving model to models/cnn_model.hdf5\n",
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 7s 851us/step - loss: 0.2856 - acc: 0.8517 - recall: 0.8517 - precision: 0.8517 - val_loss: 1.3480 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00015: saving model to models/cnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 7s 858us/step - loss: 0.2623 - acc: 0.8649 - recall: 0.8649 - precision: 0.8649 - val_loss: 1.4324 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00016: saving model to models/cnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 7s 854us/step - loss: 0.2531 - acc: 0.8709 - recall: 0.8709 - precision: 0.8709 - val_loss: 1.3788 - val_acc: 0.5219 - val_recall: 0.5219 - val_precision: 0.5219\n",
      "\n",
      "Epoch 00017: saving model to models/cnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 7s 859us/step - loss: 0.2336 - acc: 0.8827 - recall: 0.8827 - precision: 0.8827 - val_loss: 1.4714 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00018: saving model to models/cnn_model.hdf5\n",
      "Epoch 19/50\n",
      "8200/8200 [==============================] - 7s 856us/step - loss: 0.2287 - acc: 0.8867 - recall: 0.8867 - precision: 0.8867 - val_loss: 1.4418 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00019: saving model to models/cnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 7s 857us/step - loss: 0.2112 - acc: 0.8937 - recall: 0.8937 - precision: 0.8937 - val_loss: 1.4760 - val_acc: 0.5296 - val_recall: 0.5296 - val_precision: 0.5296\n",
      "\n",
      "Epoch 00020: saving model to models/cnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 7s 863us/step - loss: 0.2033 - acc: 0.8978 - recall: 0.8978 - precision: 0.8978 - val_loss: 1.5215 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00021: saving model to models/cnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 7s 863us/step - loss: 0.1979 - acc: 0.9012 - recall: 0.9012 - precision: 0.9012 - val_loss: 1.5326 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00022: saving model to models/cnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 7s 856us/step - loss: 0.1939 - acc: 0.9083 - recall: 0.9083 - precision: 0.9083 - val_loss: 1.4568 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00023: saving model to models/cnn_model.hdf5\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200/8200 [==============================] - 7s 855us/step - loss: 0.1838 - acc: 0.9134 - recall: 0.9134 - precision: 0.9134 - val_loss: 1.6199 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00024: saving model to models/cnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 7s 864us/step - loss: 0.1842 - acc: 0.9121 - recall: 0.9121 - precision: 0.9121 - val_loss: 1.5961 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00025: saving model to models/cnn_model.hdf5\n",
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 7s 860us/step - loss: 0.1743 - acc: 0.9206 - recall: 0.9206 - precision: 0.9206 - val_loss: 1.7286 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00026: saving model to models/cnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 7s 854us/step - loss: 0.1828 - acc: 0.9201 - recall: 0.9201 - precision: 0.9201 - val_loss: 1.7248 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00027: saving model to models/cnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 7s 851us/step - loss: 0.1674 - acc: 0.9265 - recall: 0.9265 - precision: 0.9265 - val_loss: 1.7102 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00028: saving model to models/cnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 7s 855us/step - loss: 0.1579 - acc: 0.9289 - recall: 0.9289 - precision: 0.9289 - val_loss: 1.7846 - val_acc: 0.5351 - val_recall: 0.5351 - val_precision: 0.5351\n",
      "\n",
      "Epoch 00029: saving model to models/cnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 7s 858us/step - loss: 0.1531 - acc: 0.9312 - recall: 0.9312 - precision: 0.9312 - val_loss: 1.9282 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00030: saving model to models/cnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 7s 858us/step - loss: 0.1545 - acc: 0.9332 - recall: 0.9332 - precision: 0.9332 - val_loss: 2.0281 - val_acc: 0.5241 - val_recall: 0.5241 - val_precision: 0.5241\n",
      "\n",
      "Epoch 00031: saving model to models/cnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 7s 859us/step - loss: 0.1504 - acc: 0.9317 - recall: 0.9317 - precision: 0.9317 - val_loss: 2.1080 - val_acc: 0.5351 - val_recall: 0.5351 - val_precision: 0.5351\n",
      "\n",
      "Epoch 00032: saving model to models/cnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 7s 856us/step - loss: 0.1615 - acc: 0.9296 - recall: 0.9296 - precision: 0.9296 - val_loss: 1.7113 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00033: saving model to models/cnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 7s 855us/step - loss: 0.1563 - acc: 0.9302 - recall: 0.9302 - precision: 0.9302 - val_loss: 2.2324 - val_acc: 0.5285 - val_recall: 0.5285 - val_precision: 0.5285\n",
      "\n",
      "Epoch 00034: saving model to models/cnn_model.hdf5\n",
      "Epoch 35/50\n",
      "8200/8200 [==============================] - 7s 860us/step - loss: 0.1347 - acc: 0.9399 - recall: 0.9399 - precision: 0.9399 - val_loss: 2.1841 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00035: saving model to models/cnn_model.hdf5\n",
      "Epoch 36/50\n",
      "8200/8200 [==============================] - 7s 852us/step - loss: 0.1403 - acc: 0.9395 - recall: 0.9395 - precision: 0.9395 - val_loss: 2.2196 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00036: saving model to models/cnn_model.hdf5\n",
      "Epoch 37/50\n",
      "8200/8200 [==============================] - 7s 862us/step - loss: 0.1250 - acc: 0.9427 - recall: 0.9427 - precision: 0.9427 - val_loss: 2.6788 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00037: saving model to models/cnn_model.hdf5\n",
      "Epoch 38/50\n",
      "8200/8200 [==============================] - 7s 857us/step - loss: 0.1192 - acc: 0.9428 - recall: 0.9428 - precision: 0.9428 - val_loss: 2.3106 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00038: saving model to models/cnn_model.hdf5\n",
      "Epoch 39/50\n",
      "8200/8200 [==============================] - 7s 854us/step - loss: 0.1978 - acc: 0.9149 - recall: 0.9149 - precision: 0.9149 - val_loss: 2.2276 - val_acc: 0.5022 - val_recall: 0.5022 - val_precision: 0.5022\n",
      "\n",
      "Epoch 00039: saving model to models/cnn_model.hdf5\n",
      "Epoch 40/50\n",
      "8200/8200 [==============================] - 7s 855us/step - loss: 0.1285 - acc: 0.9399 - recall: 0.9399 - precision: 0.9399 - val_loss: 2.5977 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00040: saving model to models/cnn_model.hdf5\n",
      "Epoch 41/50\n",
      "8200/8200 [==============================] - 7s 857us/step - loss: 0.1088 - acc: 0.9457 - recall: 0.9457 - precision: 0.9457 - val_loss: 2.6151 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00041: saving model to models/cnn_model.hdf5\n",
      "Epoch 42/50\n",
      "8200/8200 [==============================] - 7s 855us/step - loss: 0.0927 - acc: 0.9526 - recall: 0.9526 - precision: 0.9526 - val_loss: 2.6312 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00042: saving model to models/cnn_model.hdf5\n",
      "Epoch 43/50\n",
      "8200/8200 [==============================] - 7s 852us/step - loss: 0.0851 - acc: 0.9532 - recall: 0.9532 - precision: 0.9532 - val_loss: 2.7673 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00043: saving model to models/cnn_model.hdf5\n",
      "Epoch 44/50\n",
      "8200/8200 [==============================] - 7s 852us/step - loss: 0.0792 - acc: 0.9562 - recall: 0.9562 - precision: 0.9562 - val_loss: 3.1258 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00044: saving model to models/cnn_model.hdf5\n",
      "Epoch 45/50\n",
      "8200/8200 [==============================] - 7s 861us/step - loss: 0.0797 - acc: 0.9535 - recall: 0.9535 - precision: 0.9535 - val_loss: 3.0531 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00045: saving model to models/cnn_model.hdf5\n",
      "Epoch 46/50\n",
      "8200/8200 [==============================] - 7s 857us/step - loss: 0.0729 - acc: 0.9565 - recall: 0.9565 - precision: 0.9565 - val_loss: 3.3048 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00046: saving model to models/cnn_model.hdf5\n",
      "Epoch 47/50\n",
      "8200/8200 [==============================] - 7s 859us/step - loss: 0.0733 - acc: 0.9574 - recall: 0.9574 - precision: 0.9574 - val_loss: 3.3159 - val_acc: 0.5274 - val_recall: 0.5274 - val_precision: 0.5274\n",
      "\n",
      "Epoch 00047: saving model to models/cnn_model.hdf5\n",
      "Epoch 48/50\n",
      "8200/8200 [==============================] - 7s 861us/step - loss: 0.0726 - acc: 0.9578 - recall: 0.9578 - precision: 0.9578 - val_loss: 3.3976 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00048: saving model to models/cnn_model.hdf5\n",
      "Epoch 49/50\n",
      "8200/8200 [==============================] - 7s 866us/step - loss: 0.0814 - acc: 0.9543 - recall: 0.9543 - precision: 0.9543 - val_loss: 3.4349 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00049: saving model to models/cnn_model.hdf5\n",
      "Epoch 50/50\n",
      "8200/8200 [==============================] - 7s 854us/step - loss: 0.0751 - acc: 0.9561 - recall: 0.9561 - precision: 0.9561 - val_loss: 3.7340 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00050: saving model to models/cnn_model.hdf5\n",
      "2278/2278 [==============================] - 1s 611us/step\n"
     ]
    }
   ],
   "source": [
    "cnn_res = train_and_test_model(cnn_model, trainCharData[:, :, 1:],\n",
    "                               train_y, \n",
    "                               testCharData[:, :, 1:], \n",
    "                               test_y, \n",
    "                               'cnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_rnn_model(rnn_input_len, char_maxlen, vocab_size,\n",
    "                         embed_matrix, n_RNN_nodes, \n",
    "                         nb_filter=100, filter_kernels = [4] * 4,\n",
    "                         pool_size=3, n_dense_nodes=100,\n",
    "                         recurrent_dropout=0.2, \n",
    "                         drop_out=.2, n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(rnn_input_len,), name='word_input_layer')\n",
    "    char_input = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "    \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    rnn_output1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n",
    "                                      recurrent_dropout=recurrent_dropout, \n",
    "                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n",
    "    \n",
    "    rnn_output2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      dropout=drop_out, name='hidden_layer2'))(rnn_output1)\n",
    "            \n",
    "    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                  padding='valid', activation='relu',\n",
    "                  input_shape=(char_maxlen, vocab_size))(char_input)\n",
    "\n",
    "    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n",
    "\n",
    "    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                          padding='valid', activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n",
    "\n",
    "    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                          padding='valid', activation='relu')(maxpool2)\n",
    "\n",
    "    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                          padding='valid', activation='relu')(conv3)\n",
    "\n",
    "    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n",
    "    cnn_output = Flatten()(maxpool3)\n",
    "\n",
    "    merged_layer = concatenate([cnn_output, rnn_output2])\n",
    "    \n",
    "    dense_layer1 = Dense(n_dense_nodes, activation='relu', name='dense_layer')(merged_layer)\n",
    "    drop_out1 = Dropout(drop_out)(dense_layer1)\n",
    "    dense_layer2 = Dense(n_dense_nodes, activation='relu')(drop_out1)\n",
    "    drop_out2 = Dropout(drop_out)(dense_layer2)\n",
    "    \n",
    "    main_output = Dense(n_out, activation='softmax', name='output_layer')(drop_out2)\n",
    "\n",
    "    model = Model(inputs=[word_input, char_input], outputs=[main_output])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_model = create_cnn_rnn_model(rnn_input_len=train_padded_words.shape[-1], \n",
    "                                     char_maxlen=char_maxlen, \n",
    "                                     vocab_size=vocab_size,\n",
    "                                     embed_matrix=word_embed_matrix, \n",
    "                                     n_RNN_nodes=500,\n",
    "                                     nb_filter=nb_filter, \n",
    "                                     filter_kernels=filter_kernels,\n",
    "                                     pool_size=pool_size, \n",
    "                                     n_dense_nodes=400,\n",
    "                                     recurrent_dropout=0.4, \n",
    "                                     drop_out=.5, \n",
    "                                     n_out=n_out)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input_layer (InputLayer)   (None, 1024, 86)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1018, 128)    77184       char_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 203, 128)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 199, 128)     82048       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 39, 128)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 35, 128)      82048       max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "word_input_layer (InputLayer)   (None, 102)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 33, 128)      49280       conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 102, 200)     2356000     word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 6, 128)       0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 102, 1000)    2804000     word_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 768)          0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1000)         6004000     bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1768)         0           flatten_4[0][0]                  \n",
      "                                                                 bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (Dense)             (None, 400)          707600      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 400)          0           dense_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 400)          160400      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 400)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            802         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,323,362\n",
      "Trainable params: 12,323,362\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 69s 8ms/step - loss: 0.6975 - acc: 0.5011 - recall: 0.5011 - precision: 0.5011 - val_loss: 0.6932 - val_acc: 0.5011 - val_recall: 0.5011 - val_precision: 0.5011\n",
      "\n",
      "Epoch 00001: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.6999 - acc: 0.5005 - recall: 0.5005 - precision: 0.5005 - val_loss: 0.6945 - val_acc: 0.4901 - val_recall: 0.4901 - val_precision: 0.4901\n",
      "\n",
      "Epoch 00002: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.6972 - acc: 0.5156 - recall: 0.5156 - precision: 0.5156 - val_loss: 0.6940 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00003: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.6861 - acc: 0.5532 - recall: 0.5532 - precision: 0.5532 - val_loss: 0.7121 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00004: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.6630 - acc: 0.6128 - recall: 0.6128 - precision: 0.6128 - val_loss: 0.7297 - val_acc: 0.5033 - val_recall: 0.5033 - val_precision: 0.5033\n",
      "\n",
      "Epoch 00005: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.5987 - acc: 0.6793 - recall: 0.6793 - precision: 0.6793 - val_loss: 0.7772 - val_acc: 0.4846 - val_recall: 0.4846 - val_precision: 0.4846\n",
      "\n",
      "Epoch 00006: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.5198 - acc: 0.7468 - recall: 0.7468 - precision: 0.7468 - val_loss: 0.8997 - val_acc: 0.4967 - val_recall: 0.4967 - val_precision: 0.4967\n",
      "\n",
      "Epoch 00007: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.4283 - acc: 0.8057 - recall: 0.8057 - precision: 0.8057 - val_loss: 1.0074 - val_acc: 0.4978 - val_recall: 0.4978 - val_precision: 0.4978\n",
      "\n",
      "Epoch 00008: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 9/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.3717 - acc: 0.8382 - recall: 0.8382 - precision: 0.8382 - val_loss: 0.9430 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00009: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.3089 - acc: 0.8705 - recall: 0.8705 - precision: 0.8705 - val_loss: 1.1785 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00010: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.2734 - acc: 0.8854 - recall: 0.8854 - precision: 0.8854 - val_loss: 1.3748 - val_acc: 0.5011 - val_recall: 0.5011 - val_precision: 0.5011\n",
      "\n",
      "Epoch 00011: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.2256 - acc: 0.9113 - recall: 0.9113 - precision: 0.9113 - val_loss: 1.5505 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00012: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.2034 - acc: 0.9145 - recall: 0.9145 - precision: 0.9145 - val_loss: 1.6265 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00013: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.1816 - acc: 0.9294 - recall: 0.9294 - precision: 0.9294 - val_loss: 1.5272 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00014: saving model to models/cnn_rnn_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.1600 - acc: 0.9371 - recall: 0.9371 - precision: 0.9371 - val_loss: 1.7313 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00015: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.1467 - acc: 0.9435 - recall: 0.9435 - precision: 0.9435 - val_loss: 1.9298 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00016: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.1278 - acc: 0.9489 - recall: 0.9489 - precision: 0.9489 - val_loss: 1.9052 - val_acc: 0.5044 - val_recall: 0.5044 - val_precision: 0.5044\n",
      "\n",
      "Epoch 00017: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.1439 - acc: 0.9444 - recall: 0.9444 - precision: 0.9444 - val_loss: 1.9764 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00018: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 19/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.1172 - acc: 0.9544 - recall: 0.9544 - precision: 0.9544 - val_loss: 2.2214 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00019: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.1113 - acc: 0.9543 - recall: 0.9543 - precision: 0.9543 - val_loss: 2.2406 - val_acc: 0.5033 - val_recall: 0.5033 - val_precision: 0.5033\n",
      "\n",
      "Epoch 00020: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.1034 - acc: 0.9580 - recall: 0.9580 - precision: 0.9580 - val_loss: 2.2626 - val_acc: 0.4934 - val_recall: 0.4934 - val_precision: 0.4934\n",
      "\n",
      "Epoch 00021: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0959 - acc: 0.9613 - recall: 0.9613 - precision: 0.9613 - val_loss: 2.3099 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00022: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0957 - acc: 0.9610 - recall: 0.9610 - precision: 0.9610 - val_loss: 2.6263 - val_acc: 0.5044 - val_recall: 0.5044 - val_precision: 0.5044\n",
      "\n",
      "Epoch 00023: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 24/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0847 - acc: 0.9599 - recall: 0.9599 - precision: 0.9599 - val_loss: 3.0122 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00024: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0876 - acc: 0.9598 - recall: 0.9598 - precision: 0.9598 - val_loss: 2.7583 - val_acc: 0.5055 - val_recall: 0.5055 - val_precision: 0.5055\n",
      "\n",
      "Epoch 00025: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0731 - acc: 0.9648 - recall: 0.9648 - precision: 0.9648 - val_loss: 2.9250 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00026: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0765 - acc: 0.9655 - recall: 0.9655 - precision: 0.9655 - val_loss: 2.5815 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00027: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0680 - acc: 0.9660 - recall: 0.9660 - precision: 0.9660 - val_loss: 2.9273 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00028: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0764 - acc: 0.9645 - recall: 0.9645 - precision: 0.9645 - val_loss: 2.6713 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00029: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0809 - acc: 0.9620 - recall: 0.9620 - precision: 0.9620 - val_loss: 2.8928 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00030: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0650 - acc: 0.9680 - recall: 0.9680 - precision: 0.9680 - val_loss: 3.0988 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00031: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0576 - acc: 0.9685 - recall: 0.9685 - precision: 0.9685 - val_loss: 3.4854 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00032: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0576 - acc: 0.9696 - recall: 0.9696 - precision: 0.9696 - val_loss: 3.2176 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00033: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0587 - acc: 0.9711 - recall: 0.9711 - precision: 0.9711 - val_loss: 3.5994 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00034: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 35/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0513 - acc: 0.9724 - recall: 0.9724 - precision: 0.9724 - val_loss: 3.8487 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00035: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 36/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0749 - acc: 0.9639 - recall: 0.9639 - precision: 0.9639 - val_loss: 3.3975 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00036: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 37/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0551 - acc: 0.9729 - recall: 0.9729 - precision: 0.9729 - val_loss: 3.4545 - val_acc: 0.4956 - val_recall: 0.4956 - val_precision: 0.4956\n",
      "\n",
      "Epoch 00037: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 38/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0544 - acc: 0.9693 - recall: 0.9693 - precision: 0.9693 - val_loss: 3.3196 - val_acc: 0.5022 - val_recall: 0.5022 - val_precision: 0.5022\n",
      "\n",
      "Epoch 00038: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 39/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0520 - acc: 0.9718 - recall: 0.9718 - precision: 0.9718 - val_loss: 4.1463 - val_acc: 0.4901 - val_recall: 0.4901 - val_precision: 0.4901\n",
      "\n",
      "Epoch 00039: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 40/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0537 - acc: 0.9688 - recall: 0.9688 - precision: 0.9688 - val_loss: 3.7469 - val_acc: 0.4923 - val_recall: 0.4923 - val_precision: 0.4923\n",
      "\n",
      "Epoch 00040: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 41/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0508 - acc: 0.9705 - recall: 0.9705 - precision: 0.9705 - val_loss: 4.4104 - val_acc: 0.4989 - val_recall: 0.4989 - val_precision: 0.4989\n",
      "\n",
      "Epoch 00041: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 42/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0539 - acc: 0.9693 - recall: 0.9693 - precision: 0.9693 - val_loss: 3.8272 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00042: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 43/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0519 - acc: 0.9716 - recall: 0.9716 - precision: 0.9716 - val_loss: 4.0773 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00043: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 44/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0481 - acc: 0.9741 - recall: 0.9741 - precision: 0.9741 - val_loss: 4.0907 - val_acc: 0.5022 - val_recall: 0.5022 - val_precision: 0.5022\n",
      "\n",
      "Epoch 00044: saving model to models/cnn_rnn_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0601 - acc: 0.9683 - recall: 0.9683 - precision: 0.9683 - val_loss: 3.9653 - val_acc: 0.4989 - val_recall: 0.4989 - val_precision: 0.4989\n",
      "\n",
      "Epoch 00045: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 46/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0479 - acc: 0.9746 - recall: 0.9746 - precision: 0.9746 - val_loss: 3.9959 - val_acc: 0.4923 - val_recall: 0.4923 - val_precision: 0.4923\n",
      "\n",
      "Epoch 00046: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 47/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0477 - acc: 0.9717 - recall: 0.9717 - precision: 0.9717 - val_loss: 3.7596 - val_acc: 0.4989 - val_recall: 0.4989 - val_precision: 0.4989\n",
      "\n",
      "Epoch 00047: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 48/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0488 - acc: 0.9721 - recall: 0.9721 - precision: 0.9721 - val_loss: 4.1160 - val_acc: 0.4956 - val_recall: 0.4956 - val_precision: 0.4956\n",
      "\n",
      "Epoch 00048: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 49/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0491 - acc: 0.9737 - recall: 0.9737 - precision: 0.9737 - val_loss: 3.8665 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00049: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 50/50\n",
      "8200/8200 [==============================] - 64s 8ms/step - loss: 0.0454 - acc: 0.9724 - recall: 0.9724 - precision: 0.9724 - val_loss: 4.3929 - val_acc: 0.5011 - val_recall: 0.5011 - val_precision: 0.5011\n",
      "\n",
      "Epoch 00050: saving model to models/cnn_rnn_model.hdf5\n",
      "2278/2278 [==============================] - 6s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_res = train_and_test_model(cnn_rnn_model, \n",
    "                               [train_padded_words, trainCharData[:, :, 1:]],\n",
    "                               train_y, \n",
    "                               [test_padded_words, testCharData[:, :, 1:]],\n",
    "                               test_y, \n",
    "                               'cnn_rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.engine.training.Model at 0x7f28682d5908>,\n",
       " 0.5201931517306286,\n",
       " 0.5201931517306286,\n",
       " 0.5201931517306286)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_rnn_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put results in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cnn_mod</th>\n",
       "      <td>0.532046</td>\n",
       "      <td>0.532046</td>\n",
       "      <td>0.532046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_mod</th>\n",
       "      <td>0.526778</td>\n",
       "      <td>0.526778</td>\n",
       "      <td>0.526778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_rnn_mod</th>\n",
       "      <td>0.520193</td>\n",
       "      <td>0.520193</td>\n",
       "      <td>0.520193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy    recall  precision\n",
       "cnn_mod      0.532046  0.532046   0.532046\n",
       "rnn_mod      0.526778  0.526778   0.526778\n",
       "cnn_rnn_mod  0.520193  0.520193   0.520193"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records([cnn_res[1:4], rnn_res[1:4], cnn_rnn_res[1:4]], \n",
    "                          columns=['accuracy', 'recall', 'precision'], \n",
    "                         index=['cnn_mod', 'rnn_mod', 'cnn_rnn_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classifications(classifications, classType, test_y, test_text):\n",
    "    texts = [' '.join(sent) for sent in test_text[classifications.index]]\n",
    "    stock_movements = np.where(test_y[classifications.index], 'positive', 'negative')\n",
    "    \n",
    "    print('Examples of {} predictions:\\n'.format(classType))\n",
    "    for i in range(len(texts)):\n",
    "        print('Stock movement was {}'.format(stock_movements[i]))\n",
    "        print('News info:\\n{}'.format(texts[i]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print_samples(model, modelName, test_x, test_y=test['y'], test_text = test['final_text']):\n",
    "    \"\"\"\"Print out predictions of the model\"\"\"\n",
    "    print('Stats for {} model'.format(modelName))\n",
    "    \n",
    "    res = model.predict(test_x)\n",
    "    class_res = np.apply_along_axis(np.argmax, axis=1, arr=res)\n",
    "\n",
    "    comparisons = class_res == test_y\n",
    "    good_class = comparisons.loc[comparisons == True].sample(n=3)\n",
    "    bad_class = comparisons.loc[comparisons == False].sample(n=3)\n",
    "\n",
    "    print_classifications(good_class, 'correct', test_y, test_text)\n",
    "    print_classifications(bad_class, 'INcorrect', test_y, test_text)\n",
    "\n",
    "    \n",
    "    top3MostProbPosArg = np.argsort(res[:, 1])[-3:]\n",
    "    top3Y = test_y.iloc[top3MostProbPosArg]\n",
    "    top3Probs = pd.Series(res[top3MostProbPosArg, 1], index=top3Y.index)\n",
    "    top3Data = pd.concat([top3Y, top3Probs], axis=1)\n",
    "    top3Data.columns = ['Actual', 'PositiveProb']\n",
    "    print('')\n",
    "    print('Top 3 Most Positive Probability:')\n",
    "    print(top3Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Obama corporate giants announce plan to boost suppliers President Barack Obama is enlisting several major United States and multinational companies to draw attention to an initiative aimed at helping small businesses expand and hire workers\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Lloyds RBS CEOs reassure staff on strength after Brexit vote The chief executives of Lloyds Banking Group and Royal Bank of Scotland moved to reassure thousands of workers that their state backed companies would weather the turmoil sparked by Britain's decision to quit the European Union\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "UK sells more Lloyds shares has raised over 10 bln pounds Government planning retail sale later this year Adds further details on plans for Lloyds RBS sales\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Raytheon board on adopted by laws to implement proxy access On board adopted amendment by laws to implement proxy access Source text for Eikon Further company coverage Bengaluru Newsroom 1 646 223 8780\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Viacom extends COO's employment contract through 2018 no pay rise Viacom Inc has extended the employment agreement of Chief Operating Officer Thomas Dooley through 2018 settling uncertainty over whether the executive would stay with the multi billion dollar media company 10 months before his current contract was set to expire\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "TSMC purchases equipment worth T 720 8 million Taiwan Semiconductor Manufacturing Co Ltd\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "RBS    20121012 32736     0.0           1.0\n",
      "AMG    20140325 7503      0.0           1.0\n",
      "RTN    20140305 34513     1.0           1.0\n",
      "Stats for CNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "fourth quarter profit falls on lower freight volumes No 3 United States railroad CSX Corp on Tuesday reported a lower quarterly net profit citing a drop in freight volumes and said the weak global economy and United States industrial markets would weigh on results in the coming year\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "UMC's unit orders machinery equipment from Tokyo Electron Says Xiamen unit orders machinery equipment worth T 506 2 million 16 00 million from Tokyo Electron Ltd\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Marriott revenue rises 4 pct Hotel chain Marriott International Inc reported a 4 percent rise in quarterly revenue boosted by higher room rates in North America\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "CF Industries announces start up of new ammonia plant at Donaldsonville Nitrogen Complex Industries Announces Start Up of New Ammonia Plant at Donaldsonville Nitrogen Complex Source text for Eikon Further company coverage\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Trial begins against BP over Texas refinery emissions Trial began on Wednesday in the first four of 48 000 civil lawsuits filed against BP Plc for pollution from the 460 196 barrel per day bpd refinery it owned in Texas City Texas until early this year according to court documents\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Dow S P 500 end at highs on stimulus hopes IBM The Dow and S P 500 ended at record highs on Tuesday after economic data supported views that the Federal Reserve would keep its stimulus intact for several months and IBM rallied after the company announced a stock buyback\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "MCD    20140307 29512     0.0           1.0\n",
      "GOOGL  20160607 24606     1.0           1.0\n",
      "RTN    20150129 34613     0.0           1.0\n",
      "Stats for CNN_RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Britain's state backed banks scrape through stress tests The Bank of England gave Britain's state backed lenders a narrow pass in its debut annual stress tests on Tuesday but warned that next year banks would face tougher checks of their capital strength and international exposure Video\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Fitch rates Barclays Bank contingent capital notes 'BBB EXP ' Fitch Ratings has assigned Barclays Bank PLC's potential issue of contingent capital notes an expected rating of 'BBB EXP ' RATING DRIVERS AND SENSITIVITIES The notes are Tier 2 instruments without a coupon deferral feature that include a 7 capital adequacy trigger On breach of the trigger the notes will be automatically transferred to Barclays PLC the bank holding company for nil consideration resulting in a loss of principal and future interest for the investors The capital a\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "C Penney CEO sees return to growth this year CNBC C Penney Co Inc Chief Executive Ron Johnson still thinks the department store operator will return to growth in 2013 despite severe sales declines in fiscal 2012 he told CNBC in an interview on Wednesday\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Coal exporters seen adding 15 pct supply by 2014 Barclays Coal exporting countries will likely boost supplies by 15 percent over the next two years on strong Indian demand and despite lower imports in Europe and China Barclays said on Friday\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Capital will not appeal risk panel's proposed systemic tag Capital the financial services arm of General Electric said on Tuesday it will not appeal a proposal by the new United States risk council to designate the firm as systemically important a tag that will subject the company to tougher regulations\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Time Warner Cable loses subscribers says open to deals Loses more than double the video customers than expected\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "DIS    20130509 19868     1.0           1.0\n",
      "CL     20140307 18164     1.0           1.0\n",
      "RBS    20130314 33047     1.0           1.0\n"
     ]
    }
   ],
   "source": [
    "predict_and_print_samples(rnn_res[0], 'RNN', test_padded_words)\n",
    "\n",
    "predict_and_print_samples(cnn_res[0], 'CNN', testCharData[:, :, 1:])\n",
    "\n",
    "predict_and_print_samples(cnn_rnn_res[0], 'CNN_RNN', [test_padded_words, testCharData[:, :, 1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
