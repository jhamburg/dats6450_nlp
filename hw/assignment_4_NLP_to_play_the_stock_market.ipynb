{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import calendar\n",
    "\n",
    "# Prepocessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Concatenate, TimeDistributed\n",
    "from keras.layers import Dense, Bidirectional, Dropout, Conv1D, Conv2D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '../data'\n",
    "reutersFile = 'news_reuters.csv'\n",
    "stockFile = 'stockReturns.json'\n",
    "\n",
    "rawX = pd.read_csv(os.path.join(dataPath, reutersFile), header=None, \n",
    "                   names=['ticker', 'company', 'pub_date', 'headline', 'first_sent', 'category'])\n",
    "rawY = pd.read_json(os.path.join(dataPath, stockFile))\n",
    "# rawY = json.load(os.path.join(dataPath, stockFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_y_data(data, tickerType='mid'):\n",
    "    \"\"\"Convert stock data into binary postive/negative\"\"\"\n",
    "    tmp = data[tickerType].apply(pd.Series)\n",
    "    tmp = tmp.stack().rename('price', inplace=True).reset_index()\n",
    "    tmp['y'] = np.where(tmp['price'] >= 0, 1, 0)\n",
    "    tmp.rename(columns={'level_0': 'ticker', 'level_1': 'pub_date'}, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def clean_and_merge_data(X, Y):\n",
    "    \"\"\"Filter X to only those tickers with stock data\"\"\"\n",
    "    y_tickers = set(Y['ticker'])\n",
    "    X = X.loc[X['ticker'].isin(y_tickers)]\n",
    "    # Make sure data types are the same for merge    \n",
    "    Y['pub_date'] = Y['pub_date'].astype(rawX['pub_date'].dtype)\n",
    "    Y['ticker'] = Y['ticker'].astype(rawX['ticker'].dtype)\n",
    "    return X.merge(Y, on=['ticker', 'pub_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanY = reformat_y_data(rawY, 'mid')\n",
    "\n",
    "merged = clean_and_merge_data(rawX, cleanY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up text columns and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    \"\"\"Clean up text data by:\n",
    "    \n",
    "    1. Replacing double spaces into a single space\n",
    "    2. Replace U.S. to United States so U won't get deleted with next \n",
    "       replacement\n",
    "    3. Remove all capitalized words at the beginning of the \n",
    "       sentence, since those are mostly places (aka NEW YORK)\n",
    "    4. Remove unnecessary punctuation (hyphens and asterisks)\n",
    "    5. Remove dates\n",
    "    \"\"\"\n",
    "    monthStrings = list(calendar.month_name)[1:] + list(calendar.month_abbr)[1:]\n",
    "    monthPattern = '|'.join(monthStrings)\n",
    "    \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    sent = re.sub(r'U.S.', 'United States', sent)\n",
    "    sent = re.sub(r'^(\\W?[A-Z\\s\\d]+\\b-?)', '', sent)\n",
    "    sent = re.sub(r'^ ?\\W ', '', sent)\n",
    "    sent = re.sub(r'({}) \\d+'.format(monthPattern), '', sent)\n",
    "    \n",
    "    # replace double spaces one more time after previous cleaning \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    return sent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(col):\n",
    "    \"\"\"Tokenize string into a sequence of words\"\"\"\n",
    "    return [text_to_word_sequence(text, lower=False) for text in col]\n",
    "\n",
    "def filt_to_one(x, random_state=10):\n",
    "    \"\"\"Filter dataset so that there is only one observation per day.\n",
    "    \n",
    "    If there is more than one record, will use the topStory record\n",
    "    if one exists.  If one doesn't or there are 2 topStory records\n",
    "    then it will randomly select one of the observations.\n",
    "    \"\"\"\n",
    "    if x.shape[0] > 1:\n",
    "        if 'topStory' in x['category'].unique():\n",
    "            x = x.loc[x['category'] == 'topStory']\n",
    "        if x.shape[0] > 1:\n",
    "            x = x.sample(n=1, random_state=random_state)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up text\n",
    "merged['headline'] = merged.headline.apply(clean_text)\n",
    "merged['first_sent'] = merged.first_sent.apply(clean_text)\n",
    "\n",
    "# Turn sentences into tokens\n",
    "merged['headline_token'] = tokenize_sent(merged.headline)\n",
    "merged['first_sent_token'] = tokenize_sent(merged.first_sent)\n",
    "\n",
    "# Get one record per company/day\n",
    "finalData = merged.groupby(by=['ticker', 'pub_date']).apply(filt_to_one)\n",
    "\n",
    "# Combine Headline and First Sentence into one text \n",
    "finalData['final_text'] = finalData['headline_token'] + finalData.first_sent_token\n",
    "\n",
    "# Remove observations with missing stock price\n",
    "finalData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "train, test = train_test_split(finalData, test_size = .2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Lexicon and Transform Data to Integers for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lexiconTransformer():\n",
    "    \"\"\"Create a lexicon and transform sentences and\n",
    "       to indexes for use in the model.\"\"\"\n",
    "    \n",
    "    def __init__(self, words_min_freq = 1, unknown_word_token = u'<UNK>',\n",
    "                 savePath='models', saveName='stock_word_lexicon'):\n",
    "        self.words_min_freq = words_min_freq\n",
    "        self.words_lexicon = None\n",
    "        self.unknown_word_token = unknown_word_token\n",
    "        self.indx_to_words_dict = None\n",
    "        self.savePath = savePath\n",
    "        self.saveName = saveName + '.pkl'\n",
    "    \n",
    "    def fit(self, sents):\n",
    "        \"\"\"Create lexicon based on sentences\"\"\"\n",
    "        self.make_words_lexicon(sents)        \n",
    "        self.make_lexicon_reverse()\n",
    "        self.save_lexicon()\n",
    "                \n",
    "    def transform(self, sents):\n",
    "        sents_indxs = self.tokens_to_idxs(sents, self.words_lexicon)\n",
    "        return sents_indxs\n",
    "\n",
    "    def fit_transform(self, sents):\n",
    "        self.fit(sents)\n",
    "        return self.transform(sents)\n",
    "        \n",
    "    def make_words_lexicon(self, sents_token):\n",
    "        \"\"\"Wrapper for words lexicon\"\"\"\n",
    "        self.words_lexicon = self.make_lexicon(sents_token, self.words_min_freq,\n",
    "                                               self.unknown_word_token)\n",
    "\n",
    "    def make_lexicon(self, token_seqs, min_freq=1, unknown = u'<UNK>'):\n",
    "        \"\"\"Create lexicon from input based on a frequency\n",
    "\n",
    "            Parameters:\n",
    "            \n",
    "            token_seqs\n",
    "            ----------\n",
    "               A list of a list of input tokens that will be used to create the lexicon\n",
    "            \n",
    "            min_freq\n",
    "            --------\n",
    "               Number of times the token needs to be in the corpus to be included in the\n",
    "               lexicon.  Otherwise, will be replaced with the \"unknown\" entry\n",
    "            \n",
    "            unknown\n",
    "            -------\n",
    "               The word in the lexicon that should be used for tokens not existing in lexicon.\n",
    "               This can be a value that already exists in input list.  For instance, in \n",
    "               Named Entity Recognition, a value of \"other\" or \"O\" may already be a tag \n",
    "               and so having \"other\" and \"unknown\" are the same thing!\n",
    "        \"\"\"\n",
    "        # Count how often each word appears in the text.\n",
    "        token_counts = {}\n",
    "        for seq in token_seqs:\n",
    "            for token in seq:\n",
    "                if token in token_counts:\n",
    "                    token_counts[token] += 1\n",
    "                else:\n",
    "                    token_counts[token] = 1\n",
    "\n",
    "        # Then, assign each word to a numerical index. \n",
    "        # Filter words that occur less than min_freq times.\n",
    "        lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "        \n",
    "        # Have to delete unknown value from token list so not a gap in lexicon values when\n",
    "        # turning it into a lexicon (aka, if unknown == OTHER and that is the 7th value, \n",
    "        # then 7 won't exist in the lexicon which may cause issues)\n",
    "        if unknown in lexicon:\n",
    "            lexicon.remove(unknown)\n",
    "\n",
    "        # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "        lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "        \n",
    "        lexicon[unknown] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "        lexicon_size = len(lexicon)\n",
    "        return lexicon\n",
    "    \n",
    "    def save_lexicon(self):\n",
    "        \"Save lexicons by pickling them\"\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "        with open(os.path.join(self.savePath, self.saveName), 'wb') as f:\n",
    "            pickle.dump(self.words_lexicon, f)\n",
    "                        \n",
    "    def load_lexicon(self):\n",
    "        with open(os.path.join(self.savePath, self.saveName), 'rb') as f:\n",
    "            self.words_lexicon = pickle.load(f)\n",
    "                    \n",
    "        self.make_lexicon_reverse()\n",
    "        \n",
    "    def make_lexicon_reverse(self):\n",
    "        self.indx_to_words_dict = self.get_lexicon_lookup(self.words_lexicon)\n",
    "    \n",
    "    def get_lexicon_lookup(self, lexicon):\n",
    "        '''Make a dictionary where the string representation of \n",
    "           a lexicon item can be retrieved from its numerical index'''\n",
    "        lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "        return lexicon_lookup\n",
    "    \n",
    "    def tokens_to_idxs(self, token_seqs, lexicon):\n",
    "        \"\"\"Transform tokens to numeric indexes or <UNK> if doesn't exist\"\"\"\n",
    "        idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in token_seq] for token_seq in token_seqs]\n",
    "        return idx_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = lexiconTransformer(words_min_freq=2)\n",
    "\n",
    "lexicon.fit(train['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train['finalText_indx'] = lexicon.transform(train['final_text'])\n",
    "test['finalText_indx'] = lexicon.transform(test['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_seq_len(sents):\n",
    "    return max([len(idx_seq) for idx_seq in sents])\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of longest sequence\n",
    "max_seq_len = get_max_seq_len(train['finalText_indx'])\n",
    "\n",
    "#Add one to max length for offsetting sequence by 1\n",
    "train_padded_words = pad_idx_seqs(train['finalText_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "test_padded_words = pad_idx_seqs(test['finalText_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "train_y = to_categorical(train['y'])\n",
    "test_y = to_categorical(test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 2\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed_matrix(model, lexicon, embed_size):\n",
    "    \"Create a weight matrix for words\"\n",
    "    vocab_size = len(lexicon)\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_size))\n",
    "    n = 0\n",
    "    word_list = list(lexicon)\n",
    "    for i in range(vocab_size):\n",
    "        word = word_list[i]\n",
    "        if word in model.wv.vocab:\n",
    "            embedding_vector = model.wv[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[n] = embedding_vector[:embed_size]\n",
    "                n += 1\n",
    "\n",
    "    return embedding_matrix[:n, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed_len = 200\n",
    "word_embed_matrix = create_embed_matrix(w2v, lexicon.words_lexicon, \n",
    "                                   word_embed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(seq_input_len, embed_matrix, \n",
    "                     n_RNN_nodes, n_dense_nodes, \n",
    "                     batch_size=None, recurrent_dropout=0.2, \n",
    "                     drop_out=.2):\n",
    "    \n",
    "    word_input = Input(batch_shape=(batch_size, seq_input_len), name='word_input_layer')\n",
    "        \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    hidden_layer = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      name='hidden_layer'))(word_embeddings)\n",
    "\n",
    "    dense_layer = Dense(units=n_dense_nodes, activation='relu', name='dense_layer')(hidden_layer)\n",
    "\n",
    "    drop_out3 = Dropout(drop_out)(dense_layer)\n",
    "\n",
    "    output_layer = Dense(units=2, activation='softmax',\n",
    "                         name='output_layer')(drop_out3)\n",
    "\n",
    "    model = Model(inputs=[word_input], outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = create_rnn_model(seq_input_len=train_padded_words.shape[-1],\n",
    "                             embed_matrix=embed_matrix, \n",
    "                             n_RNN_nodes=200, n_dense_nodes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(model, x_train, y_train, x_test, y_test, \n",
    "                         modelSaveName, modelSavePath='models',\n",
    "                         batch_size=64, epochs=3, validation_split=.1):\n",
    "    \"\"\"Train model, save weights, and predict data\"\"\"\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    filepath = os.path.join(modelSavePath, modelSaveName + '.hdf5')\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(x=[x_train], y=y_train, batch_size=batch_size, \n",
    "              epochs=epochs, validation_split=validation_split, \n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    score, acc, rec, prec = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    return (model, acc, rec, prec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input_layer (InputLayer (None, 102)               0         \n",
      "_________________________________________________________________\n",
      "word_embedding_layer (Embedd (None, 102, 200)          2297400   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 400)               641600    \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 3,019,602\n",
      "Trainable params: 3,019,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7888 samples, validate on 877 samples\n",
      "Epoch 1/3\n",
      "7872/7888 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.5003 - recall: 0.5003 - precision: 0.5003\n",
      "Epoch 00001: saving model to models\\rnn_model.hdf5\n",
      "7888/7888 [==============================] - 58s 7ms/step - loss: 0.6957 - acc: 0.5003 - recall: 0.5003 - precision: 0.5003 - val_loss: 0.6948 - val_acc: 0.5029 - val_recall: 0.5029 - val_precision: 0.5029\n",
      "Epoch 2/3\n",
      "7872/7888 [============================>.] - ETA: 0s - loss: 0.6591 - acc: 0.6095 - recall: 0.6095 - precision: 0.6095\n",
      "Epoch 00002: saving model to models\\rnn_model.hdf5\n",
      "7888/7888 [==============================] - 55s 7ms/step - loss: 0.6591 - acc: 0.6097 - recall: 0.6097 - precision: 0.6097 - val_loss: 0.7317 - val_acc: 0.5165 - val_recall: 0.5165 - val_precision: 0.5165\n",
      "Epoch 3/3\n",
      "7872/7888 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7459 - recall: 0.7459 - precision: 0.7459\n",
      "Epoch 00003: saving model to models\\rnn_model.hdf5\n",
      "7888/7888 [==============================] - 55s 7ms/step - loss: 0.5188 - acc: 0.7459 - recall: 0.7459 - precision: 0.7459 - val_loss: 0.8469 - val_acc: 0.5245 - val_recall: 0.5245 - val_precision: 0.5245\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-74152204c3f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m rnn_res = train_and_test_model(rnn_model, train_padded_words, train_y, \n\u001b[1;32m----> 2\u001b[1;33m                                test_padded_words, test_y, 'rnn_model')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-113-319db595a7a8>\u001b[0m in \u001b[0;36mtrain_and_test_model\u001b[1;34m(model, x_train, y_train, x_test, y_test, modelSaveName, modelSavePath, batch_size, epochs, validation_split)\u001b[0m\n\u001b[0;32m     13\u001b[0m               callbacks=callbacks_list)\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_labels' is not defined"
     ]
    }
   ],
   "source": [
    "rnn_res = train_and_test_model(rnn_model, train_padded_words, train_y, \n",
    "                               test_padded_words, test_y, 'rnn_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filter = 256\n",
    "dense_outputs = 1024\n",
    "filter_kernels = [7, 7, 3, 3, 3, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 56\n"
     ]
    }
   ],
   "source": [
    "txt = ''\n",
    "for doc in docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = set(txt)\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/120\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6978 - acc: 0.5500 - val_loss: 0.6988 - val_acc: 0.4550\n",
      "Epoch 2/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.6898 - acc: 0.5550 - val_loss: 0.7686 - val_acc: 0.2400\n",
      "Epoch 3/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.6886 - acc: 0.5637 - val_loss: 0.7474 - val_acc: 0.2400\n",
      "Epoch 4/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.6907 - acc: 0.5650 - val_loss: 0.7343 - val_acc: 0.2400\n",
      "Epoch 5/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.6895 - acc: 0.5525 - val_loss: 0.7610 - val_acc: 0.2400\n",
      "Epoch 6/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.6856 - acc: 0.5600 - val_loss: 0.7646 - val_acc: 0.2400\n",
      "Epoch 7/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.7336 - acc: 0.5975 - val_loss: 0.7342 - val_acc: 0.3100\n",
      "Epoch 8/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.6419 - acc: 0.6225 - val_loss: 0.6004 - val_acc: 0.7300\n",
      "Epoch 9/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.6592 - acc: 0.6588 - val_loss: 0.6593 - val_acc: 0.5650\n",
      "Epoch 10/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.5916 - acc: 0.6788 - val_loss: 0.6915 - val_acc: 0.5400\n",
      "Epoch 11/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4872 - acc: 0.7375 - val_loss: 0.6104 - val_acc: 0.7950\n",
      "Epoch 12/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.5355 - acc: 0.7462 - val_loss: 0.8423 - val_acc: 0.6500\n",
      "Epoch 13/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3900 - acc: 0.8350 - val_loss: 1.0069 - val_acc: 0.6550\n",
      "Epoch 14/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3221 - acc: 0.8738 - val_loss: 1.1945 - val_acc: 0.6950\n",
      "Epoch 15/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3398 - acc: 0.8825 - val_loss: 1.7409 - val_acc: 0.5750\n",
      "Epoch 16/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3808 - acc: 0.8800 - val_loss: 0.8435 - val_acc: 0.7500\n",
      "Epoch 17/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2460 - acc: 0.9088 - val_loss: 1.3220 - val_acc: 0.6200\n",
      "Epoch 18/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3724 - acc: 0.9113 - val_loss: 0.8336 - val_acc: 0.7800\n",
      "Epoch 19/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2551 - acc: 0.9088 - val_loss: 0.7425 - val_acc: 0.7100\n",
      "Epoch 20/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1704 - acc: 0.9325 - val_loss: 1.8772 - val_acc: 0.6250\n",
      "Epoch 21/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3294 - acc: 0.9375 - val_loss: 1.5004 - val_acc: 0.7800\n",
      "Epoch 22/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1929 - acc: 0.9513 - val_loss: 1.3038 - val_acc: 0.7800\n",
      "Epoch 23/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1814 - acc: 0.9413 - val_loss: 1.2535 - val_acc: 0.6550\n",
      "Epoch 24/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2950 - acc: 0.9462 - val_loss: 1.3392 - val_acc: 0.6850\n",
      "Epoch 25/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0524 - acc: 0.9788 - val_loss: 1.7869 - val_acc: 0.6400\n",
      "Epoch 26/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2645 - acc: 0.9525 - val_loss: 1.5919 - val_acc: 0.7050\n",
      "Epoch 27/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3272 - acc: 0.9487 - val_loss: 0.7863 - val_acc: 0.6400\n",
      "Epoch 28/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0551 - acc: 0.9800 - val_loss: 2.3174 - val_acc: 0.6350\n",
      "Epoch 29/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.9913 - val_loss: 2.4335 - val_acc: 0.6700\n",
      "Epoch 30/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2344 - acc: 0.9662 - val_loss: 1.2130 - val_acc: 0.6500\n",
      "Epoch 31/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0355 - acc: 0.9862 - val_loss: 2.7966 - val_acc: 0.6600\n",
      "Epoch 32/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2336 - acc: 0.9688 - val_loss: 1.7737 - val_acc: 0.6600\n",
      "Epoch 33/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0082 - acc: 0.9975 - val_loss: 3.1534 - val_acc: 0.6550\n",
      "Epoch 34/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0121 - acc: 0.9975 - val_loss: 3.0125 - val_acc: 0.6700\n",
      "Epoch 35/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2635 - acc: 0.9700 - val_loss: 2.0227 - val_acc: 0.6650\n",
      "Epoch 36/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 3.0037 - val_acc: 0.6600\n",
      "Epoch 37/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0236 - acc: 0.9962 - val_loss: 2.6859 - val_acc: 0.7050\n",
      "Epoch 38/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0088 - acc: 0.9988 - val_loss: 3.3947 - val_acc: 0.7000\n",
      "Epoch 39/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0083 - acc: 0.9988 - val_loss: 3.5924 - val_acc: 0.7050\n",
      "Epoch 40/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0093 - acc: 0.9988 - val_loss: 3.3457 - val_acc: 0.7300\n",
      "Epoch 41/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0208 - acc: 0.9988 - val_loss: 5.8611 - val_acc: 0.6000\n",
      "Epoch 42/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.6978 - acc: 0.9500 - val_loss: 2.8414 - val_acc: 0.7350\n",
      "Epoch 43/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0388 - acc: 0.9925 - val_loss: 3.3279 - val_acc: 0.7100\n",
      "Epoch 44/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 3.3694 - val_acc: 0.7050\n",
      "Epoch 45/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 5.7490e-04 - acc: 1.0000 - val_loss: 4.0439 - val_acc: 0.6850\n",
      "Epoch 46/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 5.3322e-05 - acc: 1.0000 - val_loss: 4.0232 - val_acc: 0.7050\n",
      "Epoch 47/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 2.9010e-06 - acc: 1.0000 - val_loss: 4.2618 - val_acc: 0.6900\n",
      "Epoch 48/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 9.1392e-07 - acc: 1.0000 - val_loss: 4.2049 - val_acc: 0.7050\n",
      "Epoch 49/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 6.5758e-07 - acc: 1.0000 - val_loss: 4.0935 - val_acc: 0.7100\n",
      "Epoch 50/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3738 - acc: 0.9738 - val_loss: 8.2962 - val_acc: 0.4550\n",
      "Epoch 51/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.5238 - acc: 0.9600 - val_loss: 3.5118 - val_acc: 0.6850\n",
      "Epoch 52/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0150 - acc: 0.9975 - val_loss: 3.1438 - val_acc: 0.6750\n",
      "Epoch 53/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 3.4951 - val_acc: 0.6800\n",
      "Epoch 54/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.8897e-04 - acc: 1.0000 - val_loss: 4.1466 - val_acc: 0.6650\n",
      "Epoch 55/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.6543e-05 - acc: 1.0000 - val_loss: 4.3985 - val_acc: 0.6650\n",
      "Epoch 56/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.6553e-06 - acc: 1.0000 - val_loss: 4.5266 - val_acc: 0.6650\n",
      "Epoch 57/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.9506e-06 - acc: 1.0000 - val_loss: 4.7583 - val_acc: 0.6650\n",
      "Epoch 58/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.7166e-07 - acc: 1.0000 - val_loss: 4.8547 - val_acc: 0.6550\n",
      "Epoch 59/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 2.1637e-07 - acc: 1.0000 - val_loss: 4.8045 - val_acc: 0.6750\n",
      "Epoch 60/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.5259e-07 - acc: 1.0000 - val_loss: 5.1293 - val_acc: 0.6400\n",
      "Epoch 61/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.2025e-07 - acc: 1.0000 - val_loss: 5.0623 - val_acc: 0.6500\n",
      "Epoch 62/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.0623 - val_acc: 0.6500\n",
      "Epoch 63/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1951e-07 - acc: 1.0000 - val_loss: 5.1740 - val_acc: 0.6400\n",
      "Epoch 64/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.1740 - val_acc: 0.6400\n",
      "Epoch 65/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.8970 - val_acc: 0.6700\n",
      "Epoch 66/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.8970 - val_acc: 0.6700\n",
      "Epoch 67/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.2353e-07 - acc: 1.0000 - val_loss: 4.2152 - val_acc: 0.6900\n",
      "Epoch 68/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.3053e-07 - acc: 1.0000 - val_loss: 4.8606 - val_acc: 0.6700\n",
      "Epoch 69/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.8606 - val_acc: 0.6700\n",
      "Epoch 70/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.8606 - val_acc: 0.6700\n",
      "Epoch 71/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.8606 - val_acc: 0.6700\n",
      "Epoch 72/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.8606 - val_acc: 0.6700\n",
      "Epoch 73/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.8606 - val_acc: 0.6700\n",
      "Epoch 74/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 75/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 76/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 77/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 78/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 79/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 80/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 81/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 82/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 83/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 84/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 85/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 86/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 87/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 88/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 89/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 90/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 91/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 92/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.9252 - val_acc: 0.6650\n",
      "Epoch 93/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.4514 - acc: 0.9050 - val_loss: 5.0209 - val_acc: 0.5850\n",
      "Epoch 94/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0489 - acc: 0.9950 - val_loss: 3.2770 - val_acc: 0.6650\n",
      "Epoch 95/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0203 - acc: 0.9988 - val_loss: 3.4805 - val_acc: 0.6650\n",
      "Epoch 96/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.9988 - val_loss: 3.7406 - val_acc: 0.6650\n",
      "Epoch 97/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 3.8312 - val_acc: 0.6650\n",
      "Epoch 98/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.9988 - val_loss: 4.1660 - val_acc: 0.6650\n",
      "Epoch 99/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.4211 - val_acc: 0.6400\n",
      "Epoch 100/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.4354 - val_acc: 0.6550\n",
      "Epoch 101/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.4588 - val_acc: 0.6550\n",
      "Epoch 102/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.6581 - val_acc: 0.6400\n",
      "Epoch 103/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.6326 - val_acc: 0.6500\n",
      "Epoch 104/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4017 - acc: 0.9650 - val_loss: 7.5026 - val_acc: 0.4200\n",
      "Epoch 105/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1609 - acc: 0.9800 - val_loss: 3.1983 - val_acc: 0.6550\n",
      "Epoch 106/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0209 - acc: 0.9988 - val_loss: 3.4893 - val_acc: 0.6550\n",
      "Epoch 107/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.9988 - val_loss: 4.1239 - val_acc: 0.6300\n",
      "Epoch 108/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0202 - acc: 0.9988 - val_loss: 4.0918 - val_acc: 0.6700\n",
      "Epoch 109/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.3601 - val_acc: 0.6450\n",
      "Epoch 110/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.4989 - val_acc: 0.6400\n",
      "Epoch 111/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.4964 - val_acc: 0.6450\n",
      "Epoch 112/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0201 - acc: 0.9988 - val_loss: 4.7266 - val_acc: 0.6450\n",
      "Epoch 113/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2110 - acc: 0.9838 - val_loss: 10.8255 - val_acc: 0.3050\n",
      "Epoch 114/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4207 - acc: 0.9537 - val_loss: 2.0571 - val_acc: 0.6700\n",
      "Epoch 115/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1242 - acc: 0.9875 - val_loss: 2.8181 - val_acc: 0.5850\n",
      "Epoch 116/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0808 - acc: 0.9900 - val_loss: 3.0874 - val_acc: 0.5750\n",
      "Epoch 117/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0046 - acc: 0.9962 - val_loss: 3.5245 - val_acc: 0.5750\n",
      "Epoch 118/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 1.0970e-04 - acc: 1.0000 - val_loss: 4.7741 - val_acc: 0.5900\n",
      "Epoch 119/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 2.7217e-07 - acc: 1.0000 - val_loss: 4.8033 - val_acc: 0.5850\n",
      "Epoch 120/120\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 2.6096e-06 - acc: 1.0000 - val_loss: 5.4346 - val_acc: 0.5550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f02d406d5f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def vectorize_sentences(data, char_indices):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [char_indices[w] for w in sentences]\n",
    "        x2 = np.eye(len(char_indices))[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "train_data = vectorize_sentences(docs,char_indices)\n",
    "train_data.shape\n",
    "y_train = to_categorical(sentiments)\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv1D, MaxPool1D\n",
    "\n",
    "inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')\n",
    "\n",
    "conv = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                     padding='valid', activation='relu',\n",
    "                     input_shape=(maxlen, vocab_size))(inputs)\n",
    "conv = MaxPool1D(pool_size=3)(conv)\n",
    "\n",
    "conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                      padding='valid', activation='relu')(conv)\n",
    "conv1 = MaxPool1D(pool_size=3)(conv1)\n",
    "\n",
    "conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                      padding='valid', activation='relu')(conv1)\n",
    "\n",
    "conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                      padding='valid', activation='relu')(conv2)\n",
    "\n",
    "conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[4],\n",
    "                      padding='valid', activation='relu')(conv3)\n",
    "\n",
    "conv5 = Conv1D(nb_filter, kernel_size=filter_kernels[5],\n",
    "                      padding='valid', activation='relu')(conv4)\n",
    "conv5 = MaxPool1D(pool_size=3)(conv5)\n",
    "conv5 = Flatten()(conv5)\n",
    "\n",
    "z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv5))\n",
    "z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))\n",
    "\n",
    "pred = Dense(n_out, activation='softmax', name='output')(z)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=pred)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, y_train, batch_size=32,\n",
    "           epochs=120, validation_split=0.2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
