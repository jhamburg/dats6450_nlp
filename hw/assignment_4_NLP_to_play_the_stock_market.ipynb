{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Utility libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import calendar\n",
    "\n",
    "# Prepocessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Concatenate, TimeDistributed\n",
    "from keras.layers import Dense, Bidirectional, Dropout, Flatten, multiply\n",
    "from keras.layers import Conv1D, Conv2D, MaxPool1D, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '../data'\n",
    "reutersFile = 'news_reuters.csv'\n",
    "stockFile = 'stockReturns.json'\n",
    "\n",
    "rawX = pd.read_csv(os.path.join(dataPath, reutersFile), header=None, \n",
    "                   names=['ticker', 'company', 'pub_date', 'headline', 'first_sent', 'category'])\n",
    "rawY = pd.read_json(os.path.join(dataPath, stockFile))\n",
    "# rawY = json.load(os.path.join(dataPath, stockFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_y_data(data, tickerType='mid'):\n",
    "    \"\"\"Convert stock data into binary postive/negative\"\"\"\n",
    "    tmp = data[tickerType].apply(pd.Series)\n",
    "    tmp = tmp.stack().rename('price', inplace=True).reset_index()\n",
    "    tmp['y'] = np.where(tmp['price'] >= 0, 1, 0)\n",
    "    tmp.rename(columns={'level_0': 'ticker', 'level_1': 'pub_date'}, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def clean_and_merge_data(X, Y):\n",
    "    \"\"\"Filter X to only those tickers with stock data\"\"\"\n",
    "    y_tickers = set(Y['ticker'])\n",
    "    X = X.loc[X['ticker'].isin(y_tickers)]\n",
    "    # Make sure data types are the same for merge    \n",
    "    Y['pub_date'] = Y['pub_date'].astype(rawX['pub_date'].dtype)\n",
    "    Y['ticker'] = Y['ticker'].astype(rawX['ticker'].dtype)\n",
    "    return X.merge(Y, on=['ticker', 'pub_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanY = reformat_y_data(rawY, 'short')\n",
    "\n",
    "merged = clean_and_merge_data(rawX, cleanY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up text columns and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    \"\"\"Clean up text data by:\n",
    "    \n",
    "    1. Replacing double spaces into a single space\n",
    "    2. Replace U.S. to United States so U won't get deleted with next \n",
    "       replacement\n",
    "    3. Remove all capitalized words at the beginning of the \n",
    "       sentence, since those are mostly places (aka NEW YORK)\n",
    "    4. Remove unnecessary punctuation (hyphens and asterisks)\n",
    "    5. Remove dates\n",
    "    \"\"\"\n",
    "    monthStrings = list(calendar.month_name)[1:] + list(calendar.month_abbr)[1:]\n",
    "    monthPattern = '|'.join(monthStrings)\n",
    "    \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    sent = re.sub(r'U.S.', 'United States', sent)\n",
    "    sent = re.sub(r'^(\\W?[A-Z\\s\\d]+\\b-?)', '', sent)\n",
    "    sent = re.sub(r'^ ?\\W ', '', sent)\n",
    "    sent = re.sub(r'({}) \\d+'.format(monthPattern), '', sent)\n",
    "    \n",
    "    # replace double spaces one more time after previous cleaning \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    return sent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(col):\n",
    "    \"\"\"Tokenize string into a sequence of words\"\"\"\n",
    "    return [text_to_word_sequence(text, lower=False) for text in col]\n",
    "\n",
    "def filt_to_one(x, random_state=10):\n",
    "    \"\"\"Filter dataset so that there is only one observation per day.\n",
    "    \n",
    "    If there is more than one record, will use the topStory record\n",
    "    if one exists.  If one doesn't or there are 2 topStory records\n",
    "    then it will randomly select one of the observations.\n",
    "    \"\"\"\n",
    "    if x.shape[0] > 1:\n",
    "        if 'topStory' in x['category'].unique():\n",
    "            x = x.loc[x['category'] == 'topStory']\n",
    "        if x.shape[0] > 1:\n",
    "            x = x.sample(n=1, random_state=random_state)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up text\n",
    "merged['headline'] = merged.headline.apply(clean_text)\n",
    "merged['first_sent'] = merged.first_sent.apply(clean_text)\n",
    "\n",
    "# Turn sentences into tokens\n",
    "merged['headline_token'] = tokenize_sent(merged.headline)\n",
    "merged['first_sent_token'] = tokenize_sent(merged.first_sent)\n",
    "\n",
    "# Get one record per company/day\n",
    "finalData = merged.groupby(by=['ticker', 'pub_date']).apply(filt_to_one)\n",
    "\n",
    "# Combine Headline and First Sentence into one text \n",
    "finalData['final_text'] = finalData['headline_token'] + finalData.first_sent_token\n",
    "\n",
    "# Remove observations with missing stock price\n",
    "finalData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "train, test = train_test_split(finalData, test_size = .2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Lexicon and Transform Data to Integers for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lexiconTransformer():\n",
    "    \"\"\"Create a lexicon and transform sentences and\n",
    "       to indexes for use in the model.\"\"\"\n",
    "    \n",
    "    def __init__(self, words_min_freq = 1, unknown_word_token = u'<UNK>',\n",
    "                 savePath='models', saveName='stock_word_lexicon'):\n",
    "        self.words_min_freq = words_min_freq\n",
    "        self.words_lexicon = None\n",
    "        self.unknown_word_token = unknown_word_token\n",
    "        self.indx_to_words_dict = None\n",
    "        self.savePath = savePath\n",
    "        self.saveName = saveName + '.pkl'\n",
    "    \n",
    "    def fit(self, sents):\n",
    "        \"\"\"Create lexicon based on sentences\"\"\"\n",
    "        self.make_words_lexicon(sents)        \n",
    "        self.make_lexicon_reverse()\n",
    "        self.save_lexicon()\n",
    "                \n",
    "    def transform(self, sents):\n",
    "        sents_indxs = self.tokens_to_idxs(sents, self.words_lexicon)\n",
    "        return sents_indxs\n",
    "\n",
    "    def fit_transform(self, sents):\n",
    "        self.fit(sents)\n",
    "        return self.transform(sents)\n",
    "        \n",
    "    def make_words_lexicon(self, sents_token):\n",
    "        \"\"\"Wrapper for words lexicon\"\"\"\n",
    "        self.words_lexicon = self.make_lexicon(sents_token, self.words_min_freq,\n",
    "                                               self.unknown_word_token)\n",
    "\n",
    "    def make_lexicon(self, token_seqs, min_freq=1, unknown = u'<UNK>'):\n",
    "        \"\"\"Create lexicon from input based on a frequency\n",
    "\n",
    "            Parameters:\n",
    "            \n",
    "            token_seqs\n",
    "            ----------\n",
    "               A list of a list of input tokens that will be used to create the lexicon\n",
    "            \n",
    "            min_freq\n",
    "            --------\n",
    "               Number of times the token needs to be in the corpus to be included in the\n",
    "               lexicon.  Otherwise, will be replaced with the \"unknown\" entry\n",
    "            \n",
    "            unknown\n",
    "            -------\n",
    "               The word in the lexicon that should be used for tokens not existing in lexicon.\n",
    "               This can be a value that already exists in input list.  For instance, in \n",
    "               Named Entity Recognition, a value of \"other\" or \"O\" may already be a tag \n",
    "               and so having \"other\" and \"unknown\" are the same thing!\n",
    "        \"\"\"\n",
    "        # Count how often each word appears in the text.\n",
    "        token_counts = {}\n",
    "        for seq in token_seqs:\n",
    "            for token in seq:\n",
    "                if token in token_counts:\n",
    "                    token_counts[token] += 1\n",
    "                else:\n",
    "                    token_counts[token] = 1\n",
    "\n",
    "        # Then, assign each word to a numerical index. \n",
    "        # Filter words that occur less than min_freq times.\n",
    "        lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "        \n",
    "        # Have to delete unknown value from token list so not a gap in lexicon values when\n",
    "        # turning it into a lexicon (aka, if unknown == OTHER and that is the 7th value, \n",
    "        # then 7 won't exist in the lexicon which may cause issues)\n",
    "        if unknown in lexicon:\n",
    "            lexicon.remove(unknown)\n",
    "\n",
    "        # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "        lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "        \n",
    "        lexicon[unknown] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "        lexicon_size = len(lexicon)\n",
    "        return lexicon\n",
    "    \n",
    "    def save_lexicon(self):\n",
    "        \"Save lexicons by pickling them\"\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "        with open(os.path.join(self.savePath, self.saveName), 'wb') as f:\n",
    "            pickle.dump(self.words_lexicon, f)\n",
    "                        \n",
    "    def load_lexicon(self):\n",
    "        with open(os.path.join(self.savePath, self.saveName), 'rb') as f:\n",
    "            self.words_lexicon = pickle.load(f)\n",
    "                    \n",
    "        self.make_lexicon_reverse()\n",
    "        \n",
    "    def make_lexicon_reverse(self):\n",
    "        self.indx_to_words_dict = self.get_lexicon_lookup(self.words_lexicon)\n",
    "    \n",
    "    def get_lexicon_lookup(self, lexicon):\n",
    "        '''Make a dictionary where the string representation of \n",
    "           a lexicon item can be retrieved from its numerical index'''\n",
    "        lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "        return lexicon_lookup\n",
    "    \n",
    "    def tokens_to_idxs(self, token_seqs, lexicon):\n",
    "        \"\"\"Transform tokens to numeric indexes or <UNK> if doesn't exist\"\"\"\n",
    "        idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in token_seq] for token_seq in token_seqs]\n",
    "        return idx_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = lexiconTransformer(words_min_freq=2)\n",
    "\n",
    "lexicon.fit(train['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train['finalText_indx'] = lexicon.transform(train['final_text'])\n",
    "test['finalText_indx'] = lexicon.transform(test['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_seq_len(sents):\n",
    "    return max([len(idx_seq) for idx_seq in sents])\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of longest sequence\n",
    "max_seq_len = get_max_seq_len(train['finalText_indx'])\n",
    "\n",
    "#Add one to max length for offsetting sequence by 1\n",
    "train_padded_words = pad_idx_seqs(train['finalText_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "test_padded_words = pad_idx_seqs(test['finalText_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "train_y = to_categorical(train['y'])\n",
    "test_y = to_categorical(test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 2\n",
    "nb_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed_matrix(model, lexicon, embed_size):\n",
    "    \"Create a weight matrix for words\"\n",
    "    vocab_size = len(lexicon)\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_size))\n",
    "    n = 0\n",
    "    word_list = list(lexicon)\n",
    "    for i in range(vocab_size):\n",
    "        word = word_list[i]\n",
    "        if word in model.wv.vocab:\n",
    "            embedding_vector = model.wv[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[n] = embedding_vector[:embed_size]\n",
    "                n += 1\n",
    "\n",
    "    return embedding_matrix[:n, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "word_embed_len = 300\n",
    "word_embed_matrix = create_embed_matrix(w2v, lexicon.words_lexicon, \n",
    "                                   word_embed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(word_input, embed_matrix, n_RNN_nodes,\n",
    "                     n_rnn_layers=1, \n",
    "                     rnn_bidirectional=True,\n",
    "                     recurrent_dropout=None, \n",
    "                     activity_regularizer = None,\n",
    "                     recurrent_regularizer = None,\n",
    "                     dropout=None,\n",
    "                     batch_normalization=True):\n",
    "    \n",
    "    \"Wrapper function to dynamically create LSTM layers in a model\"\n",
    "           \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    # Build RNN Layers\n",
    "    rnn_layers = []\n",
    "    for i in range(n_rnn_layers):\n",
    "        if i == (n_rnn_layers - 1):\n",
    "            return_seq = False\n",
    "        else:\n",
    "            return_seq = True\n",
    "        \n",
    "        rnn_layer = LSTM(n_RNN_nodes, return_sequences=return_seq,\n",
    "                         recurrent_dropout=recurrent_dropout,\n",
    "                         activity_regularizer = activity_regularizer,\n",
    "                         recurrent_regularizer = recurrent_regularizer,\n",
    "                         dropout=dropout, \n",
    "                         name='hidden_layer{}'.format(i))\n",
    "        \n",
    "        if rnn_bidirectional:\n",
    "            rnn_layer = Bidirectional(rnn_layer)\n",
    "        rnn_layers.append(rnn_layer)\n",
    "    \n",
    "    # Call RNN layers\n",
    "    for layer_num, layer in enumerate(rnn_layers):\n",
    "        \n",
    "        if layer_num == 0:\n",
    "            rnn_layer = layer(word_embeddings)\n",
    "        else:\n",
    "            rnn_layer = layer(rnn_layer)\n",
    "        \n",
    "        # Apply batch normalization if requested\n",
    "        if batch_normalization:\n",
    "            # If not last layer, need to do TimeDistributed\n",
    "            if layer_num != n_rnn_layers - 1:\n",
    "                batch_norm = TimeDistributed(BatchNormalization())\n",
    "            else:\n",
    "                batch_norm = BatchNormalization()\n",
    "                                             \n",
    "            rnn_layer = batch_norm(rnn_layer)\n",
    "        \n",
    "    return rnn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_layers(input_layer, n_dense_layers, n_dense_nodes, \n",
    "                        dropout=None, batch_normalization=True,\n",
    "                        activity_regularizer=None,\n",
    "                        dense_activation='elu'):\n",
    "    \n",
    "        # Build RNN Layers\n",
    "    dense_layers = []\n",
    "    for i in range(n_dense_layers):\n",
    "       \n",
    "        dense_layers.append(Dense(n_dense_nodes, \n",
    "                                  activation=dense_activation, \n",
    "                                  activity_regularizer=activity_regularizer,\n",
    "                                  name='dense_layer{}'.format(i)))   \n",
    "    # Call RNN layers\n",
    "    for layer_num, layer in enumerate(dense_layers):\n",
    "        \n",
    "        if layer_num == 0:\n",
    "            dense_layer = layer(input_layer)\n",
    "        else:\n",
    "            dense_layer = layer(dense_layer)\n",
    "        \n",
    "        # Apply Dropout and batch normalization if requested\n",
    "        if dropout:\n",
    "            dense_layer = Dropout(dropout)(dense_layer)\n",
    "        if batch_normalization:\n",
    "            dense_layer = BatchNormalization()(dense_layer)\n",
    "        \n",
    "    return dense_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_standalone_model(seq_input_len, embed_matrix, n_RNN_nodes, \n",
    "                                n_dense_nodes, n_out=2, \n",
    "                                n_dense_layers=1,\n",
    "                                n_rnn_layers=1, \n",
    "                                rnn_bidirectional=True,\n",
    "                                recurrent_dropout=None, \n",
    "                                activity_regularizer = None,\n",
    "                                recurrent_regularizer = None,\n",
    "                                dropout=None,\n",
    "                                batch_normalization=True,\n",
    "                                dense_activation='elu'):\n",
    "    \n",
    "    word_input = Input(shape=(seq_input_len,), name='word_input_layer')\n",
    "    \n",
    "    rnn_model = create_rnn_model(word_input, embed_matrix, n_RNN_nodes, \n",
    "                                 n_rnn_layers, rnn_bidirectional, \n",
    "                                 recurrent_dropout, activity_regularizer, \n",
    "                                 recurrent_regularizer, dropout, batch_normalization)\n",
    "\n",
    "    dense_layers = create_dense_layers(rnn_model,\n",
    "                                       n_dense_layers, \n",
    "                                       n_dense_nodes, \n",
    "                                       dropout, \n",
    "                                       batch_normalization,\n",
    "                                       activity_regularizer,\n",
    "                                       dense_activation)\n",
    "    \n",
    "    output_layer = Dense(n_out, activation='softmax',\n",
    "                         name='output_layer')(dense_layers)\n",
    "\n",
    "    model = Model(inputs=[word_input], outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = create_rnn_standalone_model(seq_input_len=train_padded_words.shape[-1],\n",
    "                                        embed_matrix=word_embed_matrix,\n",
    "                                        n_RNN_nodes=250,\n",
    "                                        n_dense_nodes=200, \n",
    "                                        n_out=2, \n",
    "                                        n_dense_layers=1,\n",
    "                                        n_rnn_layers=1, \n",
    "                                        rnn_bidirectional=True,\n",
    "                                        recurrent_dropout=.4, \n",
    "                                        activity_regularizer = None,\n",
    "                                        recurrent_regularizer = None,\n",
    "                                        dropout=.2,\n",
    "                                        batch_normalization=True,\n",
    "                                        dense_activation='elu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(model, x_train, y_train, x_test, y_test, \n",
    "                         modelSaveName, modelSavePath='models',\n",
    "                         batch_size=128, epochs=3, validation_split=.1):\n",
    "    \"\"\"Train model, save weights, and predict data\"\"\"\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    filepath = os.path.join(modelSavePath, modelSaveName + '.hdf5')\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(x=x_train, y=y_train, batch_size=batch_size, \n",
    "              epochs=epochs, validation_split=validation_split, \n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    score, acc, rec, prec = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    return (model, acc, rec, prec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input_layer (InputLayer (None, 102)               0         \n",
      "_________________________________________________________________\n",
      "word_embedding_layer (Embedd (None, 102, 300)          3534000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 500)               1102000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_layer0 (Dense)         (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 4,739,402\n",
      "Trainable params: 4,738,002\n",
      "Non-trainable params: 1,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 28s 3ms/step - loss: 0.9244 - acc: 0.4991 - recall: 0.4991 - precision: 0.4991 - val_loss: 0.8224 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00001: saving model to models/rnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.7465 - acc: 0.5535 - recall: 0.5535 - precision: 0.5535 - val_loss: 0.7547 - val_acc: 0.4781 - val_recall: 0.4781 - val_precision: 0.4781\n",
      "\n",
      "Epoch 00002: saving model to models/rnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.6746 - acc: 0.6112 - recall: 0.6112 - precision: 0.6112 - val_loss: 0.7507 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00003: saving model to models/rnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.6079 - acc: 0.6715 - recall: 0.6715 - precision: 0.6715 - val_loss: 0.8127 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00004: saving model to models/rnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.5269 - acc: 0.7428 - recall: 0.7428 - precision: 0.7428 - val_loss: 0.8571 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00005: saving model to models/rnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.4473 - acc: 0.7899 - recall: 0.7899 - precision: 0.7899 - val_loss: 0.9354 - val_acc: 0.5219 - val_recall: 0.5219 - val_precision: 0.5219\n",
      "\n",
      "Epoch 00006: saving model to models/rnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.3833 - acc: 0.8291 - recall: 0.8291 - precision: 0.8291 - val_loss: 1.0242 - val_acc: 0.5318 - val_recall: 0.5318 - val_precision: 0.5318\n",
      "\n",
      "Epoch 00007: saving model to models/rnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.3139 - acc: 0.8678 - recall: 0.8678 - precision: 0.8678 - val_loss: 1.1688 - val_acc: 0.5329 - val_recall: 0.5329 - val_precision: 0.5329\n",
      "\n",
      "Epoch 00008: saving model to models/rnn_model.hdf5\n",
      "Epoch 9/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.2638 - acc: 0.8889 - recall: 0.8889 - precision: 0.8889 - val_loss: 1.3277 - val_acc: 0.5033 - val_recall: 0.5033 - val_precision: 0.5033\n",
      "\n",
      "Epoch 00009: saving model to models/rnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.2209 - acc: 0.9118 - recall: 0.9118 - precision: 0.9118 - val_loss: 1.4353 - val_acc: 0.5318 - val_recall: 0.5318 - val_precision: 0.5318\n",
      "\n",
      "Epoch 00010: saving model to models/rnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.2010 - acc: 0.9194 - recall: 0.9194 - precision: 0.9194 - val_loss: 1.4895 - val_acc: 0.5384 - val_recall: 0.5384 - val_precision: 0.5384\n",
      "\n",
      "Epoch 00011: saving model to models/rnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.1741 - acc: 0.9313 - recall: 0.9313 - precision: 0.9313 - val_loss: 1.5400 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00012: saving model to models/rnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.1627 - acc: 0.9362 - recall: 0.9362 - precision: 0.9362 - val_loss: 1.6865 - val_acc: 0.5340 - val_recall: 0.5340 - val_precision: 0.5340\n",
      "\n",
      "Epoch 00013: saving model to models/rnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.1516 - acc: 0.9413 - recall: 0.9413 - precision: 0.9413 - val_loss: 1.7768 - val_acc: 0.5351 - val_recall: 0.5351 - val_precision: 0.5351\n",
      "\n",
      "Epoch 00014: saving model to models/rnn_model.hdf5\n",
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.1268 - acc: 0.9494 - recall: 0.9494 - precision: 0.9494 - val_loss: 1.8053 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00015: saving model to models/rnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.1234 - acc: 0.9517 - recall: 0.9517 - precision: 0.9517 - val_loss: 1.8076 - val_acc: 0.5274 - val_recall: 0.5274 - val_precision: 0.5274\n",
      "\n",
      "Epoch 00016: saving model to models/rnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.1067 - acc: 0.9580 - recall: 0.9580 - precision: 0.9580 - val_loss: 2.0214 - val_acc: 0.5285 - val_recall: 0.5285 - val_precision: 0.5285\n",
      "\n",
      "Epoch 00017: saving model to models/rnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.1015 - acc: 0.9615 - recall: 0.9615 - precision: 0.9615 - val_loss: 1.9630 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00018: saving model to models/rnn_model.hdf5\n",
      "Epoch 19/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.1171 - acc: 0.9544 - recall: 0.9544 - precision: 0.9544 - val_loss: 1.9997 - val_acc: 0.5493 - val_recall: 0.5493 - val_precision: 0.5493\n",
      "\n",
      "Epoch 00019: saving model to models/rnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0935 - acc: 0.9622 - recall: 0.9622 - precision: 0.9622 - val_loss: 2.0091 - val_acc: 0.5384 - val_recall: 0.5384 - val_precision: 0.5384\n",
      "\n",
      "Epoch 00020: saving model to models/rnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0890 - acc: 0.9618 - recall: 0.9618 - precision: 0.9618 - val_loss: 2.3824 - val_acc: 0.5417 - val_recall: 0.5417 - val_precision: 0.5417\n",
      "\n",
      "Epoch 00021: saving model to models/rnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0856 - acc: 0.9635 - recall: 0.9635 - precision: 0.9635 - val_loss: 2.4599 - val_acc: 0.5318 - val_recall: 0.5318 - val_precision: 0.5318\n",
      "\n",
      "Epoch 00022: saving model to models/rnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0814 - acc: 0.9646 - recall: 0.9646 - precision: 0.9646 - val_loss: 2.3340 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00023: saving model to models/rnn_model.hdf5\n",
      "Epoch 24/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0765 - acc: 0.9654 - recall: 0.9654 - precision: 0.9654 - val_loss: 2.3350 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00024: saving model to models/rnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0706 - acc: 0.9671 - recall: 0.9671 - precision: 0.9671 - val_loss: 2.5509 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00025: saving model to models/rnn_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0716 - acc: 0.9673 - recall: 0.9673 - precision: 0.9673 - val_loss: 2.5071 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00026: saving model to models/rnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0798 - acc: 0.9622 - recall: 0.9622 - precision: 0.9622 - val_loss: 2.2547 - val_acc: 0.5351 - val_recall: 0.5351 - val_precision: 0.5351\n",
      "\n",
      "Epoch 00027: saving model to models/rnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0679 - acc: 0.9661 - recall: 0.9661 - precision: 0.9661 - val_loss: 2.6485 - val_acc: 0.5362 - val_recall: 0.5362 - val_precision: 0.5362\n",
      "\n",
      "Epoch 00028: saving model to models/rnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0654 - acc: 0.9652 - recall: 0.9652 - precision: 0.9652 - val_loss: 2.7770 - val_acc: 0.5285 - val_recall: 0.5285 - val_precision: 0.5285\n",
      "\n",
      "Epoch 00029: saving model to models/rnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0603 - acc: 0.9710 - recall: 0.9710 - precision: 0.9710 - val_loss: 2.7074 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00030: saving model to models/rnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0536 - acc: 0.9735 - recall: 0.9735 - precision: 0.9735 - val_loss: 2.8660 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00031: saving model to models/rnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0543 - acc: 0.9722 - recall: 0.9722 - precision: 0.9722 - val_loss: 2.8806 - val_acc: 0.5285 - val_recall: 0.5285 - val_precision: 0.5285\n",
      "\n",
      "Epoch 00032: saving model to models/rnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0633 - acc: 0.9696 - recall: 0.9696 - precision: 0.9696 - val_loss: 2.9824 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00033: saving model to models/rnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 25s 3ms/step - loss: 0.0884 - acc: 0.9623 - recall: 0.9623 - precision: 0.9623 - val_loss: 3.0840 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00034: saving model to models/rnn_model.hdf5\n",
      "Epoch 35/50\n",
      "7296/8200 [=========================>....] - ETA: 2s - loss: 0.0652 - acc: 0.9694 - recall: 0.9694 - precision: 0.9694"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-72a5fe2ab65f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m rnn_res = train_and_test_model(rnn_model, train_padded_words, train_y, \n\u001b[1;32m      2\u001b[0m                                \u001b[0mtest_padded_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rnn_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                epochs=nb_epoch)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-bb92304872f9>\u001b[0m in \u001b[0;36mtrain_and_test_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, modelSaveName, modelSavePath, batch_size, epochs, validation_split)\u001b[0m\n\u001b[1;32m     11\u001b[0m     model.fit(x=x_train, y=y_train, batch_size=batch_size, \n\u001b[1;32m     12\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m               callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_res = train_and_test_model(rnn_model, train_padded_words, train_y, \n",
    "                               test_padded_words, test_y, 'rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(data, lexicon, maxlen=200):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in sentences]\n",
    "        x2 = np.eye(len(char_indices) + 1)[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_args(arg, n_conv_layers, warning=False):\n",
    "    \"\"\"Makes sure the convolution parameters equal the size\n",
    "       of the convoluation model\"\"\"\n",
    "    if len(arg) < n_conv_layers and len(arg) != 1:\n",
    "        raise ValueError(\"\"\"One of the values passed to the following arguments \n",
    "                            has an incorrect value, either needs to be a value of 1 or a\n",
    "                            list/tuple of length >= n_conv_layers: {}\n",
    "\n",
    "                            nb_filter, filter_kernels, or pool_size\"\"\".format(n_conv_layers))\n",
    "\n",
    "    if len(arg) == 1:\n",
    "        arg = arg * n_conv_layers\n",
    "\n",
    "    if len(arg) > n_conv_layers and warning:\n",
    "        print(\"\"\"Length of argument {} > n_conv_layers. Will only use part of the\n",
    "               argument amount\"\"\".format(len(arg)))\n",
    "        \n",
    "    return arg\n",
    "\n",
    "\n",
    "def create_cnn_model(input_layer, char_maxlen, vocab_size,\n",
    "                     n_conv_layers=1,\n",
    "                     nb_filter=None,\n",
    "                     filter_kernels=None,\n",
    "                     pool_size=None, \n",
    "                     dropout=None,\n",
    "                     activity_regularizer=None,\n",
    "                     batch_normalization=True,\n",
    "                     cnn_padding='same',\n",
    "                     cnn_activation='elu'):\n",
    "    \"\"\"Function to dynamically create convolution and maxpool\n",
    "       layers.\"\"\"\n",
    "    \n",
    "    # Make sure arguments match number of layers\n",
    "    nb_filter = validate_args(nb_filter, n_conv_layers)\n",
    "    filter_kernels = validate_args(filter_kernels, n_conv_layers)\n",
    "    pool_size = validate_args(pool_size, n_conv_layers)\n",
    "    \n",
    "\n",
    "    # Create convolution layers\n",
    "    for i in range(n_conv_layers):\n",
    "        if i == 0:\n",
    "            conv_layer = Conv1D(nb_filter[i], kernel_size=filter_kernels[i],\n",
    "                                padding=cnn_padding, \n",
    "                                activation=cnn_activation,\n",
    "                                activity_regularizer = activity_regularizer,\n",
    "                                input_shape=(char_maxlen, vocab_size),\n",
    "                                name='Conv_layer{}'.format(i))(input_layer)\n",
    "        else:\n",
    "            conv_layer = Conv1D(nb_filter[i], kernel_size=filter_kernels[i],\n",
    "                                padding=cnn_padding, \n",
    "                                activation=cnn_activation,\n",
    "                                activity_regularizer = activity_regularizer,\n",
    "                                name='Conv_layer{}'.format(i))(conv_layer)\n",
    "            \n",
    "        if batch_normalization:\n",
    "            conv_layer = BatchNormalization()(conv_layer)\n",
    "            \n",
    "        if dropout:\n",
    "            conv_layer = Dropout(dropout)(conv_layer)\n",
    "        \n",
    "        conv_layer = MaxPool1D(pool_size=pool_size[i],\n",
    "                               name='MaxPool{}'.format(i))(conv_layer)\n",
    "    \n",
    "    # Flatten convolution layer to work with Dense layers\n",
    "    return Flatten()(conv_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_standalone_model(char_maxlen, vocab_size,\n",
    "                                n_dense_nodes,\n",
    "                                n_dense_layers=1, \n",
    "                                n_conv_layers=1,\n",
    "                                nb_filter=None,\n",
    "                                filter_kernels=None,\n",
    "                                pool_size=None, \n",
    "                                dropout=None,\n",
    "                                activity_regularizer=None,\n",
    "                                batch_normalization=True,\n",
    "                                cnn_padding='same',\n",
    "                                cnn_activation='elu',\n",
    "                                dense_activation='elu'):\n",
    "    \n",
    "        char_input = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "        \n",
    "        cnn_model = create_cnn_model(char_input, char_maxlen, vocab_size,\n",
    "                                     n_conv_layers, nb_filter, filter_kernels,\n",
    "                                     pool_size, dropout, activity_regularizer,\n",
    "                                     batch_normalization, cnn_padding, cnn_activation)\n",
    "        \n",
    "        dense_layers = create_dense_layers(cnn_model, n_dense_layers,\n",
    "                                           n_dense_nodes, dropout,\n",
    "                                           batch_normalization,\n",
    "                                           activity_regularizer,\n",
    "                                           dense_activation)\n",
    "\n",
    "        output_layer = Dense(n_out, activation='softmax',\n",
    "                             name='output_layer')(dense_layers)\n",
    "\n",
    "        model = Model(inputs=[char_input], outputs=output_layer)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                      metrics=['accuracy', recall, precision])\n",
    "\n",
    "        return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_maxlen = 1200 \n",
    "nb_filter = [75]\n",
    "dense_outputs = 628\n",
    "filter_kernels = [7, 3, 3, 3]\n",
    "pool_size = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 86\n"
     ]
    }
   ],
   "source": [
    "# Turn all tokens into one string and then all obs \n",
    "# into one overall string\n",
    "trainTokensAsString = train.final_text.apply(lambda x: ' '.join(x))\n",
    "testTokensAsString = test.final_text.apply(lambda x: ' '.join(x))\n",
    "oneTxt = ' '.join(trainTokensAsString)\n",
    "\n",
    "# Get info about characters\n",
    "chars = set(oneTxt)\n",
    "vocab_size = len(chars) + 1\n",
    "print('total chars:', vocab_size)\n",
    "char_indices = dict((c, i + 2) for i, c in enumerate(chars))\n",
    "indices_char = dict((i + 2, c) for i, c in enumerate(chars))\n",
    "\n",
    "char_indices['<UNK>'] = 1\n",
    "indices_char[1] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCharData = vectorize_sentences(trainTokensAsString, char_indices, char_maxlen)\n",
    "testCharData = vectorize_sentences(testTokensAsString, char_indices, char_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_standalone_model(char_maxlen=char_maxlen,\n",
    "                                        vocab_size=vocab_size,\n",
    "                                        n_dense_nodes=dense_outputs,\n",
    "                                        n_dense_layers=1, \n",
    "                                        n_conv_layers=4,\n",
    "                                        nb_filter=nb_filter,\n",
    "                                        filter_kernels=filter_kernels,\n",
    "                                        pool_size=pool_size, \n",
    "                                        dropout=.2,\n",
    "                                        activity_regularizer=None,\n",
    "                                        batch_normalization=True,\n",
    "                                        cnn_padding='same',\n",
    "                                        cnn_activation='elu',\n",
    "                                        dense_activation='elu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_input_layer (InputLayer (None, 1200, 86)          0         \n",
      "_________________________________________________________________\n",
      "Conv_layer0 (Conv1D)         (None, 1200, 75)          45225     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1200, 75)          300       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1200, 75)          0         \n",
      "_________________________________________________________________\n",
      "MaxPool0 (MaxPooling1D)      (None, 240, 75)           0         \n",
      "_________________________________________________________________\n",
      "Conv_layer1 (Conv1D)         (None, 240, 75)           16950     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 240, 75)           300       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 240, 75)           0         \n",
      "_________________________________________________________________\n",
      "MaxPool1 (MaxPooling1D)      (None, 48, 75)            0         \n",
      "_________________________________________________________________\n",
      "Conv_layer2 (Conv1D)         (None, 48, 75)            16950     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 48, 75)            300       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 48, 75)            0         \n",
      "_________________________________________________________________\n",
      "MaxPool2 (MaxPooling1D)      (None, 9, 75)             0         \n",
      "_________________________________________________________________\n",
      "Conv_layer3 (Conv1D)         (None, 9, 75)             16950     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 9, 75)             300       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 9, 75)             0         \n",
      "_________________________________________________________________\n",
      "MaxPool3 (MaxPooling1D)      (None, 1, 75)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_layer0 (Dense)         (None, 628)               47728     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 628)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 628)               2512      \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 1258      \n",
      "=================================================================\n",
      "Total params: 148,773\n",
      "Trainable params: 146,917\n",
      "Non-trainable params: 1,856\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7420 - acc: 0.4945 - recall: 0.4945 - precision: 0.4945 - val_loss: 0.6955 - val_acc: 0.4967 - val_recall: 0.4967 - val_precision: 0.4967\n",
      "\n",
      "Epoch 00001: saving model to models/cnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7253 - acc: 0.5015 - recall: 0.5015 - precision: 0.5015 - val_loss: 0.7244 - val_acc: 0.5033 - val_recall: 0.5033 - val_precision: 0.5033\n",
      "\n",
      "Epoch 00002: saving model to models/cnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7213 - acc: 0.4970 - recall: 0.4970 - precision: 0.4970 - val_loss: 0.7156 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00003: saving model to models/cnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7144 - acc: 0.4973 - recall: 0.4973 - precision: 0.4973 - val_loss: 0.6960 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00004: saving model to models/cnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7137 - acc: 0.4980 - recall: 0.4980 - precision: 0.4980 - val_loss: 0.7184 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00005: saving model to models/cnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7058 - acc: 0.5020 - recall: 0.5020 - precision: 0.5020 - val_loss: 0.6962 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00006: saving model to models/cnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7112 - acc: 0.4948 - recall: 0.4948 - precision: 0.4948 - val_loss: 0.6939 - val_acc: 0.5022 - val_recall: 0.5022 - val_precision: 0.5022\n",
      "\n",
      "Epoch 00007: saving model to models/cnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7089 - acc: 0.4980 - recall: 0.4980 - precision: 0.4980 - val_loss: 0.6996 - val_acc: 0.5011 - val_recall: 0.5011 - val_precision: 0.5011\n",
      "\n",
      "Epoch 00008: saving model to models/cnn_model.hdf5\n",
      "Epoch 9/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7088 - acc: 0.4922 - recall: 0.4922 - precision: 0.4922 - val_loss: 0.7226 - val_acc: 0.5022 - val_recall: 0.5022 - val_precision: 0.5022\n",
      "\n",
      "Epoch 00009: saving model to models/cnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7068 - acc: 0.4976 - recall: 0.4976 - precision: 0.4976 - val_loss: 0.6966 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00010: saving model to models/cnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7059 - acc: 0.5012 - recall: 0.5012 - precision: 0.5012 - val_loss: 0.6960 - val_acc: 0.4989 - val_recall: 0.4989 - val_precision: 0.4989\n",
      "\n",
      "Epoch 00011: saving model to models/cnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7070 - acc: 0.4905 - recall: 0.4905 - precision: 0.4905 - val_loss: 0.6970 - val_acc: 0.5055 - val_recall: 0.5055 - val_precision: 0.5055\n",
      "\n",
      "Epoch 00012: saving model to models/cnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7030 - acc: 0.5066 - recall: 0.5066 - precision: 0.5066 - val_loss: 0.7023 - val_acc: 0.4967 - val_recall: 0.4967 - val_precision: 0.4967\n",
      "\n",
      "Epoch 00013: saving model to models/cnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7089 - acc: 0.5037 - recall: 0.5037 - precision: 0.5037 - val_loss: 0.7128 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00014: saving model to models/cnn_model.hdf5\n",
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7114 - acc: 0.4951 - recall: 0.4951 - precision: 0.4951 - val_loss: 0.6983 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00015: saving model to models/cnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7051 - acc: 0.5138 - recall: 0.5138 - precision: 0.5138 - val_loss: 0.9714 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00016: saving model to models/cnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7036 - acc: 0.5074 - recall: 0.5074 - precision: 0.5074 - val_loss: 0.7305 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00017: saving model to models/cnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7063 - acc: 0.5074 - recall: 0.5074 - precision: 0.5074 - val_loss: 0.7420 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00018: saving model to models/cnn_model.hdf5\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7073 - acc: 0.4982 - recall: 0.4982 - precision: 0.4982 - val_loss: 0.6992 - val_acc: 0.5055 - val_recall: 0.5055 - val_precision: 0.5055\n",
      "\n",
      "Epoch 00019: saving model to models/cnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7056 - acc: 0.5017 - recall: 0.5017 - precision: 0.5017 - val_loss: 0.7408 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00020: saving model to models/cnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7102 - acc: 0.4966 - recall: 0.4966 - precision: 0.4966 - val_loss: 0.7403 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00021: saving model to models/cnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7049 - acc: 0.5027 - recall: 0.5027 - precision: 0.5027 - val_loss: 0.6943 - val_acc: 0.5044 - val_recall: 0.5044 - val_precision: 0.5044\n",
      "\n",
      "Epoch 00022: saving model to models/cnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7040 - acc: 0.5052 - recall: 0.5052 - precision: 0.5052 - val_loss: 0.7434 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00023: saving model to models/cnn_model.hdf5\n",
      "Epoch 24/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7065 - acc: 0.4989 - recall: 0.4989 - precision: 0.4989 - val_loss: 0.6964 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00024: saving model to models/cnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7002 - acc: 0.5124 - recall: 0.5124 - precision: 0.5124 - val_loss: 0.7156 - val_acc: 0.5033 - val_recall: 0.5033 - val_precision: 0.5033\n",
      "\n",
      "Epoch 00025: saving model to models/cnn_model.hdf5\n",
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7044 - acc: 0.5057 - recall: 0.5057 - precision: 0.5057 - val_loss: 0.7035 - val_acc: 0.5033 - val_recall: 0.5033 - val_precision: 0.5033\n",
      "\n",
      "Epoch 00026: saving model to models/cnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.7024 - acc: 0.5041 - recall: 0.5041 - precision: 0.5041 - val_loss: 0.7083 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00027: saving model to models/cnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6983 - acc: 0.5109 - recall: 0.5109 - precision: 0.5109 - val_loss: 0.6944 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00028: saving model to models/cnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6973 - acc: 0.5104 - recall: 0.5104 - precision: 0.5104 - val_loss: 0.7079 - val_acc: 0.5055 - val_recall: 0.5055 - val_precision: 0.5055\n",
      "\n",
      "Epoch 00029: saving model to models/cnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6953 - acc: 0.5182 - recall: 0.5182 - precision: 0.5182 - val_loss: 0.7461 - val_acc: 0.5022 - val_recall: 0.5022 - val_precision: 0.5022\n",
      "\n",
      "Epoch 00030: saving model to models/cnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6961 - acc: 0.5206 - recall: 0.5206 - precision: 0.5206 - val_loss: 0.7272 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00031: saving model to models/cnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6940 - acc: 0.5110 - recall: 0.5110 - precision: 0.5110 - val_loss: 0.7190 - val_acc: 0.5044 - val_recall: 0.5044 - val_precision: 0.5044\n",
      "\n",
      "Epoch 00032: saving model to models/cnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6922 - acc: 0.5139 - recall: 0.5139 - precision: 0.5139 - val_loss: 0.7012 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00033: saving model to models/cnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6942 - acc: 0.5115 - recall: 0.5115 - precision: 0.5115 - val_loss: 0.7397 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00034: saving model to models/cnn_model.hdf5\n",
      "Epoch 35/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6903 - acc: 0.5139 - recall: 0.5139 - precision: 0.5139 - val_loss: 0.7805 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00035: saving model to models/cnn_model.hdf5\n",
      "Epoch 36/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6883 - acc: 0.5150 - recall: 0.5150 - precision: 0.5150 - val_loss: 0.6951 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00036: saving model to models/cnn_model.hdf5\n",
      "Epoch 37/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6893 - acc: 0.5189 - recall: 0.5189 - precision: 0.5189 - val_loss: 0.7054 - val_acc: 0.4956 - val_recall: 0.4956 - val_precision: 0.4956\n",
      "\n",
      "Epoch 00037: saving model to models/cnn_model.hdf5\n",
      "Epoch 38/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6922 - acc: 0.5124 - recall: 0.5124 - precision: 0.5124 - val_loss: 0.7040 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00038: saving model to models/cnn_model.hdf5\n",
      "Epoch 39/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6853 - acc: 0.5271 - recall: 0.5271 - precision: 0.5271 - val_loss: 0.7048 - val_acc: 0.4956 - val_recall: 0.4956 - val_precision: 0.4956\n",
      "\n",
      "Epoch 00039: saving model to models/cnn_model.hdf5\n",
      "Epoch 40/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6883 - acc: 0.5096 - recall: 0.5096 - precision: 0.5096 - val_loss: 0.6971 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00040: saving model to models/cnn_model.hdf5\n",
      "Epoch 41/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6855 - acc: 0.5211 - recall: 0.5211 - precision: 0.5211 - val_loss: 0.7085 - val_acc: 0.4967 - val_recall: 0.4967 - val_precision: 0.4967\n",
      "\n",
      "Epoch 00041: saving model to models/cnn_model.hdf5\n",
      "Epoch 42/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6877 - acc: 0.5116 - recall: 0.5116 - precision: 0.5116 - val_loss: 0.7002 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00042: saving model to models/cnn_model.hdf5\n",
      "Epoch 43/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6887 - acc: 0.5110 - recall: 0.5110 - precision: 0.5110 - val_loss: 0.7114 - val_acc: 0.4989 - val_recall: 0.4989 - val_precision: 0.4989\n",
      "\n",
      "Epoch 00043: saving model to models/cnn_model.hdf5\n",
      "Epoch 44/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6859 - acc: 0.5141 - recall: 0.5141 - precision: 0.5141 - val_loss: 0.7122 - val_acc: 0.4978 - val_recall: 0.4978 - val_precision: 0.4978\n",
      "\n",
      "Epoch 00044: saving model to models/cnn_model.hdf5\n",
      "Epoch 45/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6893 - acc: 0.5077 - recall: 0.5077 - precision: 0.5077 - val_loss: 0.7285 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00045: saving model to models/cnn_model.hdf5\n",
      "Epoch 46/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6877 - acc: 0.5095 - recall: 0.5095 - precision: 0.5095 - val_loss: 0.7162 - val_acc: 0.4967 - val_recall: 0.4967 - val_precision: 0.4967\n",
      "\n",
      "Epoch 00046: saving model to models/cnn_model.hdf5\n",
      "Epoch 47/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6869 - acc: 0.5148 - recall: 0.5148 - precision: 0.5148 - val_loss: 0.7287 - val_acc: 0.4967 - val_recall: 0.4967 - val_precision: 0.4967\n",
      "\n",
      "Epoch 00047: saving model to models/cnn_model.hdf5\n",
      "Epoch 48/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6849 - acc: 0.5128 - recall: 0.5128 - precision: 0.5128 - val_loss: 0.7317 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00048: saving model to models/cnn_model.hdf5\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6860 - acc: 0.5083 - recall: 0.5083 - precision: 0.5083 - val_loss: 0.7236 - val_acc: 0.4923 - val_recall: 0.4923 - val_precision: 0.4923\n",
      "\n",
      "Epoch 00049: saving model to models/cnn_model.hdf5\n",
      "Epoch 50/50\n",
      "8200/8200 [==============================] - 10s 1ms/step - loss: 0.6863 - acc: 0.5152 - recall: 0.5152 - precision: 0.5152 - val_loss: 0.7041 - val_acc: 0.4978 - val_recall: 0.4978 - val_precision: 0.4978\n",
      "\n",
      "Epoch 00050: saving model to models/cnn_model.hdf5\n",
      "2278/2278 [==============================] - 2s 714us/step\n"
     ]
    }
   ],
   "source": [
    "cnn_res = train_and_test_model(cnn_model, trainCharData[:, :, 1:],\n",
    "                               train_y, \n",
    "                               testCharData[:, :, 1:], \n",
    "                               test_y, \n",
    "                               'cnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_rnn_model(rnn_input_len, char_maxlen, vocab_size,\n",
    "                         embed_matrix, n_RNN_nodes,\n",
    "                         n_rnn_layers=1, \n",
    "                         n_conv_layers=1,\n",
    "                         n_dense_layers=1,\n",
    "                         rnn_bidirectional=True,\n",
    "                         recurrent_dropout=None, \n",
    "                         activity_regularizer=None,\n",
    "                         recurrent_regularizer=None, \n",
    "                         dropout=None, \n",
    "                         batch_normalization=True,                         \n",
    "                         nb_filter=[100], \n",
    "                         filter_kernels = [4],\n",
    "                         pool_size=[3], \n",
    "                         cnn_padding='same', \n",
    "                         cnn_activation='elu',\n",
    "                         n_dense_nodes=100,\n",
    "                         dense_activation='elu',\n",
    "                         n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(rnn_input_len,), name='word_input_layer')\n",
    "    char_input = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "        \n",
    "    rnn_model = create_rnn_model(word_input, embed_matrix, n_RNN_nodes, \n",
    "                                 n_rnn_layers, rnn_bidirectional, \n",
    "                                 recurrent_dropout, activity_regularizer, \n",
    "                                 recurrent_regularizer, dropout, batch_normalization)    \n",
    "\n",
    "    cnn_model = create_cnn_model(char_input, char_maxlen, vocab_size,\n",
    "                                 n_conv_layers, nb_filter, filter_kernels,\n",
    "                                 pool_size, dropout, activity_regularizer,\n",
    "                                 batch_normalization, cnn_padding, cnn_activation)\n",
    "\n",
    "    merged_layer = Concatenate()([rnn_model, cnn_model])\n",
    "#     merged_layer = multiply([rnn_model, cnn_model])\n",
    "    \n",
    "    dense_layers = create_dense_layers(merged_layer,\n",
    "                                       n_dense_layers, \n",
    "                                       n_dense_nodes, \n",
    "                                       dropout, \n",
    "                                       batch_normalization,\n",
    "                                       activity_regularizer,\n",
    "                                       dense_activation)\n",
    "    \n",
    "    output_layer = Dense(n_out, activation='softmax',\n",
    "                         name='output_layer')(dense_layers)\n",
    "    \n",
    "    model = Model(inputs=[word_input, char_input], outputs=[output_layer])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_model = create_cnn_rnn_model(rnn_input_len=train_padded_words.shape[-1],\n",
    "                                     char_maxlen=char_maxlen, \n",
    "                                     vocab_size=vocab_size,\n",
    "                                     embed_matrix=word_embed_matrix, \n",
    "                                     n_RNN_nodes=550,\n",
    "                                     n_rnn_layers=1, \n",
    "                                     n_conv_layers=4,\n",
    "                                     n_dense_layers=1,\n",
    "                                     rnn_bidirectional=True,\n",
    "                                     recurrent_dropout=.4, \n",
    "                                     activity_regularizer=None,\n",
    "                                     recurrent_regularizer=None, \n",
    "                                     dropout=.2, \n",
    "                                     batch_normalization=True,                         \n",
    "                                     nb_filter=nb_filter, \n",
    "                                     filter_kernels = filter_kernels,\n",
    "                                     pool_size=pool_size, \n",
    "                                     cnn_padding='same', \n",
    "                                     cnn_activation='elu',\n",
    "                                     n_dense_nodes=400,\n",
    "                                     dense_activation='elu',\n",
    "                                     n_out=2)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input_layer (InputLayer)   (None, 1200, 86)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv_layer0 (Conv1D)            (None, 1200, 75)     45225       char_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 1200, 75)     300         Conv_layer0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1200, 75)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool0 (MaxPooling1D)         (None, 240, 75)      0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv_layer1 (Conv1D)            (None, 240, 75)      16950       MaxPool0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 240, 75)      300         Conv_layer1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 240, 75)      0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool1 (MaxPooling1D)         (None, 48, 75)       0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv_layer2 (Conv1D)            (None, 48, 75)       16950       MaxPool1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 48, 75)       300         Conv_layer2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 48, 75)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool2 (MaxPooling1D)         (None, 9, 75)        0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv_layer3 (Conv1D)            (None, 9, 75)        16950       MaxPool2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_input_layer (InputLayer)   (None, 102)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 9, 75)        300         Conv_layer3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 102, 300)     3534000     word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 9, 75)        0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1100)         3744400     word_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool3 (MaxPooling1D)         (None, 1, 75)        0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 1100)         4400        bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 75)           0           MaxPool3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1175)         0           batch_normalization_22[0][0]     \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer0 (Dense)            (None, 400)          470400      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 400)          0           dense_layer0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 400)          1600        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            802         batch_normalization_27[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 7,852,877\n",
      "Trainable params: 7,849,277\n",
      "Non-trainable params: 3,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 42s 5ms/step - loss: 1.0003 - acc: 0.4935 - recall: 0.4935 - precision: 0.4935 - val_loss: 2.5670 - val_acc: 0.4923 - val_recall: 0.4923 - val_precision: 0.4923\n",
      "\n",
      "Epoch 00001: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 37s 5ms/step - loss: 0.7646 - acc: 0.5300 - recall: 0.5300 - precision: 0.5300 - val_loss: 0.7065 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00002: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 37s 5ms/step - loss: 0.6940 - acc: 0.5743 - recall: 0.5743 - precision: 0.5743 - val_loss: 0.7810 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00003: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 37s 5ms/step - loss: 0.6490 - acc: 0.6271 - recall: 0.6271 - precision: 0.6271 - val_loss: 0.7931 - val_acc: 0.4770 - val_recall: 0.4770 - val_precision: 0.4770\n",
      "\n",
      "Epoch 00004: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 37s 5ms/step - loss: 0.5799 - acc: 0.6990 - recall: 0.6990 - precision: 0.6990 - val_loss: 0.8084 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00005: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 37s 5ms/step - loss: 0.5017 - acc: 0.7535 - recall: 0.7535 - precision: 0.7535 - val_loss: 0.8758 - val_acc: 0.5055 - val_recall: 0.5055 - val_precision: 0.5055\n",
      "\n",
      "Epoch 00006: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 37s 5ms/step - loss: 0.4303 - acc: 0.7998 - recall: 0.7998 - precision: 0.7998 - val_loss: 0.8962 - val_acc: 0.4923 - val_recall: 0.4923 - val_precision: 0.4923\n",
      "\n",
      "Epoch 00007: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 37s 5ms/step - loss: 0.3519 - acc: 0.8434 - recall: 0.8434 - precision: 0.8434 - val_loss: 1.0990 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00008: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200/8200 [==============================] - 37s 5ms/step - loss: 0.2820 - acc: 0.8771 - recall: 0.8771 - precision: 0.8771 - val_loss: 1.1075 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00009: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.2465 - acc: 0.9009 - recall: 0.9009 - precision: 0.9009 - val_loss: 1.3116 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00010: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.2173 - acc: 0.9099 - recall: 0.9099 - precision: 0.9099 - val_loss: 1.5557 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00011: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.2004 - acc: 0.9187 - recall: 0.9187 - precision: 0.9187 - val_loss: 1.2914 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00012: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.1896 - acc: 0.9229 - recall: 0.9229 - precision: 0.9229 - val_loss: 1.6132 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00013: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.1491 - acc: 0.9424 - recall: 0.9424 - precision: 0.9424 - val_loss: 1.8039 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00014: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.1410 - acc: 0.9438 - recall: 0.9438 - precision: 0.9438 - val_loss: 1.6799 - val_acc: 0.5241 - val_recall: 0.5241 - val_precision: 0.5241\n",
      "\n",
      "Epoch 00015: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.1107 - acc: 0.9550 - recall: 0.9550 - precision: 0.9550 - val_loss: 1.7270 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00016: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.1236 - acc: 0.9499 - recall: 0.9499 - precision: 0.9499 - val_loss: 1.6111 - val_acc: 0.5055 - val_recall: 0.5055 - val_precision: 0.5055\n",
      "\n",
      "Epoch 00017: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0992 - acc: 0.9579 - recall: 0.9579 - precision: 0.9579 - val_loss: 1.7773 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00018: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 19/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0930 - acc: 0.9607 - recall: 0.9607 - precision: 0.9607 - val_loss: 2.3946 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00019: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0910 - acc: 0.9618 - recall: 0.9618 - precision: 0.9618 - val_loss: 1.9978 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00020: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0764 - acc: 0.9643 - recall: 0.9643 - precision: 0.9643 - val_loss: 2.1703 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00021: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0713 - acc: 0.9645 - recall: 0.9645 - precision: 0.9645 - val_loss: 2.3518 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00022: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0795 - acc: 0.9644 - recall: 0.9644 - precision: 0.9644 - val_loss: 2.1698 - val_acc: 0.5373 - val_recall: 0.5373 - val_precision: 0.5373\n",
      "\n",
      "Epoch 00023: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 24/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0664 - acc: 0.9673 - recall: 0.9673 - precision: 0.9673 - val_loss: 2.3445 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00024: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0744 - acc: 0.9646 - recall: 0.9646 - precision: 0.9646 - val_loss: 2.6920 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00025: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.1026 - acc: 0.9573 - recall: 0.9573 - precision: 0.9573 - val_loss: 2.5382 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00026: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0982 - acc: 0.9568 - recall: 0.9568 - precision: 0.9568 - val_loss: 2.0089 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00027: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0671 - acc: 0.9687 - recall: 0.9687 - precision: 0.9687 - val_loss: 2.4124 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00028: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.1102 - acc: 0.9539 - recall: 0.9539 - precision: 0.9539 - val_loss: 2.7320 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00029: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.1043 - acc: 0.9533 - recall: 0.9533 - precision: 0.9533 - val_loss: 2.7300 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00030: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0661 - acc: 0.9660 - recall: 0.9660 - precision: 0.9660 - val_loss: 2.6200 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00031: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0594 - acc: 0.9690 - recall: 0.9690 - precision: 0.9690 - val_loss: 2.5595 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00032: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0556 - acc: 0.9716 - recall: 0.9716 - precision: 0.9716 - val_loss: 2.7052 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00033: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0727 - acc: 0.9676 - recall: 0.9676 - precision: 0.9676 - val_loss: 2.5672 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00034: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 35/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0914 - acc: 0.9635 - recall: 0.9635 - precision: 0.9635 - val_loss: 2.3731 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00035: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 36/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0881 - acc: 0.9632 - recall: 0.9632 - precision: 0.9632 - val_loss: 2.6477 - val_acc: 0.5219 - val_recall: 0.5219 - val_precision: 0.5219\n",
      "\n",
      "Epoch 00036: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 37/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0671 - acc: 0.9672 - recall: 0.9672 - precision: 0.9672 - val_loss: 2.9211 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00037: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 38/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0569 - acc: 0.9716 - recall: 0.9716 - precision: 0.9716 - val_loss: 2.9112 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00038: saving model to models/cnn_rnn_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0517 - acc: 0.9728 - recall: 0.9728 - precision: 0.9728 - val_loss: 2.9996 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00039: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 40/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0608 - acc: 0.9707 - recall: 0.9707 - precision: 0.9707 - val_loss: 3.0503 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00040: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 41/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0783 - acc: 0.9648 - recall: 0.9648 - precision: 0.9648 - val_loss: 2.4229 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00041: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 42/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0669 - acc: 0.9668 - recall: 0.9668 - precision: 0.9668 - val_loss: 2.9039 - val_acc: 0.5241 - val_recall: 0.5241 - val_precision: 0.5241\n",
      "\n",
      "Epoch 00042: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 43/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0675 - acc: 0.9687 - recall: 0.9687 - precision: 0.9687 - val_loss: 2.8676 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00043: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 44/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0715 - acc: 0.9668 - recall: 0.9668 - precision: 0.9668 - val_loss: 2.7999 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00044: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 45/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0508 - acc: 0.9702 - recall: 0.9702 - precision: 0.9702 - val_loss: 2.9874 - val_acc: 0.5219 - val_recall: 0.5219 - val_precision: 0.5219\n",
      "\n",
      "Epoch 00045: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 46/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0463 - acc: 0.9734 - recall: 0.9734 - precision: 0.9734 - val_loss: 2.9784 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00046: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 47/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0469 - acc: 0.9741 - recall: 0.9741 - precision: 0.9741 - val_loss: 3.1452 - val_acc: 0.5241 - val_recall: 0.5241 - val_precision: 0.5241\n",
      "\n",
      "Epoch 00047: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 48/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0478 - acc: 0.9733 - recall: 0.9733 - precision: 0.9733 - val_loss: 3.1007 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00048: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 49/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0418 - acc: 0.9761 - recall: 0.9761 - precision: 0.9761 - val_loss: 3.1135 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00049: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 50/50\n",
      "8200/8200 [==============================] - 38s 5ms/step - loss: 0.0439 - acc: 0.9745 - recall: 0.9745 - precision: 0.9745 - val_loss: 3.2009 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00050: saving model to models/cnn_rnn_model.hdf5\n",
      "2278/2278 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_res = train_and_test_model(cnn_rnn_model, \n",
    "                               [train_padded_words, trainCharData[:, :, 1:]],\n",
    "                               train_y, \n",
    "                               [test_padded_words, testCharData[:, :, 1:]],\n",
    "                               test_y, \n",
    "                               'cnn_rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.engine.training.Model at 0x7fbb35ff39e8>,\n",
       " 0.4991220369791126,\n",
       " 0.4991220369791126,\n",
       " 0.4991220369791126)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_rnn_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put results in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(rnn_input_len,\n",
    "                 char_maxlen, \n",
    "                 vocab_size,\n",
    "                 embed_matrix, \n",
    "                 n_RNN_nodes,\n",
    "                 n_dense_nodes,\n",
    "                 n_rnn_layers, \n",
    "                 n_conv_layers,\n",
    "                 n_dense_layers,\n",
    "                 rnn_bidirectional,\n",
    "                 recurrent_dropout, \n",
    "                 activity_regularizer,\n",
    "                 recurrent_regularizer, \n",
    "                 dropout, \n",
    "                 batch_normalization,                         \n",
    "                 nb_filter,\n",
    "                 filter_kernels,\n",
    "                 pool_size, \n",
    "                 cnn_padding, \n",
    "                 cnn_activation,\n",
    "                 dense_activation,\n",
    "                 n_out=2):\n",
    "    \n",
    "    cnn_model = create_cnn_standalone_model(char_maxlen=char_maxlen,\n",
    "                                            vocab_size=vocab_size,\n",
    "                                            n_dense_nodes=n_dense_nodes,\n",
    "                                            n_dense_layers=n_dense_layers, \n",
    "                                            n_conv_layers=n_conv_layers,\n",
    "                                            nb_filter=nb_filter,\n",
    "                                            filter_kernels=filter_kernels,\n",
    "                                            pool_size=pool_size, \n",
    "                                            dropout=dropout,\n",
    "                                            activity_regularizer=activity_regularizer,\n",
    "                                            batch_normalization=batch_normalization,\n",
    "                                            cnn_padding=cnn_padding,\n",
    "                                            cnn_activation=cnn_activation,\n",
    "                                            dense_activation=dense_activation)\n",
    "\n",
    "    rnn_model = create_rnn_standalone_model(seq_input_len=rnn_input_len,\n",
    "                                            embed_matrix=word_embed_matrix,\n",
    "                                            n_RNN_nodes=n_RNN_nodes,\n",
    "                                            n_dense_nodes=n_dense_nodes, \n",
    "                                            n_out=n_out, \n",
    "                                            n_dense_layers=n_dense_layers,\n",
    "                                            n_rnn_layers=n_rnn_layers, \n",
    "                                            rnn_bidirectional=rnn_bidirectional,\n",
    "                                            recurrent_dropout=recurrent_dropout, \n",
    "                                            activity_regularizer=activity_regularizer,\n",
    "                                            recurrent_regularizer=recurrent_regularizer,\n",
    "                                            dropout=dropout,\n",
    "                                            batch_normalization=batch_normalization,\n",
    "                                            dense_activation=dense_activation)\n",
    "\n",
    "    cnn_rnn_model = create_cnn_rnn_model(rnn_input_len=rnn_input_len,\n",
    "                                         char_maxlen=char_maxlen, \n",
    "                                         vocab_size=vocab_size,\n",
    "                                         embed_matrix=word_embed_matrix, \n",
    "                                         n_RNN_nodes=n_RNN_nodes,\n",
    "                                         n_rnn_layers=n_rnn_layers, \n",
    "                                         n_conv_layers=n_conv_layers,\n",
    "                                         n_dense_layers=n_dense_layers,\n",
    "                                         rnn_bidirectional=rnn_bidirectional,\n",
    "                                         recurrent_dropout=recurrent_dropout, \n",
    "                                         activity_regularizer=activity_regularizer,\n",
    "                                         recurrent_regularizer=recurrent_regularizer, \n",
    "                                         dropout=dropout, \n",
    "                                         batch_normalization=batch_normalization,                         \n",
    "                                         nb_filter=nb_filter, \n",
    "                                         filter_kernels = filter_kernels,\n",
    "                                         pool_size=pool_size, \n",
    "                                         cnn_padding=cnn_padding, \n",
    "                                         cnn_activation=cnn_activation,\n",
    "                                         n_dense_nodes=n_dense_nodes,\n",
    "                                         dense_activation=dense_activation,\n",
    "                                         n_out=n_out)\n",
    "    \n",
    "    return {'cnn_model': cnn_model,\n",
    "            'rnn_model': rnn_model,\n",
    "            'cnn_rnn_model': cnn_rnn_model,\n",
    "            'rnn_input_len': rnn_input_len,\n",
    "            'char_maxlen': char_maxlen, \n",
    "            'n_RNN_nodes': n_RNN_nodes,\n",
    "            'n_rnn_layers': n_rnn_layers, \n",
    "            'n_conv_layers': n_conv_layers,\n",
    "            'n_dense_layers': n_dense_layers,\n",
    "            'rnn_bidirectional': rnn_bidirectional,\n",
    "            'recurrent_dropout': recurrent_dropout, \n",
    "            'activity_regularizer': activity_regularizer,\n",
    "            'recurrent_regularizer':  recurrent_regularizer, \n",
    "            'dropout': dropout, \n",
    "            'batch_normalization': batch_normalization,                         \n",
    "            'nb_filter': nb_filter, \n",
    "            'pool_size': pool_size, \n",
    "            'cnn_padding': cnn_padding, \n",
    "            'cnn_activation': cnn_activation,\n",
    "            'n_dense_nodes': n_dense_nodes,\n",
    "            'dense_activation': dense_activation\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {'n_RNN_nodes': [210, 340, 550],\n",
    "                  'n_dense_nodes': [200, 400, 600],\n",
    "                  'n_rnn_layers': [1, 2, 3], \n",
    "                  'n_conv_layers': [2, 3, 4],\n",
    "                  'n_dense_layers': [1, 2],\n",
    "                  'rnn_bidirectional': [True],\n",
    "                  'recurrent_dropout': [None, .4],\n",
    "                  'dropout': [None, .4],\n",
    "                  'activity_regularizer': [None, regularizers.l2()],\n",
    "                  'recurrent_regularizer': [None, regularizers.l2()],\n",
    "                  'batch_normalization': [True, False],\n",
    "                  'nb_filter':  [[25], [75], [125]], \n",
    "                  'pool_size': [[3], [4], [5]], \n",
    "                  'cnn_padding': ['same'], \n",
    "                  'cnn_activation': ['elu'],\n",
    "                  'dense_activation': ['elu'],\n",
    "                  'n_out': [2]}\n",
    "\n",
    "\n",
    "totalIter = 0\n",
    "for key, val in hyperparameters.items():\n",
    "    totalIter += len(val) - 1\n",
    "    \n",
    "def hyperparameter_search(hyperparameters):\n",
    "\n",
    "    n_out=2\n",
    "    nb_epoch = 30\n",
    "    char_maxlen = 1200 \n",
    "    vocab_size=vocab_size\n",
    "    filter_kernels = [7, 5, 3, 3]\n",
    "    rnn_input_len=train_padded_words.shape[-1]\n",
    "    \n",
    "    currentIterDict = {}\n",
    "    for key in hyperparameters.keys():\n",
    "        currentIterDict[key] = hyperparameters[key][0]\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    models = build_models(rnn_input_len=rnn_input_len,\n",
    "                          char_maxlen=char_maxlen, \n",
    "                          vocab_size=vocab_size,\n",
    "                          embed_matrix=word_embed_matrix, \n",
    "                          filter_kernels=filter_kernels,\n",
    "                          n_RNN_nodes=currentIterDict['n_RNN_nodes'],\n",
    "                          n_rnn_layers=currentIterDict['n_rnn_layers'],\n",
    "                          n_conv_layers=currentIterDict['n_conv_layers'],\n",
    "                          n_dense_layers=currentIterDict['n_dense_layers'],\n",
    "                          recurrent_dropout=currentIterDict['recurrent_dropout'],\n",
    "                          activity_regularizer=currentIterDict['activity_regularizer'],\n",
    "                          recurrent_regularizer=currentIterDict['recurrent_regularizer'],\n",
    "                          dropout=currentIterDict['dropout'],\n",
    "                          batch_normalization=currentIterDict['batch_normalization'],\n",
    "                          nb_filter=currentIterDict['nb_filter'],\n",
    "                          pool_size=currentIterDict['pool_size'],\n",
    "                          cnn_padding=currentIterDict['cnn_padding'],\n",
    "                          cnn_activation=currentIterDict['cnn_activation'],\n",
    "                          n_dense_nodes=currentIterDict['n_dense_nodes'],\n",
    "                          dense_activation=currentIterDict['dense_activation']\n",
    "                          rnn_bidirectional=currentIterDict['rnn_bidirectional'],\n",
    "                          n_out=n_out)\n",
    "    \n",
    "    pd.DataFrame.from_records([cnn_res[1:4], rnn_res[1:4], cnn_rnn_res[1:4]], \n",
    "                          columns=['accuracy', 'recall', 'precision'], \n",
    "                         index=['cnn_mod', 'rnn_mod', 'cnn_rnn_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cnn_mod</th>\n",
       "      <td>0.494293</td>\n",
       "      <td>0.494293</td>\n",
       "      <td>0.494293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_mod</th>\n",
       "      <td>0.505707</td>\n",
       "      <td>0.505707</td>\n",
       "      <td>0.505707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_rnn_mod</th>\n",
       "      <td>0.499122</td>\n",
       "      <td>0.499122</td>\n",
       "      <td>0.499122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy    recall  precision\n",
       "cnn_mod      0.494293  0.494293   0.494293\n",
       "rnn_mod      0.505707  0.505707   0.505707\n",
       "cnn_rnn_mod  0.499122  0.499122   0.499122"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records([cnn_res[1:4], rnn_res[1:4], cnn_rnn_res[1:4]], \n",
    "                          columns=['accuracy', 'recall', 'precision'], \n",
    "                         index=['cnn_mod', 'rnn_mod', 'cnn_rnn_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classifications(classifications, classType, test_y, test_text):\n",
    "    texts = [' '.join(sent) for sent in test_text[classifications.index]]\n",
    "    stock_movements = np.where(test_y[classifications.index], 'positive', 'negative')\n",
    "    \n",
    "    print('Examples of {} predictions:\\n'.format(classType))\n",
    "    for i in range(len(texts)):\n",
    "        print('Stock movement was {}'.format(stock_movements[i]))\n",
    "        print('News info:\\n{}'.format(texts[i]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print_samples(model, modelName, test_x, test_y=test['y'], test_text = test['final_text']):\n",
    "    \"\"\"\"Print out predictions of the model\"\"\"\n",
    "    print('Stats for {} model'.format(modelName))\n",
    "    \n",
    "    res = model.predict(test_x)\n",
    "    class_res = np.apply_along_axis(np.argmax, axis=1, arr=res)\n",
    "\n",
    "    comparisons = class_res == test_y\n",
    "    good_class = comparisons.loc[comparisons == True].sample(n=3)\n",
    "    bad_class = comparisons.loc[comparisons == False].sample(n=3)\n",
    "\n",
    "    print_classifications(good_class, 'correct', test_y, test_text)\n",
    "    print_classifications(bad_class, 'INcorrect', test_y, test_text)\n",
    "\n",
    "    \n",
    "    top3MostProbPosArg = np.argsort(res[:, 1])[-3:]\n",
    "    top3Y = test_y.iloc[top3MostProbPosArg]\n",
    "    top3Probs = pd.Series(res[top3MostProbPosArg, 1], index=top3Y.index)\n",
    "    top3Data = pd.concat([top3Y, top3Probs], axis=1)\n",
    "    top3Data.columns = ['Actual', 'PositiveProb']\n",
    "    print('')\n",
    "    print('Top 3 Most Positive Probability:')\n",
    "    print(top3Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Taiwan's UMC posts 2015 net profit Further company coverage 1 32 7890 Taiwan dollars Reporting by Hong Kong newsroom\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Exxon carries out major evacuation from Iraq oil official\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Lone Star buys 900 mln stg Lloyds' debt portfolio United States private equity firm Lone Star said it had bought Lloyds Banking Group's Project Royal portfolio of debt worth more than 900 million pounds 1 4 billion at a price that sources said was a discount of up to 40 percent\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "United States FDA approves Amgen leukemia drug ahead of schedule United States health regulators on Wednesday approved an Amgen Inc drug that helps the immune system fight a rare type of leukemia more than five months ahead of the expected decision date\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "UK court rules in favour of Lloyds in £3 3bn bond dispute IFR Lloyds Banking Group won a decisive UK court ruling on Thursday over whether it treated investors in high interest bonds fairly saving it from the threat of paying them hundreds of millions of pounds extra\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "paid outgoing CEO Haste 2 8 mln stg for 2011 Outgoing CEO Haste received 2 77 million pounds in total pay for 2011\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "BAX    20161025 9907      1.0      0.512650\n",
      "GOOGL  20150310 24030     0.0      0.512688\n",
      "CBS    20130515 17407     1.0      0.512714\n",
      "Stats for CNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Medtronic's profit beats estimates on lower costs Medtronic Plc the world's largest standalone medical device maker reported a better than expected quarterly profit helped by lower costs and a lighter tax burden\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Pfizer to buy antibiotics business from AstraZeneca Pharmaceutical company AstraZeneca has agreed to sell its small molecule antibiotics business to Pfizer Inc in a deal that could be valued at more than 1 5 billion  Video\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Exclusive BP gags in house lawyer on oil spill lawsuits Oil giant BP has succeeded in preventing the public airing of comments from a senior in house lawyer about lawsuits stemming from the Gulf of Mexico oil spill as part of a legal claim of discrimination\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Activists meet to defend Internet from state control NAIROBI Internet activists accused governments of making it difficult for users of the Web rights campaigners and private businesses to carry out their work through state attempts to seize control of the Web\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "For United States tech firms China entices in spite of tight state control Sept 12 The sheer size of the Chinese market is so alluring to Western companies that even pro internet freedom firms like CloudFlare may have to put moral outrage to the side in their pursuit of new business\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Asia oil and gas banker Martinez quits HSBC to join Barclays source HSBC plc's head of oil and gas banking in Asia Jorge Martinez has left the firm to join Barclays in a similar position a person familiar with the matter told Reuters on Wednesday\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "MDT    20131205 30232     1.0      0.498905\n",
      "RTN    20160329 34772     0.0      0.498905\n",
      "AAPL   20141015 4447      0.0      0.498905\n",
      "Stats for CNN_RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Barclays loses semiconductor banker Righellis sources Zach Righellis a managing director at Barclays Plc who specializes in semiconductors and electronics has left the investment bank sources familiar with the matter said on Tuesday\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "reaches 18 7 billion settlement over deadly 2010 spill Plc will pay up to 18 7 billion in penalties to the United States government and five states to resolve nearly all claims from its deadly Gulf of Mexico oil spill five years ago in the largest corporate settlement in United States history\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "United States offshore oil producers brace for Storm Isaac Oil and gas producers in the Gulf of Mexico started preparing for Tropical Storm Isaac on Friday as its track looked to skirt the heart of the United States offshore energy producing zone\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Medtronic recalls loading system for heart device Medtronic Plc has recalled 6 912 units of loading system of a recently approved heart device after reports of the presence of particulates\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Salesforce com CEO speaks of failed efforts to buy Twitter LinkedIn Salesforce com Inc Chief Executive Marc Benioff spoke on Wednesday about a pair of key acquisitions that got away suggesting his vision for LinkedIn was different from Microsoft's and that he would have pursued Twitter if shareholders had not learned of his plans\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Run from the bear or grin at it The stock market dropped briefly into bear market territory on Monday What does an investor do while the market flirts with a big drop\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "WFC    20120720 43299     1.0           1.0\n",
      "RTN    20121220 34394     1.0           1.0\n",
      "P      20170112 31689     1.0           1.0\n"
     ]
    }
   ],
   "source": [
    "predict_and_print_samples(rnn_res[0], 'RNN', test_padded_words)\n",
    "\n",
    "predict_and_print_samples(cnn_res[0], 'CNN', testCharData[:, :, 1:])\n",
    "\n",
    "predict_and_print_samples(cnn_rnn_res[0], 'CNN_RNN', [test_padded_words, testCharData[:, :, 1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
