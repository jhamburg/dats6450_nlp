{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Utility libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import calendar\n",
    "\n",
    "# Prepocessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Concatenate, TimeDistributed\n",
    "from keras.layers import Dense, Bidirectional, Dropout, Flatten, merge \n",
    "from keras.layers import Conv1D, Conv2D, MaxPool1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '../data'\n",
    "reutersFile = 'news_reuters.csv'\n",
    "stockFile = 'stockReturns.json'\n",
    "\n",
    "rawX = pd.read_csv(os.path.join(dataPath, reutersFile), header=None, \n",
    "                   names=['ticker', 'company', 'pub_date', 'headline', 'first_sent', 'category'])\n",
    "rawY = pd.read_json(os.path.join(dataPath, stockFile))\n",
    "# rawY = json.load(os.path.join(dataPath, stockFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_y_data(data, tickerType='mid'):\n",
    "    \"\"\"Convert stock data into binary postive/negative\"\"\"\n",
    "    tmp = data[tickerType].apply(pd.Series)\n",
    "    tmp = tmp.stack().rename('price', inplace=True).reset_index()\n",
    "    tmp['y'] = np.where(tmp['price'] >= 0, 1, 0)\n",
    "    tmp.rename(columns={'level_0': 'ticker', 'level_1': 'pub_date'}, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def clean_and_merge_data(X, Y):\n",
    "    \"\"\"Filter X to only those tickers with stock data\"\"\"\n",
    "    y_tickers = set(Y['ticker'])\n",
    "    X = X.loc[X['ticker'].isin(y_tickers)]\n",
    "    # Make sure data types are the same for merge    \n",
    "    Y['pub_date'] = Y['pub_date'].astype(rawX['pub_date'].dtype)\n",
    "    Y['ticker'] = Y['ticker'].astype(rawX['ticker'].dtype)\n",
    "    return X.merge(Y, on=['ticker', 'pub_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanY = reformat_y_data(rawY, 'short')\n",
    "\n",
    "merged = clean_and_merge_data(rawX, cleanY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up text columns and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    \"\"\"Clean up text data by:\n",
    "    \n",
    "    1. Replacing double spaces into a single space\n",
    "    2. Replace U.S. to United States so U won't get deleted with next \n",
    "       replacement\n",
    "    3. Remove all capitalized words at the beginning of the \n",
    "       sentence, since those are mostly places (aka NEW YORK)\n",
    "    4. Remove unnecessary punctuation (hyphens and asterisks)\n",
    "    5. Remove dates\n",
    "    \"\"\"\n",
    "    monthStrings = list(calendar.month_name)[1:] + list(calendar.month_abbr)[1:]\n",
    "    monthPattern = '|'.join(monthStrings)\n",
    "    \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    sent = re.sub(r'U.S.', 'United States', sent)\n",
    "    sent = re.sub(r'^(\\W?[A-Z\\s\\d]+\\b-?)', '', sent)\n",
    "    sent = re.sub(r'^ ?\\W ', '', sent)\n",
    "    sent = re.sub(r'({}) \\d+'.format(monthPattern), '', sent)\n",
    "    \n",
    "    # replace double spaces one more time after previous cleaning \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    return sent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(col):\n",
    "    \"\"\"Tokenize string into a sequence of words\"\"\"\n",
    "    return [text_to_word_sequence(text, lower=False) for text in col]\n",
    "\n",
    "def filt_to_one(x, random_state=10):\n",
    "    \"\"\"Filter dataset so that there is only one observation per day.\n",
    "    \n",
    "    If there is more than one record, will use the topStory record\n",
    "    if one exists.  If one doesn't or there are 2 topStory records\n",
    "    then it will randomly select one of the observations.\n",
    "    \"\"\"\n",
    "    if x.shape[0] > 1:\n",
    "        if 'topStory' in x['category'].unique():\n",
    "            x = x.loc[x['category'] == 'topStory']\n",
    "        if x.shape[0] > 1:\n",
    "            x = x.sample(n=1, random_state=random_state)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up text\n",
    "merged['headline'] = merged.headline.apply(clean_text)\n",
    "merged['first_sent'] = merged.first_sent.apply(clean_text)\n",
    "\n",
    "# Turn sentences into tokens\n",
    "merged['headline_token'] = tokenize_sent(merged.headline)\n",
    "merged['first_sent_token'] = tokenize_sent(merged.first_sent)\n",
    "\n",
    "# Get one record per company/day\n",
    "finalData = merged.groupby(by=['ticker', 'pub_date']).apply(filt_to_one)\n",
    "\n",
    "# Combine Headline and First Sentence into one text \n",
    "finalData['final_text'] = finalData['headline_token'] + finalData.first_sent_token\n",
    "\n",
    "# Remove observations with missing stock price\n",
    "finalData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "train, test = train_test_split(finalData, test_size = .2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Lexicon and Transform Data to Integers for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lexiconTransformer():\n",
    "    \"\"\"Create a lexicon and transform sentences and\n",
    "       to indexes for use in the model.\"\"\"\n",
    "    \n",
    "    def __init__(self, words_min_freq = 1, unknown_word_token = u'<UNK>',\n",
    "                 savePath='models', saveName='stock_word_lexicon'):\n",
    "        self.words_min_freq = words_min_freq\n",
    "        self.words_lexicon = None\n",
    "        self.unknown_word_token = unknown_word_token\n",
    "        self.indx_to_words_dict = None\n",
    "        self.savePath = savePath\n",
    "        self.saveName = saveName + '.pkl'\n",
    "    \n",
    "    def fit(self, sents):\n",
    "        \"\"\"Create lexicon based on sentences\"\"\"\n",
    "        self.make_words_lexicon(sents)        \n",
    "        self.make_lexicon_reverse()\n",
    "        self.save_lexicon()\n",
    "                \n",
    "    def transform(self, sents):\n",
    "        sents_indxs = self.tokens_to_idxs(sents, self.words_lexicon)\n",
    "        return sents_indxs\n",
    "\n",
    "    def fit_transform(self, sents):\n",
    "        self.fit(sents)\n",
    "        return self.transform(sents)\n",
    "        \n",
    "    def make_words_lexicon(self, sents_token):\n",
    "        \"\"\"Wrapper for words lexicon\"\"\"\n",
    "        self.words_lexicon = self.make_lexicon(sents_token, self.words_min_freq,\n",
    "                                               self.unknown_word_token)\n",
    "\n",
    "    def make_lexicon(self, token_seqs, min_freq=1, unknown = u'<UNK>'):\n",
    "        \"\"\"Create lexicon from input based on a frequency\n",
    "\n",
    "            Parameters:\n",
    "            \n",
    "            token_seqs\n",
    "            ----------\n",
    "               A list of a list of input tokens that will be used to create the lexicon\n",
    "            \n",
    "            min_freq\n",
    "            --------\n",
    "               Number of times the token needs to be in the corpus to be included in the\n",
    "               lexicon.  Otherwise, will be replaced with the \"unknown\" entry\n",
    "            \n",
    "            unknown\n",
    "            -------\n",
    "               The word in the lexicon that should be used for tokens not existing in lexicon.\n",
    "               This can be a value that already exists in input list.  For instance, in \n",
    "               Named Entity Recognition, a value of \"other\" or \"O\" may already be a tag \n",
    "               and so having \"other\" and \"unknown\" are the same thing!\n",
    "        \"\"\"\n",
    "        # Count how often each word appears in the text.\n",
    "        token_counts = {}\n",
    "        for seq in token_seqs:\n",
    "            for token in seq:\n",
    "                if token in token_counts:\n",
    "                    token_counts[token] += 1\n",
    "                else:\n",
    "                    token_counts[token] = 1\n",
    "\n",
    "        # Then, assign each word to a numerical index. \n",
    "        # Filter words that occur less than min_freq times.\n",
    "        lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "        \n",
    "        # Have to delete unknown value from token list so not a gap in lexicon values when\n",
    "        # turning it into a lexicon (aka, if unknown == OTHER and that is the 7th value, \n",
    "        # then 7 won't exist in the lexicon which may cause issues)\n",
    "        if unknown in lexicon:\n",
    "            lexicon.remove(unknown)\n",
    "\n",
    "        # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "        lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "        \n",
    "        lexicon[unknown] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "        lexicon_size = len(lexicon)\n",
    "        return lexicon\n",
    "    \n",
    "    def save_lexicon(self):\n",
    "        \"Save lexicons by pickling them\"\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "        with open(os.path.join(self.savePath, self.saveName), 'wb') as f:\n",
    "            pickle.dump(self.words_lexicon, f)\n",
    "                        \n",
    "    def load_lexicon(self):\n",
    "        with open(os.path.join(self.savePath, self.saveName), 'rb') as f:\n",
    "            self.words_lexicon = pickle.load(f)\n",
    "                    \n",
    "        self.make_lexicon_reverse()\n",
    "        \n",
    "    def make_lexicon_reverse(self):\n",
    "        self.indx_to_words_dict = self.get_lexicon_lookup(self.words_lexicon)\n",
    "    \n",
    "    def get_lexicon_lookup(self, lexicon):\n",
    "        '''Make a dictionary where the string representation of \n",
    "           a lexicon item can be retrieved from its numerical index'''\n",
    "        lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "        return lexicon_lookup\n",
    "    \n",
    "    def tokens_to_idxs(self, token_seqs, lexicon):\n",
    "        \"\"\"Transform tokens to numeric indexes or <UNK> if doesn't exist\"\"\"\n",
    "        idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in token_seq] for token_seq in token_seqs]\n",
    "        return idx_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = lexiconTransformer(words_min_freq=2)\n",
    "\n",
    "lexicon.fit(train['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train['finalText_indx'] = lexicon.transform(train['final_text'])\n",
    "test['finalText_indx'] = lexicon.transform(test['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_seq_len(sents):\n",
    "    return max([len(idx_seq) for idx_seq in sents])\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of longest sequence\n",
    "max_seq_len = get_max_seq_len(train['finalText_indx'])\n",
    "\n",
    "#Add one to max length for offsetting sequence by 1\n",
    "train_padded_words = pad_idx_seqs(train['finalText_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "test_padded_words = pad_idx_seqs(test['finalText_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "train_y = to_categorical(train['y'])\n",
    "test_y = to_categorical(test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 2\n",
    "nb_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed_matrix(model, lexicon, embed_size):\n",
    "    \"Create a weight matrix for words\"\n",
    "    vocab_size = len(lexicon)\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_size))\n",
    "    n = 0\n",
    "    word_list = list(lexicon)\n",
    "    for i in range(vocab_size):\n",
    "        word = word_list[i]\n",
    "        if word in model.wv.vocab:\n",
    "            embedding_vector = model.wv[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[n] = embedding_vector[:embed_size]\n",
    "                n += 1\n",
    "\n",
    "    return embedding_matrix[:n, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "word_embed_len = 200\n",
    "word_embed_matrix = create_embed_matrix(w2v, lexicon.words_lexicon, \n",
    "                                   word_embed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(seq_input_len, embed_matrix, \n",
    "                     n_RNN_nodes, n_dense_nodes, \n",
    "                     recurrent_dropout=0.2, \n",
    "                     drop_out=.2, n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(seq_input_len,), name='word_input_layer')\n",
    "        \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    hidden_layer1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n",
    "                                      recurrent_dropout=recurrent_dropout, \n",
    "                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n",
    "    \n",
    "    hidden_layer2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      dropout=drop_out, name='hidden_layer2'))(hidden_layer1)\n",
    "\n",
    "    dense_layer = Dense(units=n_dense_nodes, activation='relu', name='dense_layer')(hidden_layer2)\n",
    "\n",
    "    drop_out3 = Dropout(drop_out)(dense_layer)\n",
    "\n",
    "    output_layer = Dense(units=n_out, activation='softmax',\n",
    "                         name='output_layer')(drop_out3)\n",
    "\n",
    "    model = Model(inputs=[word_input], outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = create_rnn_model(seq_input_len=train_padded_words.shape[-1],\n",
    "                             embed_matrix=word_embed_matrix, drop_out=.5,\n",
    "                             n_RNN_nodes=200, n_dense_nodes=200, n_out=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(model, x_train, y_train, x_test, y_test, \n",
    "                         modelSaveName, modelSavePath='models',\n",
    "                         batch_size=128, epochs=3, validation_split=.1):\n",
    "    \"\"\"Train model, save weights, and predict data\"\"\"\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    filepath = os.path.join(modelSavePath, modelSaveName + '.hdf5')\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(x=x_train, y=y_train, batch_size=batch_size, \n",
    "              epochs=epochs, validation_split=validation_split, \n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    score, acc, rec, prec = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    return (model, acc, rec, prec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input_layer (InputLayer (None, 102)               0         \n",
      "_________________________________________________________________\n",
      "word_embedding_layer (Embedd (None, 102, 200)          2356000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 400)          641600    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 4,039,802\n",
      "Trainable params: 4,039,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 50s 6ms/step - loss: 0.6968 - acc: 0.5002 - recall: 0.5002 - precision: 0.5002 - val_loss: 0.6942 - val_acc: 0.4825 - val_recall: 0.4825 - val_precision: 0.4825\n",
      "\n",
      "Epoch 00001: saving model to models/rnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.6926 - acc: 0.5220 - recall: 0.5220 - precision: 0.5220 - val_loss: 0.6930 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00002: saving model to models/rnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.6762 - acc: 0.5700 - recall: 0.5700 - precision: 0.5700 - val_loss: 0.7096 - val_acc: 0.4934 - val_recall: 0.4934 - val_precision: 0.4934\n",
      "\n",
      "Epoch 00003: saving model to models/rnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.6279 - acc: 0.6493 - recall: 0.6493 - precision: 0.6493 - val_loss: 0.7386 - val_acc: 0.5417 - val_recall: 0.5417 - val_precision: 0.5417\n",
      "\n",
      "Epoch 00004: saving model to models/rnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.5506 - acc: 0.7202 - recall: 0.7202 - precision: 0.7202 - val_loss: 0.7800 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00005: saving model to models/rnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.4623 - acc: 0.7874 - recall: 0.7874 - precision: 0.7874 - val_loss: 0.9614 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00006: saving model to models/rnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.3947 - acc: 0.8216 - recall: 0.8216 - precision: 0.8216 - val_loss: 0.9531 - val_acc: 0.5219 - val_recall: 0.5219 - val_precision: 0.5219\n",
      "\n",
      "Epoch 00007: saving model to models/rnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.3295 - acc: 0.8580 - recall: 0.8580 - precision: 0.8580 - val_loss: 1.0715 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00008: saving model to models/rnn_model.hdf5\n",
      "Epoch 9/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.2852 - acc: 0.8788 - recall: 0.8788 - precision: 0.8788 - val_loss: 1.2009 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00009: saving model to models/rnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.2431 - acc: 0.9001 - recall: 0.9001 - precision: 0.9001 - val_loss: 1.5049 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00010: saving model to models/rnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.2187 - acc: 0.9140 - recall: 0.9140 - precision: 0.9140 - val_loss: 1.5229 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00011: saving model to models/rnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.1952 - acc: 0.9206 - recall: 0.9206 - precision: 0.9206 - val_loss: 1.6804 - val_acc: 0.5384 - val_recall: 0.5384 - val_precision: 0.5384\n",
      "\n",
      "Epoch 00012: saving model to models/rnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.1662 - acc: 0.9341 - recall: 0.9341 - precision: 0.9341 - val_loss: 1.6622 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00013: saving model to models/rnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 47s 6ms/step - loss: 0.1528 - acc: 0.9378 - recall: 0.9378 - precision: 0.9378 - val_loss: 1.8829 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00014: saving model to models/rnn_model.hdf5\n",
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 46s 6ms/step - loss: 0.1450 - acc: 0.9441 - recall: 0.9441 - precision: 0.9441 - val_loss: 2.0429 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00015: saving model to models/rnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 46s 6ms/step - loss: 0.1434 - acc: 0.9430 - recall: 0.9430 - precision: 0.9430 - val_loss: 1.9417 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00016: saving model to models/rnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 46s 6ms/step - loss: 0.1341 - acc: 0.9451 - recall: 0.9451 - precision: 0.9451 - val_loss: 1.8813 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00017: saving model to models/rnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 46s 6ms/step - loss: 0.1178 - acc: 0.9534 - recall: 0.9534 - precision: 0.9534 - val_loss: 1.9623 - val_acc: 0.5219 - val_recall: 0.5219 - val_precision: 0.5219\n",
      "\n",
      "Epoch 00018: saving model to models/rnn_model.hdf5\n",
      "Epoch 19/50\n",
      "8200/8200 [==============================] - 46s 6ms/step - loss: 0.1139 - acc: 0.9527 - recall: 0.9527 - precision: 0.9527 - val_loss: 1.7776 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00019: saving model to models/rnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 46s 6ms/step - loss: 0.1042 - acc: 0.9566 - recall: 0.9566 - precision: 0.9566 - val_loss: 2.1767 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00020: saving model to models/rnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.1025 - acc: 0.9557 - recall: 0.9557 - precision: 0.9557 - val_loss: 2.3560 - val_acc: 0.5241 - val_recall: 0.5241 - val_precision: 0.5241\n",
      "\n",
      "Epoch 00021: saving model to models/rnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0907 - acc: 0.9607 - recall: 0.9607 - precision: 0.9607 - val_loss: 2.3202 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00022: saving model to models/rnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0883 - acc: 0.9616 - recall: 0.9616 - precision: 0.9616 - val_loss: 2.5134 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00023: saving model to models/rnn_model.hdf5\n",
      "Epoch 24/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0831 - acc: 0.9616 - recall: 0.9616 - precision: 0.9616 - val_loss: 2.9399 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00024: saving model to models/rnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 46s 6ms/step - loss: 0.0861 - acc: 0.9609 - recall: 0.9609 - precision: 0.9609 - val_loss: 2.8662 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00025: saving model to models/rnn_model.hdf5\n",
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 45s 6ms/step - loss: 0.0794 - acc: 0.9644 - recall: 0.9644 - precision: 0.9644 - val_loss: 2.3107 - val_acc: 0.5055 - val_recall: 0.5055 - val_precision: 0.5055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: saving model to models/rnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0802 - acc: 0.9632 - recall: 0.9632 - precision: 0.9632 - val_loss: 2.9917 - val_acc: 0.5241 - val_recall: 0.5241 - val_precision: 0.5241\n",
      "\n",
      "Epoch 00027: saving model to models/rnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 45s 6ms/step - loss: 0.0698 - acc: 0.9643 - recall: 0.9643 - precision: 0.9643 - val_loss: 2.9058 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00028: saving model to models/rnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0696 - acc: 0.9685 - recall: 0.9685 - precision: 0.9685 - val_loss: 3.1834 - val_acc: 0.5285 - val_recall: 0.5285 - val_precision: 0.5285\n",
      "\n",
      "Epoch 00029: saving model to models/rnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 46s 6ms/step - loss: 0.0622 - acc: 0.9667 - recall: 0.9667 - precision: 0.9667 - val_loss: 3.3889 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00030: saving model to models/rnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0701 - acc: 0.9641 - recall: 0.9641 - precision: 0.9641 - val_loss: 3.0448 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00031: saving model to models/rnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0655 - acc: 0.9674 - recall: 0.9674 - precision: 0.9674 - val_loss: 3.3138 - val_acc: 0.5219 - val_recall: 0.5219 - val_precision: 0.5219\n",
      "\n",
      "Epoch 00032: saving model to models/rnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0651 - acc: 0.9655 - recall: 0.9655 - precision: 0.9655 - val_loss: 3.6602 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00033: saving model to models/rnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 45s 6ms/step - loss: 0.0614 - acc: 0.9689 - recall: 0.9689 - precision: 0.9689 - val_loss: 3.4312 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00034: saving model to models/rnn_model.hdf5\n",
      "Epoch 35/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0661 - acc: 0.9652 - recall: 0.9652 - precision: 0.9652 - val_loss: 3.8439 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00035: saving model to models/rnn_model.hdf5\n",
      "Epoch 36/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0618 - acc: 0.9709 - recall: 0.9709 - precision: 0.9709 - val_loss: 3.1760 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00036: saving model to models/rnn_model.hdf5\n",
      "Epoch 37/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0625 - acc: 0.9698 - recall: 0.9698 - precision: 0.9698 - val_loss: 3.4646 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00037: saving model to models/rnn_model.hdf5\n",
      "Epoch 38/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0652 - acc: 0.9695 - recall: 0.9695 - precision: 0.9695 - val_loss: 3.5021 - val_acc: 0.5318 - val_recall: 0.5318 - val_precision: 0.5318\n",
      "\n",
      "Epoch 00038: saving model to models/rnn_model.hdf5\n",
      "Epoch 39/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0595 - acc: 0.9689 - recall: 0.9689 - precision: 0.9689 - val_loss: 3.6659 - val_acc: 0.5241 - val_recall: 0.5241 - val_precision: 0.5241\n",
      "\n",
      "Epoch 00039: saving model to models/rnn_model.hdf5\n",
      "Epoch 40/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0589 - acc: 0.9691 - recall: 0.9691 - precision: 0.9691 - val_loss: 3.5609 - val_acc: 0.5110 - val_recall: 0.5110 - val_precision: 0.5110\n",
      "\n",
      "Epoch 00040: saving model to models/rnn_model.hdf5\n",
      "Epoch 41/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0609 - acc: 0.9684 - recall: 0.9684 - precision: 0.9684 - val_loss: 3.7833 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00041: saving model to models/rnn_model.hdf5\n",
      "Epoch 42/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0588 - acc: 0.9707 - recall: 0.9707 - precision: 0.9707 - val_loss: 3.4263 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00042: saving model to models/rnn_model.hdf5\n",
      "Epoch 43/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0542 - acc: 0.9698 - recall: 0.9698 - precision: 0.9698 - val_loss: 3.8546 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00043: saving model to models/rnn_model.hdf5\n",
      "Epoch 44/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0527 - acc: 0.9696 - recall: 0.9696 - precision: 0.9696 - val_loss: 3.8997 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00044: saving model to models/rnn_model.hdf5\n",
      "Epoch 45/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0511 - acc: 0.9728 - recall: 0.9728 - precision: 0.9728 - val_loss: 3.9756 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00045: saving model to models/rnn_model.hdf5\n",
      "Epoch 46/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0480 - acc: 0.9727 - recall: 0.9727 - precision: 0.9727 - val_loss: 4.0904 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00046: saving model to models/rnn_model.hdf5\n",
      "Epoch 47/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0516 - acc: 0.9718 - recall: 0.9718 - precision: 0.9718 - val_loss: 3.4119 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00047: saving model to models/rnn_model.hdf5\n",
      "Epoch 48/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0488 - acc: 0.9733 - recall: 0.9733 - precision: 0.9733 - val_loss: 3.9653 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00048: saving model to models/rnn_model.hdf5\n",
      "Epoch 49/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0485 - acc: 0.9700 - recall: 0.9700 - precision: 0.9700 - val_loss: 3.9696 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00049: saving model to models/rnn_model.hdf5\n",
      "Epoch 50/50\n",
      "8200/8200 [==============================] - 45s 5ms/step - loss: 0.0467 - acc: 0.9734 - recall: 0.9734 - precision: 0.9734 - val_loss: 4.3104 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00050: saving model to models/rnn_model.hdf5\n",
      "2278/2278 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "rnn_res = train_and_test_model(rnn_model, train_padded_words, train_y, \n",
    "                               test_padded_words, test_y, 'rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.engine.training.Model at 0x7f2708444630>,\n",
       " 0.5263388947607447,\n",
       " 0.5263388947607447,\n",
       " 0.5263388947607447)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sentences(data, lexicon, maxlen=200):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in sentences]\n",
    "        x2 = np.eye(len(char_indices) + 1)[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "def create_cnn_model(char_maxlen, vocab_size,\n",
    "                     nb_filter=100, filter_kernels = [4] * 4,\n",
    "                     pool_size=3, n_dense_nodes=100,\n",
    "                     drop_out=.2, n_out=2):\n",
    "\n",
    "    inputs = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "\n",
    "    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                  padding='valid', activation='relu',\n",
    "                  input_shape=(char_maxlen, vocab_size))(inputs)\n",
    "    \n",
    "    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n",
    "\n",
    "    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                          padding='valid', activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n",
    "\n",
    "    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                          padding='valid', activation='relu')(maxpool2)\n",
    "\n",
    "    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                          padding='valid', activation='relu')(conv3)\n",
    "\n",
    "    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n",
    "    flatten = Flatten()(maxpool3)\n",
    "\n",
    "    dense_layer = Dense(n_dense_nodes, activation='relu')(flatten)\n",
    "    dropout = Dropout(drop_out)(dense_layer)\n",
    "\n",
    "    output_layer = Dense(n_out, activation='softmax', name='output')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_maxlen = 1024 \n",
    "nb_filter = 256\n",
    "dense_outputs = 1024\n",
    "filter_kernels = [7, 7, 3, 3]\n",
    "pool_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 86\n"
     ]
    }
   ],
   "source": [
    "# Turn all tokens into one string and then all obs \n",
    "# into one overall string\n",
    "trainTokensAsString = train.final_text.apply(lambda x: ' '.join(x))\n",
    "testTokensAsString = test.final_text.apply(lambda x: ' '.join(x))\n",
    "oneTxt = ' '.join(trainTokensAsString)\n",
    "\n",
    "# Get info about characters\n",
    "chars = set(oneTxt)\n",
    "vocab_size = len(chars) + 1\n",
    "print('total chars:', vocab_size)\n",
    "char_indices = dict((c, i + 2) for i, c in enumerate(chars))\n",
    "indices_char = dict((i + 2, c) for i, c in enumerate(chars))\n",
    "\n",
    "char_indices['<UNK>'] = 1\n",
    "indices_char[1] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCharData = vectorize_sentences(trainTokensAsString, char_indices, char_maxlen)\n",
    "testCharData = vectorize_sentences(testTokensAsString, char_indices, char_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_model(char_maxlen=char_maxlen, \n",
    "                             vocab_size=vocab_size,\n",
    "                             nb_filter=nb_filter, \n",
    "                             filter_kernels=filter_kernels,\n",
    "                             pool_size=pool_size, \n",
    "                             n_dense_nodes=dense_outputs,\n",
    "                             drop_out=.5, \n",
    "                             n_out=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_input_layer (InputLayer (None, 1024, 86)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1018, 256)         154368    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 339, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 333, 256)          459008    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 111, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 109, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 107, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 35, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8960)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              9176064   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 10,185,218\n",
      "Trainable params: 10,185,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 16s 2ms/step - loss: 0.6956 - acc: 0.4988 - recall: 0.4988 - precision: 0.4988 - val_loss: 0.6931 - val_acc: 0.5066 - val_recall: 0.5066 - val_precision: 0.5066\n",
      "\n",
      "Epoch 00001: saving model to models/cnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.6932 - acc: 0.4989 - recall: 0.4989 - precision: 0.4989 - val_loss: 0.6932 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00002: saving model to models/cnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.6933 - acc: 0.4970 - recall: 0.4970 - precision: 0.4970 - val_loss: 0.6931 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00003: saving model to models/cnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.6932 - acc: 0.5006 - recall: 0.5006 - precision: 0.5006 - val_loss: 0.6932 - val_acc: 0.4978 - val_recall: 0.4978 - val_precision: 0.4978\n",
      "\n",
      "Epoch 00004: saving model to models/cnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.6933 - acc: 0.5026 - recall: 0.5026 - precision: 0.5026 - val_loss: 0.6934 - val_acc: 0.4901 - val_recall: 0.4901 - val_precision: 0.4901\n",
      "\n",
      "Epoch 00005: saving model to models/cnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.6917 - acc: 0.5140 - recall: 0.5140 - precision: 0.5140 - val_loss: 0.6951 - val_acc: 0.4967 - val_recall: 0.4967 - val_precision: 0.4967\n",
      "\n",
      "Epoch 00006: saving model to models/cnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.6864 - acc: 0.5393 - recall: 0.5393 - precision: 0.5393 - val_loss: 0.7061 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00007: saving model to models/cnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.6700 - acc: 0.5678 - recall: 0.5678 - precision: 0.5678 - val_loss: 0.7181 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00008: saving model to models/cnn_model.hdf5\n",
      "Epoch 9/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.6268 - acc: 0.6273 - recall: 0.6273 - precision: 0.6273 - val_loss: 0.7585 - val_acc: 0.4759 - val_recall: 0.4759 - val_precision: 0.4759\n",
      "\n",
      "Epoch 00009: saving model to models/cnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.5537 - acc: 0.6922 - recall: 0.6922 - precision: 0.6922 - val_loss: 1.0019 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00010: saving model to models/cnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.4605 - acc: 0.7709 - recall: 0.7709 - precision: 0.7709 - val_loss: 0.9262 - val_acc: 0.4890 - val_recall: 0.4890 - val_precision: 0.4890\n",
      "\n",
      "Epoch 00011: saving model to models/cnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.3674 - acc: 0.8256 - recall: 0.8256 - precision: 0.8256 - val_loss: 1.1909 - val_acc: 0.4879 - val_recall: 0.4879 - val_precision: 0.4879\n",
      "\n",
      "Epoch 00012: saving model to models/cnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.3044 - acc: 0.8659 - recall: 0.8659 - precision: 0.8659 - val_loss: 1.2784 - val_acc: 0.4803 - val_recall: 0.4803 - val_precision: 0.4803\n",
      "\n",
      "Epoch 00013: saving model to models/cnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.2375 - acc: 0.9006 - recall: 0.9006 - precision: 0.9006 - val_loss: 1.9238 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00014: saving model to models/cnn_model.hdf5\n",
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.1822 - acc: 0.9305 - recall: 0.9305 - precision: 0.9305 - val_loss: 1.7838 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00015: saving model to models/cnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.1690 - acc: 0.9350 - recall: 0.9350 - precision: 0.9350 - val_loss: 2.3347 - val_acc: 0.4934 - val_recall: 0.4934 - val_precision: 0.4934\n",
      "\n",
      "Epoch 00016: saving model to models/cnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.1301 - acc: 0.9510 - recall: 0.9510 - precision: 0.9510 - val_loss: 2.4636 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00017: saving model to models/cnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.1253 - acc: 0.9566 - recall: 0.9566 - precision: 0.9566 - val_loss: 2.3583 - val_acc: 0.5022 - val_recall: 0.5022 - val_precision: 0.5022\n",
      "\n",
      "Epoch 00018: saving model to models/cnn_model.hdf5\n",
      "Epoch 19/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.1127 - acc: 0.9605 - recall: 0.9605 - precision: 0.9605 - val_loss: 2.3641 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00019: saving model to models/cnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0958 - acc: 0.9640 - recall: 0.9640 - precision: 0.9640 - val_loss: 2.4799 - val_acc: 0.4836 - val_recall: 0.4836 - val_precision: 0.4836\n",
      "\n",
      "Epoch 00020: saving model to models/cnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0733 - acc: 0.9690 - recall: 0.9690 - precision: 0.9690 - val_loss: 2.6473 - val_acc: 0.4879 - val_recall: 0.4879 - val_precision: 0.4879\n",
      "\n",
      "Epoch 00021: saving model to models/cnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0685 - acc: 0.9713 - recall: 0.9713 - precision: 0.9713 - val_loss: 2.8013 - val_acc: 0.4912 - val_recall: 0.4912 - val_precision: 0.4912\n",
      "\n",
      "Epoch 00022: saving model to models/cnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0655 - acc: 0.9727 - recall: 0.9727 - precision: 0.9727 - val_loss: 2.5775 - val_acc: 0.4868 - val_recall: 0.4868 - val_precision: 0.4868\n",
      "\n",
      "Epoch 00023: saving model to models/cnn_model.hdf5\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0628 - acc: 0.9718 - recall: 0.9718 - precision: 0.9718 - val_loss: 2.6503 - val_acc: 0.4901 - val_recall: 0.4901 - val_precision: 0.4901\n",
      "\n",
      "Epoch 00024: saving model to models/cnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0589 - acc: 0.9726 - recall: 0.9726 - precision: 0.9726 - val_loss: 2.8433 - val_acc: 0.4901 - val_recall: 0.4901 - val_precision: 0.4901\n",
      "\n",
      "Epoch 00025: saving model to models/cnn_model.hdf5\n",
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0598 - acc: 0.9735 - recall: 0.9735 - precision: 0.9735 - val_loss: 2.7155 - val_acc: 0.4748 - val_recall: 0.4748 - val_precision: 0.4748\n",
      "\n",
      "Epoch 00026: saving model to models/cnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0549 - acc: 0.9721 - recall: 0.9721 - precision: 0.9721 - val_loss: 2.8486 - val_acc: 0.4890 - val_recall: 0.4890 - val_precision: 0.4890\n",
      "\n",
      "Epoch 00027: saving model to models/cnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0546 - acc: 0.9735 - recall: 0.9735 - precision: 0.9735 - val_loss: 2.9648 - val_acc: 0.4792 - val_recall: 0.4792 - val_precision: 0.4792\n",
      "\n",
      "Epoch 00028: saving model to models/cnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0552 - acc: 0.9733 - recall: 0.9733 - precision: 0.9733 - val_loss: 2.9629 - val_acc: 0.4846 - val_recall: 0.4846 - val_precision: 0.4846\n",
      "\n",
      "Epoch 00029: saving model to models/cnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0563 - acc: 0.9720 - recall: 0.9720 - precision: 0.9720 - val_loss: 2.8291 - val_acc: 0.5044 - val_recall: 0.5044 - val_precision: 0.5044\n",
      "\n",
      "Epoch 00030: saving model to models/cnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0509 - acc: 0.9752 - recall: 0.9752 - precision: 0.9752 - val_loss: 3.0360 - val_acc: 0.4868 - val_recall: 0.4868 - val_precision: 0.4868\n",
      "\n",
      "Epoch 00031: saving model to models/cnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0552 - acc: 0.9734 - recall: 0.9734 - precision: 0.9734 - val_loss: 3.3967 - val_acc: 0.5022 - val_recall: 0.5022 - val_precision: 0.5022\n",
      "\n",
      "Epoch 00032: saving model to models/cnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0838 - acc: 0.9641 - recall: 0.9641 - precision: 0.9641 - val_loss: 2.4981 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00033: saving model to models/cnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.1641 - acc: 0.9327 - recall: 0.9327 - precision: 0.9327 - val_loss: 2.2873 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00034: saving model to models/cnn_model.hdf5\n",
      "Epoch 35/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.1220 - acc: 0.9467 - recall: 0.9467 - precision: 0.9467 - val_loss: 1.9538 - val_acc: 0.4978 - val_recall: 0.4978 - val_precision: 0.4978\n",
      "\n",
      "Epoch 00035: saving model to models/cnn_model.hdf5\n",
      "Epoch 36/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.1062 - acc: 0.9578 - recall: 0.9578 - precision: 0.9578 - val_loss: 2.7401 - val_acc: 0.4857 - val_recall: 0.4857 - val_precision: 0.4857\n",
      "\n",
      "Epoch 00036: saving model to models/cnn_model.hdf5\n",
      "Epoch 37/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0689 - acc: 0.9707 - recall: 0.9707 - precision: 0.9707 - val_loss: 2.9303 - val_acc: 0.4868 - val_recall: 0.4868 - val_precision: 0.4868\n",
      "\n",
      "Epoch 00037: saving model to models/cnn_model.hdf5\n",
      "Epoch 38/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0563 - acc: 0.9724 - recall: 0.9724 - precision: 0.9724 - val_loss: 2.6988 - val_acc: 0.4989 - val_recall: 0.4989 - val_precision: 0.4989\n",
      "\n",
      "Epoch 00038: saving model to models/cnn_model.hdf5\n",
      "Epoch 39/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0481 - acc: 0.9751 - recall: 0.9751 - precision: 0.9751 - val_loss: 3.4617 - val_acc: 0.4868 - val_recall: 0.4868 - val_precision: 0.4868\n",
      "\n",
      "Epoch 00039: saving model to models/cnn_model.hdf5\n",
      "Epoch 40/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0432 - acc: 0.9766 - recall: 0.9766 - precision: 0.9766 - val_loss: 3.0962 - val_acc: 0.4945 - val_recall: 0.4945 - val_precision: 0.4945\n",
      "\n",
      "Epoch 00040: saving model to models/cnn_model.hdf5\n",
      "Epoch 41/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0461 - acc: 0.9755 - recall: 0.9755 - precision: 0.9755 - val_loss: 3.4609 - val_acc: 0.4814 - val_recall: 0.4814 - val_precision: 0.4814\n",
      "\n",
      "Epoch 00041: saving model to models/cnn_model.hdf5\n",
      "Epoch 42/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0433 - acc: 0.9745 - recall: 0.9745 - precision: 0.9745 - val_loss: 3.4519 - val_acc: 0.4901 - val_recall: 0.4901 - val_precision: 0.4901\n",
      "\n",
      "Epoch 00042: saving model to models/cnn_model.hdf5\n",
      "Epoch 43/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0425 - acc: 0.9745 - recall: 0.9745 - precision: 0.9745 - val_loss: 3.9327 - val_acc: 0.4803 - val_recall: 0.4803 - val_precision: 0.4803\n",
      "\n",
      "Epoch 00043: saving model to models/cnn_model.hdf5\n",
      "Epoch 44/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0459 - acc: 0.9759 - recall: 0.9759 - precision: 0.9759 - val_loss: 3.8201 - val_acc: 0.4846 - val_recall: 0.4846 - val_precision: 0.4846\n",
      "\n",
      "Epoch 00044: saving model to models/cnn_model.hdf5\n",
      "Epoch 45/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0428 - acc: 0.9740 - recall: 0.9740 - precision: 0.9740 - val_loss: 3.4419 - val_acc: 0.4901 - val_recall: 0.4901 - val_precision: 0.4901\n",
      "\n",
      "Epoch 00045: saving model to models/cnn_model.hdf5\n",
      "Epoch 46/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0421 - acc: 0.9754 - recall: 0.9754 - precision: 0.9754 - val_loss: 4.1158 - val_acc: 0.4803 - val_recall: 0.4803 - val_precision: 0.4803\n",
      "\n",
      "Epoch 00046: saving model to models/cnn_model.hdf5\n",
      "Epoch 47/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0416 - acc: 0.9761 - recall: 0.9761 - precision: 0.9761 - val_loss: 3.4704 - val_acc: 0.5044 - val_recall: 0.5044 - val_precision: 0.5044\n",
      "\n",
      "Epoch 00047: saving model to models/cnn_model.hdf5\n",
      "Epoch 48/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0406 - acc: 0.9767 - recall: 0.9767 - precision: 0.9767 - val_loss: 3.6750 - val_acc: 0.4956 - val_recall: 0.4956 - val_precision: 0.4956\n",
      "\n",
      "Epoch 00048: saving model to models/cnn_model.hdf5\n",
      "Epoch 49/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0422 - acc: 0.9763 - recall: 0.9763 - precision: 0.9763 - val_loss: 3.1785 - val_acc: 0.5000 - val_recall: 0.5000 - val_precision: 0.5000\n",
      "\n",
      "Epoch 00049: saving model to models/cnn_model.hdf5\n",
      "Epoch 50/50\n",
      "8200/8200 [==============================] - 13s 2ms/step - loss: 0.0415 - acc: 0.9788 - recall: 0.9788 - precision: 0.9788 - val_loss: 3.7329 - val_acc: 0.5055 - val_recall: 0.5055 - val_precision: 0.5055\n",
      "\n",
      "Epoch 00050: saving model to models/cnn_model.hdf5\n",
      "2278/2278 [==============================] - 2s 813us/step\n"
     ]
    }
   ],
   "source": [
    "cnn_res = train_and_test_model(cnn_model, trainCharData[:, :, 1:],\n",
    "                               train_y, \n",
    "                               testCharData[:, :, 1:], \n",
    "                               test_y, \n",
    "                               'cnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_rnn_model(rnn_input_len, char_maxlen, vocab_size,\n",
    "                         embed_matrix, n_RNN_nodes, \n",
    "                         nb_filter=100, filter_kernels = [4] * 4,\n",
    "                         pool_size=3, n_dense_nodes=100,\n",
    "                         recurrent_dropout=0.2, \n",
    "                         drop_out=.2, n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(rnn_input_len,), name='word_input_layer')\n",
    "    char_input = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "    \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    rnn_output1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n",
    "                                      recurrent_dropout=recurrent_dropout, \n",
    "                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n",
    "    \n",
    "    rnn_output2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      dropout=drop_out, name='hidden_layer2'))(rnn_output1)\n",
    "            \n",
    "    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                  padding='valid', activation='relu',\n",
    "                  input_shape=(char_maxlen, vocab_size))(char_input)\n",
    "\n",
    "    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n",
    "\n",
    "    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                          padding='valid', activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n",
    "\n",
    "    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                          padding='valid', activation='relu')(maxpool2)\n",
    "\n",
    "    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                          padding='valid', activation='relu')(conv3)\n",
    "\n",
    "    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n",
    "    cnn_output = Flatten()(maxpool3)\n",
    "\n",
    "    merged_layer = concatenate([cnn_output, rnn_output2])\n",
    "    \n",
    "    dense_layer1 = Dense(n_dense_nodes, activation='relu', name='dense_layer')(merged_layer)\n",
    "    drop_out1 = Dropout(drop_out)(dense_layer1)\n",
    "    dense_layer2 = Dense(n_dense_nodes, activation='relu')(drop_out1)\n",
    "    drop_out2 = Dropout(drop_out)(dense_layer2)\n",
    "    \n",
    "    main_output = Dense(n_out, activation='softmax', name='output_layer')(drop_out2)\n",
    "\n",
    "    model = Model(inputs=[word_input, char_input], outputs=[main_output])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_model = create_cnn_rnn_model(rnn_input_len=train_padded_words.shape[-1], \n",
    "                                     char_maxlen=char_maxlen, \n",
    "                                     vocab_size=vocab_size,\n",
    "                                     embed_matrix=word_embed_matrix, \n",
    "                                     n_RNN_nodes=200,\n",
    "                                     nb_filter=nb_filter, \n",
    "                                     filter_kernels=filter_kernels,\n",
    "                                     pool_size=pool_size, \n",
    "                                     n_dense_nodes=200,\n",
    "                                     recurrent_dropout=0.2, \n",
    "                                     drop_out=.5, \n",
    "                                     n_out=n_out)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input_layer (InputLayer)   (None, 1024, 86)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1018, 256)    154368      char_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 339, 256)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 333, 256)     459008      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 111, 256)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 109, 256)     196864      max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "word_input_layer (InputLayer)   (None, 102)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 107, 256)     196864      conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 102, 200)     2356000     word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 35, 256)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 102, 400)     641600      word_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8960)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 400)          961600      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9360)         0           flatten_2[0][0]                  \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (Dense)             (None, 200)          1872200     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200)          0           dense_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          40200       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            402         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,879,106\n",
      "Trainable params: 6,879,106\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 8200 samples, validate on 912 samples\n",
      "Epoch 1/50\n",
      "8200/8200 [==============================] - 60s 7ms/step - loss: 0.6953 - acc: 0.5007 - recall: 0.5007 - precision: 0.5007 - val_loss: 0.6927 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00001: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 2/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.6949 - acc: 0.4952 - recall: 0.4952 - precision: 0.4952 - val_loss: 0.6927 - val_acc: 0.5406 - val_recall: 0.5406 - val_precision: 0.5406\n",
      "\n",
      "Epoch 00002: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 3/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.6936 - acc: 0.5109 - recall: 0.5109 - precision: 0.5109 - val_loss: 0.6930 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00003: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 4/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.6863 - acc: 0.5434 - recall: 0.5434 - precision: 0.5434 - val_loss: 0.6938 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00004: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 5/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.6476 - acc: 0.6302 - recall: 0.6302 - precision: 0.6302 - val_loss: 0.7146 - val_acc: 0.5077 - val_recall: 0.5077 - val_precision: 0.5077\n",
      "\n",
      "Epoch 00005: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 6/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.5650 - acc: 0.7150 - recall: 0.7150 - precision: 0.7150 - val_loss: 0.8175 - val_acc: 0.5362 - val_recall: 0.5362 - val_precision: 0.5362\n",
      "\n",
      "Epoch 00006: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 7/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.4656 - acc: 0.7799 - recall: 0.7799 - precision: 0.7799 - val_loss: 0.9487 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00007: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 8/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.4901 - acc: 0.7739 - recall: 0.7739 - precision: 0.7739 - val_loss: 0.9436 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00008: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 9/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.3929 - acc: 0.8245 - recall: 0.8245 - precision: 0.8245 - val_loss: 1.0564 - val_acc: 0.5296 - val_recall: 0.5296 - val_precision: 0.5296\n",
      "\n",
      "Epoch 00009: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 10/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.3144 - acc: 0.8685 - recall: 0.8685 - precision: 0.8685 - val_loss: 1.1398 - val_acc: 0.5099 - val_recall: 0.5099 - val_precision: 0.5099\n",
      "\n",
      "Epoch 00010: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 11/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.2798 - acc: 0.8862 - recall: 0.8862 - precision: 0.8862 - val_loss: 1.1462 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00011: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 12/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.2423 - acc: 0.8993 - recall: 0.8993 - precision: 0.8993 - val_loss: 1.5703 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00012: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 13/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.2201 - acc: 0.9132 - recall: 0.9132 - precision: 0.9132 - val_loss: 1.4256 - val_acc: 0.5329 - val_recall: 0.5329 - val_precision: 0.5329\n",
      "\n",
      "Epoch 00013: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 14/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1949 - acc: 0.9250 - recall: 0.9250 - precision: 0.9250 - val_loss: 1.5676 - val_acc: 0.5296 - val_recall: 0.5296 - val_precision: 0.5296\n",
      "\n",
      "Epoch 00014: saving model to models/cnn_rnn_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1678 - acc: 0.9334 - recall: 0.9334 - precision: 0.9334 - val_loss: 1.8392 - val_acc: 0.5285 - val_recall: 0.5285 - val_precision: 0.5285\n",
      "\n",
      "Epoch 00015: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 16/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1580 - acc: 0.9351 - recall: 0.9351 - precision: 0.9351 - val_loss: 1.7370 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00016: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 17/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1477 - acc: 0.9382 - recall: 0.9382 - precision: 0.9382 - val_loss: 2.1973 - val_acc: 0.5329 - val_recall: 0.5329 - val_precision: 0.5329\n",
      "\n",
      "Epoch 00017: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 18/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1367 - acc: 0.9460 - recall: 0.9460 - precision: 0.9460 - val_loss: 2.1753 - val_acc: 0.5340 - val_recall: 0.5340 - val_precision: 0.5340\n",
      "\n",
      "Epoch 00018: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 19/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.1394 - acc: 0.9445 - recall: 0.9445 - precision: 0.9445 - val_loss: 2.1471 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00019: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 20/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.1318 - acc: 0.9461 - recall: 0.9461 - precision: 0.9461 - val_loss: 1.9847 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00020: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 21/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1205 - acc: 0.9523 - recall: 0.9523 - precision: 0.9523 - val_loss: 2.0860 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00021: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 22/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.1053 - acc: 0.9576 - recall: 0.9576 - precision: 0.9576 - val_loss: 2.2518 - val_acc: 0.5340 - val_recall: 0.5340 - val_precision: 0.5340\n",
      "\n",
      "Epoch 00022: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 23/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1111 - acc: 0.9544 - recall: 0.9544 - precision: 0.9544 - val_loss: 1.9101 - val_acc: 0.5362 - val_recall: 0.5362 - val_precision: 0.5362\n",
      "\n",
      "Epoch 00023: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 24/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.1061 - acc: 0.9584 - recall: 0.9584 - precision: 0.9584 - val_loss: 2.2109 - val_acc: 0.5274 - val_recall: 0.5274 - val_precision: 0.5274\n",
      "\n",
      "Epoch 00024: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 25/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1033 - acc: 0.9565 - recall: 0.9565 - precision: 0.9565 - val_loss: 2.5654 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00025: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 26/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0997 - acc: 0.9607 - recall: 0.9607 - precision: 0.9607 - val_loss: 2.2398 - val_acc: 0.5384 - val_recall: 0.5384 - val_precision: 0.5384\n",
      "\n",
      "Epoch 00026: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 27/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.0913 - acc: 0.9601 - recall: 0.9601 - precision: 0.9601 - val_loss: 2.6049 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00027: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 28/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.1012 - acc: 0.9571 - recall: 0.9571 - precision: 0.9571 - val_loss: 2.4745 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00028: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 29/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0938 - acc: 0.9609 - recall: 0.9609 - precision: 0.9609 - val_loss: 2.4479 - val_acc: 0.5154 - val_recall: 0.5154 - val_precision: 0.5154\n",
      "\n",
      "Epoch 00029: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 30/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0939 - acc: 0.9602 - recall: 0.9602 - precision: 0.9602 - val_loss: 2.5283 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00030: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 31/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0841 - acc: 0.9632 - recall: 0.9632 - precision: 0.9632 - val_loss: 2.7030 - val_acc: 0.5274 - val_recall: 0.5274 - val_precision: 0.5274\n",
      "\n",
      "Epoch 00031: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 32/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0778 - acc: 0.9654 - recall: 0.9654 - precision: 0.9654 - val_loss: 3.1628 - val_acc: 0.5121 - val_recall: 0.5121 - val_precision: 0.5121\n",
      "\n",
      "Epoch 00032: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 33/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0736 - acc: 0.9670 - recall: 0.9670 - precision: 0.9670 - val_loss: 2.9636 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00033: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 34/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0798 - acc: 0.9638 - recall: 0.9638 - precision: 0.9638 - val_loss: 2.8821 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00034: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 35/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.0757 - acc: 0.9668 - recall: 0.9668 - precision: 0.9668 - val_loss: 3.0101 - val_acc: 0.5274 - val_recall: 0.5274 - val_precision: 0.5274\n",
      "\n",
      "Epoch 00035: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 36/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.0754 - acc: 0.9628 - recall: 0.9628 - precision: 0.9628 - val_loss: 2.7008 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00036: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 37/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0867 - acc: 0.9630 - recall: 0.9630 - precision: 0.9630 - val_loss: 2.8839 - val_acc: 0.5186 - val_recall: 0.5186 - val_precision: 0.5186\n",
      "\n",
      "Epoch 00037: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 38/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.0733 - acc: 0.9667 - recall: 0.9667 - precision: 0.9667 - val_loss: 3.1698 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00038: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 39/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.0641 - acc: 0.9687 - recall: 0.9687 - precision: 0.9687 - val_loss: 3.6164 - val_acc: 0.5263 - val_recall: 0.5263 - val_precision: 0.5263\n",
      "\n",
      "Epoch 00039: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 40/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0706 - acc: 0.9668 - recall: 0.9668 - precision: 0.9668 - val_loss: 3.1363 - val_acc: 0.5132 - val_recall: 0.5132 - val_precision: 0.5132\n",
      "\n",
      "Epoch 00040: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 41/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.0633 - acc: 0.9709 - recall: 0.9709 - precision: 0.9709 - val_loss: 3.5186 - val_acc: 0.5175 - val_recall: 0.5175 - val_precision: 0.5175\n",
      "\n",
      "Epoch 00041: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 42/50\n",
      "8200/8200 [==============================] - 57s 7ms/step - loss: 0.0596 - acc: 0.9696 - recall: 0.9696 - precision: 0.9696 - val_loss: 3.6054 - val_acc: 0.5252 - val_recall: 0.5252 - val_precision: 0.5252\n",
      "\n",
      "Epoch 00042: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 43/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0665 - acc: 0.9657 - recall: 0.9657 - precision: 0.9657 - val_loss: 3.6940 - val_acc: 0.5296 - val_recall: 0.5296 - val_precision: 0.5296\n",
      "\n",
      "Epoch 00043: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 44/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0607 - acc: 0.9671 - recall: 0.9671 - precision: 0.9671 - val_loss: 3.5984 - val_acc: 0.5230 - val_recall: 0.5230 - val_precision: 0.5230\n",
      "\n",
      "Epoch 00044: saving model to models/cnn_rnn_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0657 - acc: 0.9687 - recall: 0.9687 - precision: 0.9687 - val_loss: 3.1525 - val_acc: 0.5143 - val_recall: 0.5143 - val_precision: 0.5143\n",
      "\n",
      "Epoch 00045: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 46/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0572 - acc: 0.9699 - recall: 0.9699 - precision: 0.9699 - val_loss: 4.0747 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00046: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 47/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0605 - acc: 0.9691 - recall: 0.9691 - precision: 0.9691 - val_loss: 3.5645 - val_acc: 0.5088 - val_recall: 0.5088 - val_precision: 0.5088\n",
      "\n",
      "Epoch 00047: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 48/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0590 - acc: 0.9704 - recall: 0.9704 - precision: 0.9704 - val_loss: 3.6550 - val_acc: 0.5208 - val_recall: 0.5208 - val_precision: 0.5208\n",
      "\n",
      "Epoch 00048: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 49/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0545 - acc: 0.9715 - recall: 0.9715 - precision: 0.9715 - val_loss: 3.5545 - val_acc: 0.5164 - val_recall: 0.5164 - val_precision: 0.5164\n",
      "\n",
      "Epoch 00049: saving model to models/cnn_rnn_model.hdf5\n",
      "Epoch 50/50\n",
      "8200/8200 [==============================] - 56s 7ms/step - loss: 0.0538 - acc: 0.9695 - recall: 0.9695 - precision: 0.9695 - val_loss: 3.9270 - val_acc: 0.5197 - val_recall: 0.5197 - val_precision: 0.5197\n",
      "\n",
      "Epoch 00050: saving model to models/cnn_rnn_model.hdf5\n",
      "2278/2278 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_res = train_and_test_model(cnn_rnn_model, \n",
    "                               [train_padded_words, trainCharData[:, :, 1:]],\n",
    "                               train_y, \n",
    "                               [test_padded_words, testCharData[:, :, 1:]],\n",
    "                               test_y, \n",
    "                               'cnn_rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.engine.training.Model at 0x7f2687e3fac8>,\n",
       " 0.5162423190785878,\n",
       " 0.5162423190785878,\n",
       " 0.5162423190785878)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_rnn_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put results in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cnn_mod</th>\n",
       "      <td>0.507902</td>\n",
       "      <td>0.507902</td>\n",
       "      <td>0.507902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_mod</th>\n",
       "      <td>0.526339</td>\n",
       "      <td>0.526339</td>\n",
       "      <td>0.526339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_rnn_mod</th>\n",
       "      <td>0.516242</td>\n",
       "      <td>0.516242</td>\n",
       "      <td>0.516242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy    recall  precision\n",
       "cnn_mod      0.507902  0.507902   0.507902\n",
       "rnn_mod      0.526339  0.526339   0.526339\n",
       "cnn_rnn_mod  0.516242  0.516242   0.516242"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records([cnn_res[1:4], rnn_res[1:4], cnn_rnn_res[1:4]], \n",
    "                          columns=['accuracy', 'recall', 'precision'], \n",
    "                         index=['cnn_mod', 'rnn_mod', 'cnn_rnn_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classifications(classifications, classType, test_y, test_text):\n",
    "    texts = [' '.join(sent) for sent in test_text[classifications.index]]\n",
    "    stock_movements = np.where(test_y[classifications.index], 'positive', 'negative')\n",
    "    \n",
    "    print('Examples of {} predictions:\\n'.format(classType))\n",
    "    for i in range(len(texts)):\n",
    "        print('Stock movement was {}'.format(stock_movements[i]))\n",
    "        print('News info:\\n{}'.format(texts[i]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print_samples(model, modelName, test_x, test_y=test['y'], test_text = test['final_text']):\n",
    "    \"\"\"\"Print out predictions of the model\"\"\"\n",
    "    print('Stats for {} model'.format(modelName))\n",
    "    \n",
    "    res = model.predict(test_x)\n",
    "    class_res = np.apply_along_axis(np.argmax, axis=1, arr=res)\n",
    "\n",
    "    comparisons = class_res == test_y\n",
    "    good_class = comparisons.loc[comparisons == True].sample(n=3)\n",
    "    bad_class = comparisons.loc[comparisons == False].sample(n=3)\n",
    "\n",
    "    print_classifications(good_class, 'correct', test_y, test_text)\n",
    "    print_classifications(bad_class, 'INcorrect', test_y, test_text)\n",
    "\n",
    "    \n",
    "    top3MostProbPosArg = np.argsort(res[:, 1])[-3:]\n",
    "    top3Y = test_y.iloc[top3MostProbPosArg]\n",
    "    top3Probs = pd.Series(res[top3MostProbPosArg, 1], index=top3Y.index)\n",
    "    top3Data = pd.concat([top3Y, top3Probs], axis=1)\n",
    "    top3Data.columns = ['Actual', 'PositiveProb']\n",
    "    print('')\n",
    "    print('Top 3 Most Positive Probability:')\n",
    "    print(top3Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Apple files for potential notes offering Files for potential notes offering size not disclosed SEC filing Source text http 1 usa gov 1WCYXoN Further company coverage Bengaluru Newsroom 1 646 223 8780\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Prices slip as bond markets sell off 30 year yield tops 3 percent Thirty year yield over 3 percent for first time since December\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Fox loses United States copyright claims over Dish ad skipper United States judge has rejected portions of Twenty First Century Fox Inc's lawsuit seeking to stop Dish Network Corp from selling devices that let viewers skip over commercials when playing back shows\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Software AG sticks to outlook despite weak quarter Thomson Reuters I B E S sales estimate 280 9 mln euros\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "RBS branch bidders left hanging as bank mulls options Bidders had expected shortlist or preferred bidder in Jan\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Gulf of Mexico output to rise 43 pct by 2019 WoodMac An exploration and production resurgence in the Gulf of Mexico two years after a six month drilling shutdown because of the BP Plc oil spill is expected to increase output nearly 43 percent by 2019 energy consultancy Wood Mackenzie said on Wednesday\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "BT     20120613 16270     0.0           1.0\n",
      "P      20131114 31503     1.0           1.0\n",
      "FOX    20161013 20812     1.0           1.0\n",
      "Stats for CNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Caterpillar's global woes ripple out through supplier base Caterpillar Inc's suppliers were feeling the pain of slumping sales at the world's largest mining and construction equipment maker long before it announced an extensive cost cutting program\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "T trade groups mount court challenge to FCC Internet rules T Inc and three cable and wireless trade groups filed separate lawsuits on Tuesday challenging the United States Federal Communications Commission over its new web traffic regulations\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Barclays plans to raise capital WSJ Barclays Plc is in the final stages of executing a plan to boost its capital levels that will likely involve the bank issuing billions of pounds worth of new securities the Wall Street Journal reported citing people familiar with the matter\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Blue Buffalo to participate in Barclays 2016 Global Consumer Staples Conference Says to participate in Barclays 2016 Global Consumer Staples Conference and reaffirms full year fiscal 2016 outlook\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Taiwan's TSMC guides Q3 revenue up but tips Q4 to decline sequentially Sept 23 Taiwan Semiconductor Manufacturing Co Ltd\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "J says halting development of Botox rival Johnson Johnson said on Friday it was ending its efforts to bring to market a rival drug to Allergan Inc's popular Botox anti wrinkle treatment\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "SAP    20111115 34914     1.0           1.0\n",
      "FOX    20150319 20650     0.0           1.0\n",
      "MDT    20140604 30274     1.0           1.0\n",
      "Stats for CNN_RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "S P500 index buoyed by commodity sectors Apple drags futures lower Dow up 0 07 pct S P up 0 19 pct Nasdaq down 0 15 pct Updates volume adds Apple Twitter earnings\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "IndusInd Bank to buy RBS's India jewellery financing business IndusInd Bank Ltd said it would buy Royal Bank of Scotland's diamond and jewellery financing business in India and the related deposit portfolio\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Lloyds Banking Group redeems 1 bln preference shares Redemption of preference shares issued by Lloyds Banking Group plc\n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Verizon asks court to halt FCC net neutrality rule Verizon Communications Inc on Friday asked a federal appeals court to block the Federal Communications Commission from imposing new rules on how Internet service providers manage their networks\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Big Caterpillar dealer Finning to cut more jobs shut outlets Canada's Finning International Inc the world's biggest dealer of Caterpillar Inc equipment said on Thursday it would cut another 1 100 jobs in Canada and South America and close 11 Canadian locations due to weak sales\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Apple's devoted shareholders get rich and hang on When Anton Marinovich turned 18 his grandmother gave him 1 000 with strict instructions to invest in the stock market He chose Apple Inc\n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "                       Actual  PositiveProb\n",
      "ticker pub_date                            \n",
      "UMC    20160511 41165     1.0           1.0\n",
      "BP     20140122 15224     1.0           1.0\n",
      "AMGN   20170106 8050      1.0           1.0\n"
     ]
    }
   ],
   "source": [
    "predict_and_print_samples(rnn_res[0], 'RNN', test_padded_words)\n",
    "\n",
    "predict_and_print_samples(cnn_res[0], 'CNN', testCharData[:, :, 1:])\n",
    "\n",
    "predict_and_print_samples(cnn_rnn_res[0], 'CNN_RNN', [test_padded_words, testCharData[:, :, 1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
