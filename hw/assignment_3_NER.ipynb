{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Named Entity Recognition\n",
    "\n",
    "In this assignment, we are going to build a Named Entity Recognition model. With this model, we will also tag new data.\n",
    "\n",
    "More on Named Entity Recognition:\n",
    "\n",
    "https://blog.paralleldots.com/data-science/named-entity-recognition-milestone-models-papers-and-technologies/\n",
    "\n",
    "https://blog.paralleldots.com/product/applications-named-entity-recognition-api/\n",
    "\n",
    "### Steps:\n",
    "\n",
    "**1. Import the data**\n",
    "\n",
    "**2. Build the model**\n",
    "\n",
    "**3. Pick a dataset to run the model on**\n",
    "\n",
    "**4. Build a function to load new data and print the tags**\n",
    "\n",
    "Your web application will load small sections of text (such as tweets or headlines) and from that, you will tag the text based on the presence of named entities.\n",
    "\n",
    "*What you will be graded on:*\n",
    "\n",
    "1. Ability to build a model on word and tag data\n",
    "\n",
    "2. Ability to use the model to predict on new data and display that prediction\n",
    "\n",
    "*The model will be based on:*\n",
    "1. Embeddings from words\n",
    "2. Embeddings from tag inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the data\n",
    "\n",
    "Below is some code to get you started. As in the part of speech tagging example, you will have to write code to:\n",
    "\n",
    "0. Split your data into a train/test set (Do a 80/20 or 90/10 split since we'll be later applying this model to an entirely separate set of data)\n",
    "1. Find the set of all words\n",
    "2. Find the set of all tags\n",
    "3. **Create a function called ent_tagger** that will turn a sentence into this output for model building :\n",
    "``` [('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have',  'O'), ('marched',  'O'), ('through',  'O'), ('London', 'B-geo'), ('to',  'O'), ('protest',  'O'), ('the',  'O'), ('war',  'O'), ('in',  'O'), ('Iraq',  'B-geo'), ('and', 'O'), ('demand',  'O'), ('the',  'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops',  'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n",
    "```\n",
    "4. Make a dictionary of words to index and entity tag to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565  Sentence: 47958     impact   NN      O\n",
       "1048566  Sentence: 47958          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568  Sentence: 47959     forces  NNS      O\n",
       "1048569  Sentence: 47959       said  VBD      O\n",
       "1048570  Sentence: 47959       they  PRP      O\n",
       "1048571  Sentence: 47959  responded  VBD      O\n",
       "1048572  Sentence: 47959         to   TO      O\n",
       "1048573  Sentence: 47959        the   DT      O\n",
       "1048574  Sentence: 47959     attack   NN      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"../data/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data so that each sentence is put into a vector per row of a pandas dataframe\n",
    "cleanDat = data.groupby('Sentence #', sort=False).apply(lambda x: pd.DataFrame(data = {'token_sents': [x.Word.tolist()], 'token_tags': [x.Tag.tolist()]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random State\n",
    "seed = np.random.seed(10)\n",
    "\n",
    "# Split data based on sentence number\n",
    "train_sents, test_sents = train_test_split(cleanDat, test_size = .15, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return (lexicon, list(token_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:\n",
      "LEXICON SAMPLE (18835 total items):\n",
      "{'President': 2, 'Bush': 3, 'has': 4, 'outlined': 5, 'the': 6, 'agenda': 7, 'for': 8, 'his': 9, 'second': 10, 'term': 11, 'in': 12, 'office': 13, 'and': 14, 'asked': 15, 'support': 16, 'of': 17, 'all': 18, 'Americans': 19, 'weekly': 20, 'Saturday': 21}\n",
      "\n",
      "TAGS:\n",
      "LEXICON SAMPLE (18 total items):\n",
      "{'B-per': 2, 'I-per': 3, 'O': 4, 'B-gpe': 5, 'B-tim': 6, 'I-tim': 7, 'B-org': 8, 'B-geo': 9, 'I-org': 10, 'B-art': 11, 'I-geo': 12, 'B-eve': 13, 'I-eve': 14, 'I-gpe': 15, 'I-art': 16, 'B-nat': 17, 'I-nat': 18, '<UNK>': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Words:\")\n",
    "words_lexicon, all_words = make_lexicon(train_sents.token_sents, min_freq=2)\n",
    "with open('models/words_lexicon.pkl', 'wb') as f: #save the tags lexicon by pickling it\n",
    "    pickle.dump(words_lexicon, f)\n",
    "\n",
    "print('')\n",
    "print(\"TAGS:\")\n",
    "tags_lexicon, all_tags = make_lexicon(train_sents.token_tags, min_freq=2)\n",
    "with open('models/tags_lexicon.pkl', 'wb') as f: #save the words lexicon by pickling it\n",
    "    pickle.dump(tags_lexicon, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_tagger(sentence):\n",
    "    return [(word, tag) for word, tag in zip(sentence.token_sent, sentence.token_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Formatting the data\n",
    "Data will need to be\n",
    "\n",
    "1. Indexed\n",
    "2. Limited by vocabulary (ie replace tokens with UNKNOWN if they are too rare, come up with a reasonable limit based on your survey of the data and also model performance)\n",
    "3. Padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Make a dictionary where the string representation of a lexicon item can be retrieved from its numerical index'''\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
    "    print(dict(list(lexicon_lookup.items())[:20]))\n",
    "    return lexicon_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token_sents</th>\n",
       "      <th>Sentence_Idxs</th>\n",
       "      <th>token_tags</th>\n",
       "      <th>Tag_Idxs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence: 30966</th>\n",
       "      <th>0</th>\n",
       "      <td>[President, Bush, has, outlined, the, agenda, ...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 4...</td>\n",
       "      <td>[B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 32442</th>\n",
       "      <th>0</th>\n",
       "      <td>[Commuters, were, angered, Tuesday, morning, w...</td>\n",
       "      <td>[1, 26, 27, 28, 22, 29, 30, 31, 32, 33, 17, 34...</td>\n",
       "      <td>[O, O, O, B-tim, I-tim, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[4, 4, 4, 6, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 13584</th>\n",
       "      <th>0</th>\n",
       "      <td>[Retirement, and, social, assistance, pensions...</td>\n",
       "      <td>[1, 14, 36, 37, 38, 26, 39, 40, 25]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 38902</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, Shepherd, did, so, ,, and, the, Lion, ,,...</td>\n",
       "      <td>[41, 42, 43, 44, 45, 14, 6, 46, 45, 47, 48, 49...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 38245</th>\n",
       "      <th>0</th>\n",
       "      <td>[Lee, ,, a, former, Hyundai, executive, and, S...</td>\n",
       "      <td>[59, 45, 60, 61, 62, 63, 14, 64, 65, 66, 67, 6...</td>\n",
       "      <td>[B-per, O, O, O, B-org, O, O, B-geo, O, O, O, ...</td>\n",
       "      <td>[2, 4, 4, 4, 8, 4, 4, 9, 4, 4, 4, 4, 8, 10, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 8197</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, largest, of, sea, turtles, roams, the, w...</td>\n",
       "      <td>[41, 80, 17, 81, 82, 1, 6, 83, 71, 84, 45, 1, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 47512</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, region, is, located, along, a, major, As...</td>\n",
       "      <td>[41, 88, 89, 90, 91, 60, 92, 93, 94, 95, 8, 96...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 21414</th>\n",
       "      <th>0</th>\n",
       "      <td>[Nobel, laureate, and, former, U.S., Vice, Pre...</td>\n",
       "      <td>[97, 98, 14, 61, 99, 100, 2, 101, 102, 4, 103,...</td>\n",
       "      <td>[B-art, O, O, O, B-geo, B-per, I-per, I-per, I...</td>\n",
       "      <td>[11, 4, 4, 4, 9, 2, 3, 3, 3, 4, 4, 4, 8, 4, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 30009</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, infrastructure, loans, are, part, of, $,...</td>\n",
       "      <td>[41, 110, 111, 112, 113, 17, 114, 115, 116, 12...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-geo, O,...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 25218</th>\n",
       "      <th>0</th>\n",
       "      <td>[A, major, figure, in, U.S., journalism, ,, Ti...</td>\n",
       "      <td>[127, 92, 128, 12, 99, 129, 45, 130, 131, 45, ...</td>\n",
       "      <td>[O, O, O, O, B-geo, O, O, B-per, I-per, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 9, 4, 4, 2, 3, 4, 4, 4, 4, 9, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         token_sents  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [President, Bush, has, outlined, the, agenda, ...   \n",
       "Sentence: 32442 0  [Commuters, were, angered, Tuesday, morning, w...   \n",
       "Sentence: 13584 0  [Retirement, and, social, assistance, pensions...   \n",
       "Sentence: 38902 0  [The, Shepherd, did, so, ,, and, the, Lion, ,,...   \n",
       "Sentence: 38245 0  [Lee, ,, a, former, Hyundai, executive, and, S...   \n",
       "Sentence: 8197  0  [The, largest, of, sea, turtles, roams, the, w...   \n",
       "Sentence: 47512 0  [The, region, is, located, along, a, major, As...   \n",
       "Sentence: 21414 0  [Nobel, laureate, and, former, U.S., Vice, Pre...   \n",
       "Sentence: 30009 0  [The, infrastructure, loans, are, part, of, $,...   \n",
       "Sentence: 25218 0  [A, major, figure, in, U.S., journalism, ,, Ti...   \n",
       "\n",
       "                                                       Sentence_Idxs  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 4...   \n",
       "Sentence: 32442 0  [1, 26, 27, 28, 22, 29, 30, 31, 32, 33, 17, 34...   \n",
       "Sentence: 13584 0                [1, 14, 36, 37, 38, 26, 39, 40, 25]   \n",
       "Sentence: 38902 0  [41, 42, 43, 44, 45, 14, 6, 46, 45, 47, 48, 49...   \n",
       "Sentence: 38245 0  [59, 45, 60, 61, 62, 63, 14, 64, 65, 66, 67, 6...   \n",
       "Sentence: 8197  0  [41, 80, 17, 81, 82, 1, 6, 83, 71, 84, 45, 1, ...   \n",
       "Sentence: 47512 0  [41, 88, 89, 90, 91, 60, 92, 93, 94, 95, 8, 96...   \n",
       "Sentence: 21414 0  [97, 98, 14, 61, 99, 100, 2, 101, 102, 4, 103,...   \n",
       "Sentence: 30009 0  [41, 110, 111, 112, 113, 17, 114, 115, 116, 12...   \n",
       "Sentence: 25218 0  [127, 92, 128, 12, 99, 129, 45, 130, 131, 45, ...   \n",
       "\n",
       "                                                          token_tags  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...   \n",
       "Sentence: 32442 0  [O, O, O, B-tim, I-tim, O, O, O, O, O, O, O, O...   \n",
       "Sentence: 13584 0                        [O, O, O, O, O, O, O, O, O]   \n",
       "Sentence: 38902 0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "Sentence: 38245 0  [B-per, O, O, O, B-org, O, O, B-geo, O, O, O, ...   \n",
       "Sentence: 8197  0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "Sentence: 47512 0            [O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "Sentence: 21414 0  [B-art, O, O, O, B-geo, B-per, I-per, I-per, I...   \n",
       "Sentence: 30009 0  [O, O, O, O, O, O, O, O, O, O, O, O, B-geo, O,...   \n",
       "Sentence: 25218 0  [O, O, O, O, B-geo, O, O, B-per, I-per, O, O, ...   \n",
       "\n",
       "                                                            Tag_Idxs  \n",
       "Sentence #                                                            \n",
       "Sentence: 30966 0  [2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 32442 0         [4, 4, 4, 6, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 13584 0                        [4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 38902 0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 38245 0  [2, 4, 4, 4, 8, 4, 4, 9, 4, 4, 4, 4, 8, 10, 10...  \n",
       "Sentence: 8197  0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 47512 0            [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 21414 0  [11, 4, 4, 4, 9, 2, 3, 3, 3, 4, 4, 4, 8, 4, 2,...  \n",
       "Sentence: 30009 0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 4, ...  \n",
       "Sentence: 25218 0  [4, 4, 4, 4, 9, 4, 4, 2, 3, 4, 4, 4, 4, 9, 4, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_sents['Sentence_Idxs'] = tokens_to_idxs(train_sents['token_sents'], words_lexicon)\n",
    "train_sents['Tag_Idxs'] = tokens_to_idxs(train_sents['token_tags'], tags_lexicon)\n",
    "train_sents[['token_sents', 'Sentence_Idxs', 'token_tags', 'Tag_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      " [[   0    0    0 ...   23   24   25]\n",
      " [   0    0    0 ...   34   35   25]\n",
      " [   0    0    0 ...   39   40   25]\n",
      " ...\n",
      " [   0    0    0 ...    6  318   25]\n",
      " [   0    0    0 ...  259  374   25]\n",
      " [   0    0    0 ... 3531  781   25]]\n",
      "SHAPE: (40765, 105) \n",
      "\n",
      "TAGS:\n",
      " [[0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " ...\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]]\n",
      "SHAPE: (40765, 105) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_sents['Sentence_Idxs']]) # Get length of longest sequence\n",
    "train_padded_words = pad_idx_seqs(train_sents['Sentence_Idxs'], \n",
    "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "train_padded_tags = pad_idx_seqs(train_sents['Tag_Idxs'],\n",
    "                                 max_seq_len + 1)  #Add one to max length for offsetting sequence by 1\n",
    "\n",
    "print(\"WORDS:\\n\", train_padded_words)\n",
    "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n",
    "\n",
    "print(\"TAGS:\\n\", train_padded_tags)\n",
    "print(\"SHAPE:\", train_padded_tags.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Build the model\n",
    "\n",
    "Here we will build a Bidirectional LSTM-CRF model using the `Bidirectional` function from Keras and `CRF` function from Keras-contrib\n",
    "\n",
    "**Documentation and source code:**\n",
    "\n",
    "https://keras.io/layers/wrappers/#bidirectional\n",
    "\n",
    "https://github.com/keras-team/keras-contrib\n",
    "\n",
    "Fit your model with a validation split of 0.1, feel free to use as many epochs as you like. Base your predictions both from the input words **and** the tags from previous words like in the POS example.\n",
    "\n",
    "After building your model, grade your performance on your test set, both by comparing your predicted output to the actual (*at least 3 examples*) and calculate the averaged precision and recall for your tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Concatenate, TimeDistributed, Dense, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_input_len, n_word_input_nodes, n_tag_input_nodes, n_word_embedding_nodes,\n",
    "                 n_tag_embedding_nodes, n_hidden_nodes, n_dense_nodes, \n",
    "                 stateful=False, batch_size=None):\n",
    "    \n",
    "    #Layers 1\n",
    "    word_input = Input(batch_shape=(batch_size, seq_input_len), name='word_input_layer')\n",
    "    tag_input = Input(batch_shape=(batch_size, seq_input_len), name='tag_input_layer')\n",
    "\n",
    "    #Layers 2\n",
    "    word_embeddings = Embedding(input_dim=n_word_input_nodes,\n",
    "                                output_dim=n_word_embedding_nodes, \n",
    "                                mask_zero=True, name='word_embedding_layer')(word_input) #mask_zero will ignore 0 padding\n",
    "    #Output shape = (batch_size, seq_input_len, n_word_embedding_nodes)\n",
    "    tag_embeddings = Embedding(input_dim=n_tag_input_nodes,\n",
    "                               output_dim=n_tag_embedding_nodes,\n",
    "                               mask_zero=True, name='tag_embedding_layer')(tag_input) \n",
    "    #Output shape = (batch_size, seq_input_len, n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 3\n",
    "#     merged_embeddings = Concatenate(axis=-1, name='concat_embedding_layer')([word_embeddings, tag_embeddings])\n",
    "    merged_embeddings = concatenate([word_embeddings, tag_embeddings], name='concat_embedding_layer')\n",
    "    #Output shape =  (batch_size, seq_input_len, n_word_embedding_nodes + n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 4\n",
    "    hidden_layer = Bidirectional(LSTM(units=n_hidden_nodes, return_sequences=True, \n",
    "                                     stateful=stateful, name='hidden_layer'))(merged_embeddings)\n",
    "#     hidden_layer = Bidirectional(GRU(units=n_hidden_nodes, return_sequences=True, \n",
    "#                                      stateful=stateful, name='hidden_layer', \n",
    "#                                      recurrent_regularizer=regularizers.l2(.01),\n",
    "#                                      kernel_regularizer=regularizers.l2(0.01),\n",
    "#                                      activity_regularizer=regularizers.l2(0.01)))(merged_embeddings)\n",
    "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 5\n",
    "    dense_layer = TimeDistributed(Dense(units=n_dense_nodes, activation='relu'), name='dense_layer')(hidden_layer)\n",
    "\n",
    "    #Layer 6\n",
    "    crf = CRF(units=n_tag_input_nodes, learn_mode='marginal', sparse_target=True, name='output_layer')\n",
    "#     output_layer = crf(hidden_layer)\n",
    "    output_layer = crf(dense_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "#     adamOpt = Adam(clipvalue = 1, clipnorm = 1)\n",
    "    model.compile(loss=crf.loss_function, optimizer=\"rmsprop\", metrics=[crf.accuracy])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#     output_layer = TimeDistributed(Dense(units=n_tag_input_nodes, \n",
    "#                                          activation='softmax'), name='output_layer')(hidden_layer)\n",
    "#     # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "#     #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "#     model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_word_embedding_nodes=300\n",
    "n_tag_embedding_nodes=150\n",
    "n_hidden_nodes=400\n",
    "n_dense_nodes=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
    "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_word_embedding_nodes=n_word_embedding_nodes,\n",
    "                     n_tag_embedding_nodes=n_tag_embedding_nodes,\n",
    "                     n_hidden_nodes=n_hidden_nodes, \n",
    "                     n_dense_nodes=n_dense_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input_layer (InputLayer)   (None, 104)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tag_input_layer (InputLayer)    (None, 104)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 104, 300)     5650800     word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tag_embedding_layer (Embedding) (None, 104, 150)     2850        tag_input_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concat_embedding_layer (Concate (None, 104, 450)     0           word_embedding_layer[0][0]       \n",
      "                                                                 tag_embedding_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 104, 800)     2723200     concat_embedding_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (TimeDistributed)   (None, 104, 100)     80100       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (CRF)              (None, 104, 19)      2318        dense_layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 8,459,268\n",
      "Trainable params: 8,459,268\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./models/ner_temp_model_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36688 samples, validate on 4077 samples\n",
      "Epoch 1/15\n",
      "36608/36688 [============================>.] - ETA: 1s - loss: 0.1602 - acc: 0.9641\n",
      "Epoch 00001: saving model to ./models/ner_temp_model_weights-01-0.1598.hdf5\n",
      "36688/36688 [==============================] - 530s 14ms/step - loss: 0.1598 - acc: 0.9642 - val_loss: 9.8894e-04 - val_acc: 0.9997\n",
      "Epoch 2/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 9.4167e-04 - acc: 0.9998\n",
      "Epoch 00002: saving model to ./models/ner_temp_model_weights-02-0.0009.hdf5\n",
      "36688/36688 [==============================] - 323s 9ms/step - loss: 9.3965e-04 - acc: 0.9998 - val_loss: 1.4135e-04 - val_acc: 1.0000\n",
      "Epoch 3/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 1.5369e-04 - acc: 1.0000\n",
      "Epoch 00003: saving model to ./models/ner_temp_model_weights-03-0.0002.hdf5\n",
      "36688/36688 [==============================] - 323s 9ms/step - loss: 1.5336e-04 - acc: 1.0000 - val_loss: 9.3988e-05 - val_acc: 1.0000\n",
      "Epoch 4/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 1.3125e-04 - acc: 1.0000\n",
      "Epoch 00004: saving model to ./models/ner_temp_model_weights-04-0.0001.hdf5\n",
      "36688/36688 [==============================] - 323s 9ms/step - loss: 1.3096e-04 - acc: 1.0000 - val_loss: 7.4061e-05 - val_acc: 1.0000\n",
      "Epoch 5/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 3.5957e-05 - acc: 1.0000\n",
      "Epoch 00005: saving model to ./models/ner_temp_model_weights-05-0.0000.hdf5\n",
      "36688/36688 [==============================] - 323s 9ms/step - loss: 3.5883e-05 - acc: 1.0000 - val_loss: 7.9739e-05 - val_acc: 1.0000\n",
      "Epoch 6/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 4.5665e-05 - acc: 1.0000\n",
      "Epoch 00006: saving model to ./models/ner_temp_model_weights-06-0.0000.hdf5\n",
      "36688/36688 [==============================] - 329s 9ms/step - loss: 4.5570e-05 - acc: 1.0000 - val_loss: 5.5598e-05 - val_acc: 1.0000\n",
      "Epoch 7/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 1.1514e-05 - acc: 1.0000\n",
      "Epoch 00007: saving model to ./models/ner_temp_model_weights-07-0.0000.hdf5\n",
      "36688/36688 [==============================] - 328s 9ms/step - loss: 1.1493e-05 - acc: 1.0000 - val_loss: 7.7907e-05 - val_acc: 1.0000\n",
      "Epoch 8/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 4.4345e-06 - acc: 1.0000\n",
      "Epoch 00008: saving model to ./models/ner_temp_model_weights-08-0.0000.hdf5\n",
      "36688/36688 [==============================] - 328s 9ms/step - loss: 4.4292e-06 - acc: 1.0000 - val_loss: 9.0126e-05 - val_acc: 1.0000\n",
      "Epoch 9/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 2.0680e-06 - acc: 1.0000\n",
      "Epoch 00009: saving model to ./models/ner_temp_model_weights-09-0.0000.hdf5\n",
      "36688/36688 [==============================] - 328s 9ms/step - loss: 2.0679e-06 - acc: 1.0000 - val_loss: 1.3946e-04 - val_acc: 1.0000\n",
      "Epoch 10/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 2.0397e-06 - acc: 1.0000\n",
      "Epoch 00010: saving model to ./models/ner_temp_model_weights-10-0.0000.hdf5\n",
      "36688/36688 [==============================] - 328s 9ms/step - loss: 2.0397e-06 - acc: 1.0000 - val_loss: 1.5623e-04 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 2.0355e-06 - acc: 1.0000\n",
      "Epoch 00011: saving model to ./models/ner_temp_model_weights-11-0.0000.hdf5\n",
      "36688/36688 [==============================] - 328s 9ms/step - loss: 2.0355e-06 - acc: 1.0000 - val_loss: 1.5664e-04 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 2.0335e-06 - acc: 1.0000\n",
      "Epoch 00012: saving model to ./models/ner_temp_model_weights-12-0.0000.hdf5\n",
      "36688/36688 [==============================] - 329s 9ms/step - loss: 2.0335e-06 - acc: 1.0000 - val_loss: 1.5985e-04 - val_acc: 1.0000\n",
      "Epoch 13/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 2.0323e-06 - acc: 1.0000\n",
      "Epoch 00013: saving model to ./models/ner_temp_model_weights-13-0.0000.hdf5\n",
      "36688/36688 [==============================] - 329s 9ms/step - loss: 2.0323e-06 - acc: 1.0000 - val_loss: 1.6352e-04 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 2.0315e-06 - acc: 1.0000\n",
      "Epoch 00014: saving model to ./models/ner_temp_model_weights-14-0.0000.hdf5\n",
      "36688/36688 [==============================] - 329s 9ms/step - loss: 2.0315e-06 - acc: 1.0000 - val_loss: 1.6494e-04 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "36608/36688 [============================>.] - ETA: 0s - loss: 2.0309e-06 - acc: 1.0000\n",
      "Epoch 00015: saving model to ./models/ner_temp_model_weights-15-0.0000.hdf5\n",
      "36688/36688 [==============================] - 329s 9ms/step - loss: 2.0309e-06 - acc: 1.0000 - val_loss: 1.6547e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246a16abcc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
    "model.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
    "          y=train_padded_tags[:, 1:, None], \n",
    "          batch_size=128, epochs=15, validation_split=.1, \n",
    "          callbacks=callbacks_list)\n",
    "# model.save_weights('models/ner_temp_model_weights.h5') #Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'B-per', 3: 'I-per', 4: 'O', 5: 'B-gpe', 6: 'B-tim', 7: 'I-tim', 8: 'B-org', 9: 'B-geo', 10: 'I-org', 11: 'B-art', 12: 'I-geo', 13: 'B-eve', 14: 'I-eve', 15: 'I-gpe', 16: 'I-art', 17: 'B-nat', 18: 'I-nat', 1: '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "with open('models/words_lexicon.pkl', 'rb') as f:\n",
    "    words_lexicon = pickle.load(f)\n",
    "    \n",
    "with open('models/tags_lexicon.pkl', 'rb') as f:\n",
    "    tags_lexicon = pickle.load(f)\n",
    "\n",
    "tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)\n",
    "\n",
    "predictor_model = create_model(seq_input_len=1,\n",
    "                               n_word_input_nodes=len(words_lexicon) + 1,\n",
    "                               n_tag_input_nodes=len(tags_lexicon) + 1,\n",
    "                               n_word_embedding_nodes=n_word_embedding_nodes,\n",
    "                               n_tag_embedding_nodes=n_tag_embedding_nodes,\n",
    "                               n_hidden_nodes=n_hidden_nodes, \n",
    "                               n_dense_nodes=n_dense_nodes,\n",
    "                               stateful=True,\n",
    "                               batch_size=1)\n",
    "\n",
    "#Transfer the weights from the trained model\n",
    "predictor_model.load_weights('./models/ner_temp_model_weights-15-0.0000.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "'''Load the test set and apply same processing steps performed above for training set'''\n",
    "\n",
    "test_sents['Sentence_Idxs'] = tokens_to_idxs(test_sents['token_sents'], words_lexicon)\n",
    "test_sents['Tag_Idxs'] = tokens_to_idxs(test_sents['token_tags'], tags_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE:\tThe\tGeo\tand\tSAMA\tchannels\t,\twere\ttaken\toff\tthe\tair\tfor\ta\tshort\tperiod\tof\ttime\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tB-org\tO\tB-org\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tFalse\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tThe\tprovincial\tpolice\tchief\t,\tChaudhry\tMohammed\tYaqoob\t,\tsays\tthe\tsuspects\tare\tall\tethnic\tBaluch\ttribesmen\tand\twere\tarrested\tovernight\tin\ta\tseries\tof\traids\tin\tQuetta\t,\tthe\tcapital\tof\tBaluchistan\tprovince\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tB-per\tI-per\tI-per\tO\tO\tO\tO\tO\tO\tO\tB-org\tO\tO\tO\tO\tB-tim\tO\tO\tO\tO\tO\tO\tB-geo\tO\tO\tO\tO\tB-geo\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tPalestinian\tand\tEgyptian\tsecurity\tsources\tsay\tMahmoud\tal-Zahar\tcrossed\tinto\tEgypt\tThursday\twith\ta\tsmall\tdelegation\tof\tHamas\tofficials\t.\n",
      "PREDICTED:\tB-gpe\tO\tB-gpe\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tB-gpe\tO\tB-gpe\tO\tO\tO\tB-per\tI-per\tO\tO\tB-geo\tB-tim\tO\tO\tO\tO\tO\tB-org\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tTrue\tTrue\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tThe\tprivate\tsector\taccounts\tfor\tmore\tthan\t80\t%\tof\tGDP\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tIn\treturn\t,\tit\twill\treceive\tabout\tone\tmillion\ttons\tof\tfuel\toil\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tVOA\t's\tMil\tArcega\treports\t.\n",
      "PREDICTED:\tB-geo\tO\tB-geo\tO\tB-geo\tO\n",
      "GOLD:\t\tB-org\tI-org\tB-per\tI-per\tO\tO\n",
      "CORRECT:\tFalse\tFalse\tFalse\tFalse\tFalse\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tOfficials\tin\tBeirut\ttoday\tMonday\tsay\tthe\tlatest\twithdrawal\tbrings\tthe\tSyrian\tmilitary\tpresence\tdown\tto\tabout\teight-thousand\ttroops\t,\tthe\tlowest\tlevel\tsince\tthey\tentered\tLebanon\tnearly\tthree\tdecades\tago\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tB-geo\tB-tim\tI-tim\tO\tO\tO\tO\tO\tO\tB-gpe\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tB-geo\tO\tB-tim\tI-tim\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tFalse\tFalse\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tElsewhere\t,\ta\tsuicide\tcar\tbomber\tkilled\tfour\tIraqi\tsoldiers\tnear\tSaddam\t's\thometown\tof\tTikrit\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tO\tO\tO\tB-gpe\tO\tO\tB-per\tO\tO\tO\tB-geo\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tFalse\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tHe\tsaid\tVenezuela\thas\talready\timplemented\ta\t50,000\tbarrel\tper\tday\toutput\treduction\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tB-geo\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tAuthorities\tsay\tthe\taccident\toccurred\tSaturday\t,\tnear\tthe\ttown\tof\tVeligonda\tin\tsouthern\tAndhra\tPradesh\tstate\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tB-tim\tO\tO\tO\tO\tO\tB-geo\tO\tB-geo\tI-geo\tI-geo\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tFalse\tFalse\tFalse\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tIn\tJuly\t2009\t,\tafter\tmonths\tof\tharassment\tagainst\this\topponents\tand\tmedia\tcritics\t,\tBAKIEV\twon\tre-election\tin\ta\tpresidential\tcampaign\tthat\tthe\tinternational\tcommunity\tdeemed\tflawed\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tB-tim\tI-tim\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tB-per\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tThe\tIsraeli\tmilitary\tsays\ttwo\tIslamic\tJihad\tmembers\twere\tkilled\tafter\tthey\topened\tfire\ton\tIsraeli\tspecial\tforces\toperating\tin\tthe\tarea\t.\n",
      "PREDICTED:\tO\tB-gpe\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tB-gpe\tO\tO\tO\tB-org\tI-org\tO\tO\tO\tO\tO\tO\tO\tO\tB-gpe\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tIvory\tCoast\t's\tprime\tminister\tsays\the\tis\t\"\tconvinced\t\"\tthat\tnew\tAfrican\tUnion\tpresident\tDenis\tSassou-Nguesso\twill\tfully\tsupport\tthe\tU.N.\tpeace\tplan\tfor\this\tcountry\t.\n",
      "PREDICTED:\tB-geo\tI-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\n",
      "GOLD:\t\tB-org\tI-org\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tB-geo\tI-geo\tO\tB-per\tI-per\tO\tO\tO\tO\tB-geo\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tFalse\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\tTrue\tFalse\tFalse\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tThe\ttwo\tnuclear-armed\trivals\t,\twhich\thave\tfought\tthree\twars\tin\tthe\tpast\tfive\tdecades\t,\tare\tcurrently\tin\tpeace\ttalks\tover\tall\toutstanding\tissues\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tB-tim\tI-tim\tI-tim\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tThey\tare\tthe\tfirst\treported\tin\tBurma\tsince\tMarch\t2006\t.\n",
      "PREDICTED:\tB-gpe\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tO\tB-geo\tB-tim\tI-tim\tI-tim\tO\n",
      "CORRECT:\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tFalse\tFalse\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tHe\tis\texpected\tto\tspend\tMonday\tvisiting\ttroops\ton\tthe\tbase\tand\teating\twith\tthem\tin\tthe\tdining\tfacilities\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tB-tim\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tXinhua\tsays\tthe\tpurpose\tof\tthe\tbody\tis\tto\tconduct\tpolitical\tconsultation\tand\tdebate\tissues\tof\tstate\t.\n",
      "PREDICTED:\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\tB-geo\tO\n",
      "GOLD:\t\tB-org\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "CORRECT:\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue\tFalse\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tThe\tpolice\tofficers\twere\tcaptured\tin\ta\tFARC\toffensive\tlate\tlast\tyear\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tO\tO\tB-org\tO\tO\tO\tO\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tThe\tstrike\t,\tset\tfor\tMonday\t,\twould\tcome\ttwo\tweeks\tafter\tthe\tassassination\tof\tformer\tLebanese\tPrime\tMinister\tRafik\tHariri\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tO\tO\tO\tO\tB-tim\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tB-gpe\tB-per\tO\tB-per\tI-per\tO\n",
      "CORRECT:\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tFalse\tTrue\tFalse\tFalse\tTrue \n",
      "\n",
      "\n",
      "SENTENCE:\tA\tMarine\tpatrol\tfound\tthe\tunidentified\tbody\tin\ta\tstreet\tof\tthe\tviolence-torn\tIraqi\tcity\t.\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "GOLD:\t\tO\tB-org\tI-org\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tB-gpe\tO\tO\n",
      "CORRECT:\tTrue\tFalse\tFalse\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tTrue\tFalse\tTrue\tTrue \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "'''Predict tags for sentences in test set'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "pred_tags = []\n",
    "for _, sent in test_sents.iterrows():\n",
    "    tok_sent = sent['token_sents']\n",
    "    sent_idxs = sent['Sentence_Idxs']\n",
    "    sent_gold_tags = sent['token_tags']\n",
    "    sent_pred_tags = []\n",
    "    prev_tag = 1  #initialize predicted tag sequence with padding\n",
    "#     prev_tag = 0  #initialize predicted tag sequence with padding\n",
    "    for cur_word in sent_idxs:\n",
    "        # cur_word and prev_tag are just integers, but the model expects an input array\n",
    "        # with the shape (batch_size, seq_input_len), so prepend two dimensions to these values\n",
    "        p_next_tag = predictor_model.predict(x=[numpy.array(cur_word)[None, None],\n",
    "                                                numpy.array(prev_tag)[None, None]])[0]\n",
    "        prev_tag = numpy.argmax(p_next_tag, axis=-1)[0]\n",
    "        sent_pred_tags.append(prev_tag)\n",
    "    predictor_model.reset_states()\n",
    "\n",
    "    #Map tags back to string labels\n",
    "    sent_pred_tags = [tags_lexicon_lookup[tag] for tag in sent_pred_tags]\n",
    "    pred_tags.append(sent_pred_tags) #filter padding \n",
    "\n",
    "test_sents['Predicted_token_tags'] = pred_tags\n",
    "\n",
    "#print sample\n",
    "for _, sent in test_sents[30:50].iterrows():\n",
    "    print(\"SENTENCE:\\t{}\".format(\"\\t\".join(sent['token_sents'])))\n",
    "    print(\"PREDICTED:\\t{}\".format(\"\\t\".join(sent['Predicted_token_tags'])))\n",
    "    print(\"GOLD:\\t\\t{}\".format(\"\\t\".join(sent['token_tags'])))\n",
    "    print(\"CORRECT:\\t{}\".format(\"\\t\".join([str(x) for x in np.array(sent['token_tags']) == np.array(sent['Predicted_token_tags'])])), \"\\n\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.741\n",
      "PRECISION: 0.741\n",
      "RECALL: 0.741\n",
      "F1: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "'''Evalute the model by precision, recall, and F1'''\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_gold_tags = [tag for sent_tags in test_sents['token_tags'] for tag in sent_tags]\n",
    "    all_pred_tags = [tag for sent_tags in test_sents['Predicted_token_tags'] for tag in sent_tags]\n",
    "    accuracy = accuracy_score(y_true=all_gold_tags, y_pred=all_pred_tags)\n",
    "    precision = precision_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "    recall = recall_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "    f1 = f1_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "\n",
    "    print(\"ACCURACY: {:.3f}\".format(accuracy))\n",
    "    print(\"PRECISION: {:.3f}\".format(precision))\n",
    "    print(\"RECALL: {:.3f}\".format(recall))\n",
    "    print(\"F1: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Pick a dataset\n",
    "\n",
    "Pick a dataset that has short text, similar to the sentences you just tagged. Headlines and tweets are good choices.\n",
    "\n",
    "https://www.kaggle.com/datasets?sortBy=relevance&group=public&search=news&page=1&pageSize=20&size=all&filetype=all&license=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit new data to only 100 for testing purposes rows\n",
    "newData = pd.read_csv('../data/abcnews-date-text.csv', nrows = 100)\n",
    "newData['token_sents'] = newData['headline_text'].apply(lambda x: text_to_word_sequence(x, lower=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>token_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>[aba, decides, against, community, broadcastin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>[act, fire, witnesses, must, be, aware, of, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>[a, g, calls, for, infrastructure, protection,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>[air, nz, staff, in, aust, strike, for, pay, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>[air, nz, strike, to, affect, australian, trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20030219</td>\n",
       "      <td>ambitious olsson wins triple jump</td>\n",
       "      <td>[ambitious, olsson, wins, triple, jump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20030219</td>\n",
       "      <td>antic delighted with record breaking barca</td>\n",
       "      <td>[antic, delighted, with, record, breaking, barca]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aussie qualifier stosur wastes four memphis match</td>\n",
       "      <td>[aussie, qualifier, stosur, wastes, four, memp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aust addresses un security council over iraq</td>\n",
       "      <td>[aust, addresses, un, security, council, over,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20030219</td>\n",
       "      <td>australia is locked into war timetable opp</td>\n",
       "      <td>[australia, is, locked, into, war, timetable, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20030219</td>\n",
       "      <td>australia to contribute 10 million in aid to iraq</td>\n",
       "      <td>[australia, to, contribute, 10, million, in, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20030219</td>\n",
       "      <td>barca take record as robson celebrates birthda...</td>\n",
       "      <td>[barca, take, record, as, robson, celebrates, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20030219</td>\n",
       "      <td>bathhouse plans move ahead</td>\n",
       "      <td>[bathhouse, plans, move, ahead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20030219</td>\n",
       "      <td>big hopes for launceston cycling championship</td>\n",
       "      <td>[big, hopes, for, launceston, cycling, champio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20030219</td>\n",
       "      <td>big plan to boost paroo water supplies</td>\n",
       "      <td>[big, plan, to, boost, paroo, water, supplies]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20030219</td>\n",
       "      <td>blizzard buries united states in bills</td>\n",
       "      <td>[blizzard, buries, united, states, in, bills]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20030219</td>\n",
       "      <td>brigadier dismisses reports troops harassed in</td>\n",
       "      <td>[brigadier, dismisses, reports, troops, harass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20030219</td>\n",
       "      <td>british combat troops arriving daily in kuwait</td>\n",
       "      <td>[british, combat, troops, arriving, daily, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20030219</td>\n",
       "      <td>bryant leads lakers to double overtime win</td>\n",
       "      <td>[bryant, leads, lakers, to, double, overtime, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20030219</td>\n",
       "      <td>bushfire victims urged to see centrelink</td>\n",
       "      <td>[bushfire, victims, urged, to, see, centrelink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20030219</td>\n",
       "      <td>businesses should prepare for terrorist attacks</td>\n",
       "      <td>[businesses, should, prepare, for, terrorist, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20030219</td>\n",
       "      <td>calleri avenges final defeat to eliminate massu</td>\n",
       "      <td>[calleri, avenges, final, defeat, to, eliminat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20030219</td>\n",
       "      <td>call for ethanol blend fuel to go ahead</td>\n",
       "      <td>[call, for, ethanol, blend, fuel, to, go, ahead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20030219</td>\n",
       "      <td>carews freak goal leaves roma in ruins</td>\n",
       "      <td>[carews, freak, goal, leaves, roma, in, ruins]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20030219</td>\n",
       "      <td>cemeteries miss out on funds</td>\n",
       "      <td>[cemeteries, miss, out, on, funds]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20030219</td>\n",
       "      <td>code of conduct toughens organ donation regula...</td>\n",
       "      <td>[code, of, conduct, toughens, organ, donation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20030219</td>\n",
       "      <td>commonwealth bank cuts fixed home loan rates</td>\n",
       "      <td>[commonwealth, bank, cuts, fixed, home, loan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20030219</td>\n",
       "      <td>community urged to help homeless youth</td>\n",
       "      <td>[community, urged, to, help, homeless, youth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20030219</td>\n",
       "      <td>council chief executive fails to secure position</td>\n",
       "      <td>[council, chief, executive, fails, to, secure,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20030219</td>\n",
       "      <td>councillor to contest wollongong as independent</td>\n",
       "      <td>[councillor, to, contest, wollongong, as, inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>20030219</td>\n",
       "      <td>heavy metal deposits survey nearing end</td>\n",
       "      <td>[heavy, metal, deposits, survey, nearing, end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>20030219</td>\n",
       "      <td>injured rios pulls out of buenos aires open</td>\n",
       "      <td>[injured, rios, pulls, out, of, buenos, aires,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>20030219</td>\n",
       "      <td>inquest finds mans death accidental</td>\n",
       "      <td>[inquest, finds, mans, death, accidental]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>20030219</td>\n",
       "      <td>investigations underway into death toll of korean</td>\n",
       "      <td>[investigations, underway, into, death, toll, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>20030219</td>\n",
       "      <td>investigation underway into elster creek spill</td>\n",
       "      <td>[investigation, underway, into, elster, creek,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>20030219</td>\n",
       "      <td>iraqs neighbours plead for continued un inspec...</td>\n",
       "      <td>[iraqs, neighbours, plead, for, continued, un,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20030219</td>\n",
       "      <td>iraq to pay for own rebuilding white house</td>\n",
       "      <td>[iraq, to, pay, for, own, rebuilding, white, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>20030219</td>\n",
       "      <td>irish man arrested over omagh bombing</td>\n",
       "      <td>[irish, man, arrested, over, omagh, bombing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20030219</td>\n",
       "      <td>irrigators vote over river management</td>\n",
       "      <td>[irrigators, vote, over, river, management]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>20030219</td>\n",
       "      <td>israeli forces push into gaza strip</td>\n",
       "      <td>[israeli, forces, push, into, gaza, strip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>20030219</td>\n",
       "      <td>jury to consider verdict in murder case</td>\n",
       "      <td>[jury, to, consider, verdict, in, murder, case]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>20030219</td>\n",
       "      <td>juvenile sex offenders unlikely to reoffend as</td>\n",
       "      <td>[juvenile, sex, offenders, unlikely, to, reoff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20030219</td>\n",
       "      <td>kelly disgusted at alleged bp ethanol scare</td>\n",
       "      <td>[kelly, disgusted, at, alleged, bp, ethanol, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>20030219</td>\n",
       "      <td>kelly not surprised ethanol confidence low</td>\n",
       "      <td>[kelly, not, surprised, ethanol, confidence, low]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>20030219</td>\n",
       "      <td>korean subway fire 314 still missing</td>\n",
       "      <td>[korean, subway, fire, 314, still, missing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>20030219</td>\n",
       "      <td>last minute call hands alinghi big lead</td>\n",
       "      <td>[last, minute, call, hands, alinghi, big, lead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>20030219</td>\n",
       "      <td>low demand forces air service cuts</td>\n",
       "      <td>[low, demand, forces, air, service, cuts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>20030219</td>\n",
       "      <td>man arrested after central qld hijack attempt</td>\n",
       "      <td>[man, arrested, after, central, qld, hijack, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>20030219</td>\n",
       "      <td>man charged over cooma murder</td>\n",
       "      <td>[man, charged, over, cooma, murder]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>20030219</td>\n",
       "      <td>man fined after aboriginal tent embassy raid</td>\n",
       "      <td>[man, fined, after, aboriginal, tent, embassy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>20030219</td>\n",
       "      <td>man jailed over keno fraud</td>\n",
       "      <td>[man, jailed, over, keno, fraud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>20030219</td>\n",
       "      <td>man with knife hijacks light plane</td>\n",
       "      <td>[man, with, knife, hijacks, light, plane]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>20030219</td>\n",
       "      <td>martin to lobby against losing nt seat in fed</td>\n",
       "      <td>[martin, to, lobby, against, losing, nt, seat,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>20030219</td>\n",
       "      <td>massive drug crop discovered in western nsw</td>\n",
       "      <td>[massive, drug, crop, discovered, in, western,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>20030219</td>\n",
       "      <td>mayor warns landfill protesters</td>\n",
       "      <td>[mayor, warns, landfill, protesters]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>20030219</td>\n",
       "      <td>meeting to consider tick clearance costs</td>\n",
       "      <td>[meeting, to, consider, tick, clearance, costs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>20030219</td>\n",
       "      <td>meeting to focus on broken hill water woes</td>\n",
       "      <td>[meeting, to, focus, on, broken, hill, water, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>20030219</td>\n",
       "      <td>moderate lift in wages growth</td>\n",
       "      <td>[moderate, lift, in, wages, growth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>20030219</td>\n",
       "      <td>more than 40 pc of young men drink alcohol at</td>\n",
       "      <td>[more, than, 40, pc, of, young, men, drink, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>20030219</td>\n",
       "      <td>more water restrictions predicted for northern...</td>\n",
       "      <td>[more, water, restrictions, predicted, for, no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    publish_date                                      headline_text  \\\n",
       "0       20030219  aba decides against community broadcasting lic...   \n",
       "1       20030219     act fire witnesses must be aware of defamation   \n",
       "2       20030219     a g calls for infrastructure protection summit   \n",
       "3       20030219           air nz staff in aust strike for pay rise   \n",
       "4       20030219      air nz strike to affect australian travellers   \n",
       "5       20030219                  ambitious olsson wins triple jump   \n",
       "6       20030219         antic delighted with record breaking barca   \n",
       "7       20030219  aussie qualifier stosur wastes four memphis match   \n",
       "8       20030219       aust addresses un security council over iraq   \n",
       "9       20030219         australia is locked into war timetable opp   \n",
       "10      20030219  australia to contribute 10 million in aid to iraq   \n",
       "11      20030219  barca take record as robson celebrates birthda...   \n",
       "12      20030219                         bathhouse plans move ahead   \n",
       "13      20030219      big hopes for launceston cycling championship   \n",
       "14      20030219             big plan to boost paroo water supplies   \n",
       "15      20030219             blizzard buries united states in bills   \n",
       "16      20030219     brigadier dismisses reports troops harassed in   \n",
       "17      20030219     british combat troops arriving daily in kuwait   \n",
       "18      20030219         bryant leads lakers to double overtime win   \n",
       "19      20030219           bushfire victims urged to see centrelink   \n",
       "20      20030219    businesses should prepare for terrorist attacks   \n",
       "21      20030219    calleri avenges final defeat to eliminate massu   \n",
       "22      20030219            call for ethanol blend fuel to go ahead   \n",
       "23      20030219             carews freak goal leaves roma in ruins   \n",
       "24      20030219                       cemeteries miss out on funds   \n",
       "25      20030219  code of conduct toughens organ donation regula...   \n",
       "26      20030219       commonwealth bank cuts fixed home loan rates   \n",
       "27      20030219             community urged to help homeless youth   \n",
       "28      20030219   council chief executive fails to secure position   \n",
       "29      20030219    councillor to contest wollongong as independent   \n",
       "..           ...                                                ...   \n",
       "70      20030219            heavy metal deposits survey nearing end   \n",
       "71      20030219        injured rios pulls out of buenos aires open   \n",
       "72      20030219                inquest finds mans death accidental   \n",
       "73      20030219  investigations underway into death toll of korean   \n",
       "74      20030219     investigation underway into elster creek spill   \n",
       "75      20030219  iraqs neighbours plead for continued un inspec...   \n",
       "76      20030219         iraq to pay for own rebuilding white house   \n",
       "77      20030219              irish man arrested over omagh bombing   \n",
       "78      20030219              irrigators vote over river management   \n",
       "79      20030219                israeli forces push into gaza strip   \n",
       "80      20030219            jury to consider verdict in murder case   \n",
       "81      20030219     juvenile sex offenders unlikely to reoffend as   \n",
       "82      20030219        kelly disgusted at alleged bp ethanol scare   \n",
       "83      20030219         kelly not surprised ethanol confidence low   \n",
       "84      20030219               korean subway fire 314 still missing   \n",
       "85      20030219            last minute call hands alinghi big lead   \n",
       "86      20030219                 low demand forces air service cuts   \n",
       "87      20030219      man arrested after central qld hijack attempt   \n",
       "88      20030219                      man charged over cooma murder   \n",
       "89      20030219       man fined after aboriginal tent embassy raid   \n",
       "90      20030219                         man jailed over keno fraud   \n",
       "91      20030219                 man with knife hijacks light plane   \n",
       "92      20030219      martin to lobby against losing nt seat in fed   \n",
       "93      20030219        massive drug crop discovered in western nsw   \n",
       "94      20030219                    mayor warns landfill protesters   \n",
       "95      20030219           meeting to consider tick clearance costs   \n",
       "96      20030219         meeting to focus on broken hill water woes   \n",
       "97      20030219                      moderate lift in wages growth   \n",
       "98      20030219      more than 40 pc of young men drink alcohol at   \n",
       "99      20030219  more water restrictions predicted for northern...   \n",
       "\n",
       "                                          token_sents  \n",
       "0   [aba, decides, against, community, broadcastin...  \n",
       "1   [act, fire, witnesses, must, be, aware, of, de...  \n",
       "2   [a, g, calls, for, infrastructure, protection,...  \n",
       "3   [air, nz, staff, in, aust, strike, for, pay, r...  \n",
       "4   [air, nz, strike, to, affect, australian, trav...  \n",
       "5             [ambitious, olsson, wins, triple, jump]  \n",
       "6   [antic, delighted, with, record, breaking, barca]  \n",
       "7   [aussie, qualifier, stosur, wastes, four, memp...  \n",
       "8   [aust, addresses, un, security, council, over,...  \n",
       "9   [australia, is, locked, into, war, timetable, ...  \n",
       "10  [australia, to, contribute, 10, million, in, a...  \n",
       "11  [barca, take, record, as, robson, celebrates, ...  \n",
       "12                    [bathhouse, plans, move, ahead]  \n",
       "13  [big, hopes, for, launceston, cycling, champio...  \n",
       "14     [big, plan, to, boost, paroo, water, supplies]  \n",
       "15      [blizzard, buries, united, states, in, bills]  \n",
       "16  [brigadier, dismisses, reports, troops, harass...  \n",
       "17  [british, combat, troops, arriving, daily, in,...  \n",
       "18  [bryant, leads, lakers, to, double, overtime, ...  \n",
       "19    [bushfire, victims, urged, to, see, centrelink]  \n",
       "20  [businesses, should, prepare, for, terrorist, ...  \n",
       "21  [calleri, avenges, final, defeat, to, eliminat...  \n",
       "22   [call, for, ethanol, blend, fuel, to, go, ahead]  \n",
       "23     [carews, freak, goal, leaves, roma, in, ruins]  \n",
       "24                 [cemeteries, miss, out, on, funds]  \n",
       "25  [code, of, conduct, toughens, organ, donation,...  \n",
       "26  [commonwealth, bank, cuts, fixed, home, loan, ...  \n",
       "27      [community, urged, to, help, homeless, youth]  \n",
       "28  [council, chief, executive, fails, to, secure,...  \n",
       "29  [councillor, to, contest, wollongong, as, inde...  \n",
       "..                                                ...  \n",
       "70     [heavy, metal, deposits, survey, nearing, end]  \n",
       "71  [injured, rios, pulls, out, of, buenos, aires,...  \n",
       "72          [inquest, finds, mans, death, accidental]  \n",
       "73  [investigations, underway, into, death, toll, ...  \n",
       "74  [investigation, underway, into, elster, creek,...  \n",
       "75  [iraqs, neighbours, plead, for, continued, un,...  \n",
       "76  [iraq, to, pay, for, own, rebuilding, white, h...  \n",
       "77       [irish, man, arrested, over, omagh, bombing]  \n",
       "78        [irrigators, vote, over, river, management]  \n",
       "79         [israeli, forces, push, into, gaza, strip]  \n",
       "80    [jury, to, consider, verdict, in, murder, case]  \n",
       "81  [juvenile, sex, offenders, unlikely, to, reoff...  \n",
       "82  [kelly, disgusted, at, alleged, bp, ethanol, s...  \n",
       "83  [kelly, not, surprised, ethanol, confidence, low]  \n",
       "84        [korean, subway, fire, 314, still, missing]  \n",
       "85    [last, minute, call, hands, alinghi, big, lead]  \n",
       "86          [low, demand, forces, air, service, cuts]  \n",
       "87  [man, arrested, after, central, qld, hijack, a...  \n",
       "88                [man, charged, over, cooma, murder]  \n",
       "89  [man, fined, after, aboriginal, tent, embassy,...  \n",
       "90                   [man, jailed, over, keno, fraud]  \n",
       "91          [man, with, knife, hijacks, light, plane]  \n",
       "92  [martin, to, lobby, against, losing, nt, seat,...  \n",
       "93  [massive, drug, crop, discovered, in, western,...  \n",
       "94               [mayor, warns, landfill, protesters]  \n",
       "95    [meeting, to, consider, tick, clearance, costs]  \n",
       "96  [meeting, to, focus, on, broken, hill, water, ...  \n",
       "97                [moderate, lift, in, wages, growth]  \n",
       "98  [more, than, 40, pc, of, young, men, drink, al...  \n",
       "99  [more, water, restrictions, predicted, for, no...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert lexicon to lower case since all new data are lower case\n",
    "all_lower_words_lexicon = {}\n",
    "for key, val in words_lexicon.items():\n",
    "    all_lower_words_lexicon[key.lower()] = val\n",
    "\n",
    "# Replace <UNK> as uppercase to work with previouse functions\n",
    "all_lower_words_lexicon['<UNK>'] = all_lower_words_lexicon['<unk>']\n",
    "all_lower_words_lexicon['<unk>'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Tag your new data!\n",
    "\n",
    "Create a modification to the **ent_tagger function** that combined words and tags from your original dataset. Now allow the function to also load new text from your new data set, and output the tags predicted from your trained model alongside the text. Make your function load five random texts from your data and output the tagged text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner_tag_from_words(new_sents, words_lexicon=None, print_obs=True):\n",
    "    '''Predict tags for sentences in new dataset'''\n",
    "    \n",
    "    new_sents['Sentence_Idxs'] = tokens_to_idxs(new_sents['token_sents'], words_lexicon)\n",
    "    print(new_sents)\n",
    "\n",
    "    pred_tags = []\n",
    "    for _, sent in new_sents.iterrows():\n",
    "        tok_sent = sent['token_sents']\n",
    "        sent_idxs = sent['Sentence_Idxs']\n",
    "        sent_pred_tags = []\n",
    "        prev_tag = 1  #initialize predicted tag sequence to UNKNOWN since padding breaks...\n",
    "#         prev_tag = 0  #initialize predicted tag sequence with padding\n",
    "        for cur_word in sent_idxs:\n",
    "            # cur_word and prev_tag are just integers, but the model expects an input array\n",
    "            # with the shape (batch_size, seq_input_len), so prepend two dimensions to these values\n",
    "            p_next_tag = predictor_model.predict(x=[numpy.array(cur_word)[None, None],\n",
    "                                                    numpy.array(prev_tag)[None, None]])[0]\n",
    "            prev_tag = numpy.argmax(p_next_tag, axis=-1)[0]\n",
    "            sent_pred_tags.append(prev_tag)\n",
    "        predictor_model.reset_states()\n",
    "\n",
    "        #Map tags back to string labels\n",
    "        sent_pred_tags = [tags_lexicon_lookup[tag] for tag in sent_pred_tags]\n",
    "        pred_tags.append(sent_pred_tags) #filter padding \n",
    "\n",
    "    new_sents['Predicted_token_tags'] = pred_tags\n",
    "\n",
    "    if print_obs:\n",
    "        for _, sent in new_sents.iterrows():\n",
    "            print(\"SENTENCE:\\t{}\".format(\"\\t\".join(sent['token_sents'])))\n",
    "            print(\"PREDICTED:\\t{}\".format(\"\\t\".join(sent['Predicted_token_tags'])))\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    publish_date                                      headline_text  \\\n",
      "0       20030219  aba decides against community broadcasting lic...   \n",
      "1       20030219     act fire witnesses must be aware of defamation   \n",
      "2       20030219     a g calls for infrastructure protection summit   \n",
      "3       20030219           air nz staff in aust strike for pay rise   \n",
      "4       20030219      air nz strike to affect australian travellers   \n",
      "5       20030219                  ambitious olsson wins triple jump   \n",
      "6       20030219         antic delighted with record breaking barca   \n",
      "7       20030219  aussie qualifier stosur wastes four memphis match   \n",
      "8       20030219       aust addresses un security council over iraq   \n",
      "9       20030219         australia is locked into war timetable opp   \n",
      "10      20030219  australia to contribute 10 million in aid to iraq   \n",
      "11      20030219  barca take record as robson celebrates birthda...   \n",
      "12      20030219                         bathhouse plans move ahead   \n",
      "13      20030219      big hopes for launceston cycling championship   \n",
      "14      20030219             big plan to boost paroo water supplies   \n",
      "15      20030219             blizzard buries united states in bills   \n",
      "16      20030219     brigadier dismisses reports troops harassed in   \n",
      "17      20030219     british combat troops arriving daily in kuwait   \n",
      "18      20030219         bryant leads lakers to double overtime win   \n",
      "19      20030219           bushfire victims urged to see centrelink   \n",
      "20      20030219    businesses should prepare for terrorist attacks   \n",
      "21      20030219    calleri avenges final defeat to eliminate massu   \n",
      "22      20030219            call for ethanol blend fuel to go ahead   \n",
      "23      20030219             carews freak goal leaves roma in ruins   \n",
      "24      20030219                       cemeteries miss out on funds   \n",
      "25      20030219  code of conduct toughens organ donation regula...   \n",
      "26      20030219       commonwealth bank cuts fixed home loan rates   \n",
      "27      20030219             community urged to help homeless youth   \n",
      "28      20030219   council chief executive fails to secure position   \n",
      "29      20030219    councillor to contest wollongong as independent   \n",
      "..           ...                                                ...   \n",
      "70      20030219            heavy metal deposits survey nearing end   \n",
      "71      20030219        injured rios pulls out of buenos aires open   \n",
      "72      20030219                inquest finds mans death accidental   \n",
      "73      20030219  investigations underway into death toll of korean   \n",
      "74      20030219     investigation underway into elster creek spill   \n",
      "75      20030219  iraqs neighbours plead for continued un inspec...   \n",
      "76      20030219         iraq to pay for own rebuilding white house   \n",
      "77      20030219              irish man arrested over omagh bombing   \n",
      "78      20030219              irrigators vote over river management   \n",
      "79      20030219                israeli forces push into gaza strip   \n",
      "80      20030219            jury to consider verdict in murder case   \n",
      "81      20030219     juvenile sex offenders unlikely to reoffend as   \n",
      "82      20030219        kelly disgusted at alleged bp ethanol scare   \n",
      "83      20030219         kelly not surprised ethanol confidence low   \n",
      "84      20030219               korean subway fire 314 still missing   \n",
      "85      20030219            last minute call hands alinghi big lead   \n",
      "86      20030219                 low demand forces air service cuts   \n",
      "87      20030219      man arrested after central qld hijack attempt   \n",
      "88      20030219                      man charged over cooma murder   \n",
      "89      20030219       man fined after aboriginal tent embassy raid   \n",
      "90      20030219                         man jailed over keno fraud   \n",
      "91      20030219                 man with knife hijacks light plane   \n",
      "92      20030219      martin to lobby against losing nt seat in fed   \n",
      "93      20030219        massive drug crop discovered in western nsw   \n",
      "94      20030219                    mayor warns landfill protesters   \n",
      "95      20030219           meeting to consider tick clearance costs   \n",
      "96      20030219         meeting to focus on broken hill water woes   \n",
      "97      20030219                      moderate lift in wages growth   \n",
      "98      20030219      more than 40 pc of young men drink alcohol at   \n",
      "99      20030219  more water restrictions predicted for northern...   \n",
      "\n",
      "                                          token_sents  \\\n",
      "0   [aba, decides, against, community, broadcastin...   \n",
      "1   [act, fire, witnesses, must, be, aware, of, de...   \n",
      "2   [a, g, calls, for, infrastructure, protection,...   \n",
      "3   [air, nz, staff, in, aust, strike, for, pay, r...   \n",
      "4   [air, nz, strike, to, affect, australian, trav...   \n",
      "5             [ambitious, olsson, wins, triple, jump]   \n",
      "6   [antic, delighted, with, record, breaking, barca]   \n",
      "7   [aussie, qualifier, stosur, wastes, four, memp...   \n",
      "8   [aust, addresses, un, security, council, over,...   \n",
      "9   [australia, is, locked, into, war, timetable, ...   \n",
      "10  [australia, to, contribute, 10, million, in, a...   \n",
      "11  [barca, take, record, as, robson, celebrates, ...   \n",
      "12                    [bathhouse, plans, move, ahead]   \n",
      "13  [big, hopes, for, launceston, cycling, champio...   \n",
      "14     [big, plan, to, boost, paroo, water, supplies]   \n",
      "15      [blizzard, buries, united, states, in, bills]   \n",
      "16  [brigadier, dismisses, reports, troops, harass...   \n",
      "17  [british, combat, troops, arriving, daily, in,...   \n",
      "18  [bryant, leads, lakers, to, double, overtime, ...   \n",
      "19    [bushfire, victims, urged, to, see, centrelink]   \n",
      "20  [businesses, should, prepare, for, terrorist, ...   \n",
      "21  [calleri, avenges, final, defeat, to, eliminat...   \n",
      "22   [call, for, ethanol, blend, fuel, to, go, ahead]   \n",
      "23     [carews, freak, goal, leaves, roma, in, ruins]   \n",
      "24                 [cemeteries, miss, out, on, funds]   \n",
      "25  [code, of, conduct, toughens, organ, donation,...   \n",
      "26  [commonwealth, bank, cuts, fixed, home, loan, ...   \n",
      "27      [community, urged, to, help, homeless, youth]   \n",
      "28  [council, chief, executive, fails, to, secure,...   \n",
      "29  [councillor, to, contest, wollongong, as, inde...   \n",
      "..                                                ...   \n",
      "70     [heavy, metal, deposits, survey, nearing, end]   \n",
      "71  [injured, rios, pulls, out, of, buenos, aires,...   \n",
      "72          [inquest, finds, mans, death, accidental]   \n",
      "73  [investigations, underway, into, death, toll, ...   \n",
      "74  [investigation, underway, into, elster, creek,...   \n",
      "75  [iraqs, neighbours, plead, for, continued, un,...   \n",
      "76  [iraq, to, pay, for, own, rebuilding, white, h...   \n",
      "77       [irish, man, arrested, over, omagh, bombing]   \n",
      "78        [irrigators, vote, over, river, management]   \n",
      "79         [israeli, forces, push, into, gaza, strip]   \n",
      "80    [jury, to, consider, verdict, in, murder, case]   \n",
      "81  [juvenile, sex, offenders, unlikely, to, reoff...   \n",
      "82  [kelly, disgusted, at, alleged, bp, ethanol, s...   \n",
      "83  [kelly, not, surprised, ethanol, confidence, low]   \n",
      "84        [korean, subway, fire, 314, still, missing]   \n",
      "85    [last, minute, call, hands, alinghi, big, lead]   \n",
      "86          [low, demand, forces, air, service, cuts]   \n",
      "87  [man, arrested, after, central, qld, hijack, a...   \n",
      "88                [man, charged, over, cooma, murder]   \n",
      "89  [man, fined, after, aboriginal, tent, embassy,...   \n",
      "90                   [man, jailed, over, keno, fraud]   \n",
      "91          [man, with, knife, hijacks, light, plane]   \n",
      "92  [martin, to, lobby, against, losing, nt, seat,...   \n",
      "93  [massive, drug, crop, discovered, in, western,...   \n",
      "94               [mayor, warns, landfill, protesters]   \n",
      "95    [meeting, to, consider, tick, clearance, costs]   \n",
      "96  [meeting, to, focus, on, broken, hill, water, ...   \n",
      "97                [moderate, lift, in, wages, growth]   \n",
      "98  [more, than, 40, pc, of, young, men, drink, al...   \n",
      "99  [more, water, restrictions, predicted, for, no...   \n",
      "\n",
      "                                        Sentence_Idxs  \n",
      "0                   [1, 17503, 14329, 8007, 10490, 1]  \n",
      "1   [5442, 13139, 6258, 527, 7345, 6005, 5903, 17111]  \n",
      "2             [127, 13333, 499, 763, 110, 6943, 3801]  \n",
      "3       [2731, 1, 4327, 221, 1, 2547, 763, 253, 3146]  \n",
      "4               [2731, 1, 2547, 7867, 10862, 1431, 1]  \n",
      "5                   [15258, 11711, 4345, 14690, 7355]  \n",
      "6                        [1, 1, 4944, 3776, 11280, 1]  \n",
      "7                      [1, 1, 1, 1, 3581, 4852, 6577]  \n",
      "8              [1, 4656, 14317, 651, 3227, 5865, 123]  \n",
      "9               [2206, 4671, 4916, 578, 833, 7284, 1]  \n",
      "10  [2206, 7867, 1498, 1139, 16070, 221, 13105, 78...  \n",
      "11          [1, 1780, 3776, 5557, 1, 4517, 9006, 221]  \n",
      "12                            [1, 18069, 1030, 15379]  \n",
      "13                    [13949, 4005, 763, 1, 1, 18318]  \n",
      "14          [13949, 1331, 7867, 3610, 1, 14239, 2139]  \n",
      "15                  [16849, 1, 2326, 1212, 221, 4033]  \n",
      "16                [11981, 1, 8219, 12575, 10017, 221]  \n",
      "17         [971, 13939, 12575, 3890, 9787, 221, 2403]  \n",
      "18             [1, 5746, 1, 7867, 1419, 16323, 12923]  \n",
      "19                    [1, 8501, 1578, 7867, 10722, 1]  \n",
      "20                  [6560, 156, 563, 763, 4768, 7822]  \n",
      "21                  [1, 1, 5928, 2603, 7867, 7215, 1]  \n",
      "22         [13291, 763, 1, 1, 4215, 7867, 251, 15379]  \n",
      "23                   [1, 1, 1189, 5019, 1, 221, 5190]  \n",
      "24                      [1, 16527, 14579, 1621, 3351]  \n",
      "25          [12511, 5903, 2379, 1, 14804, 7687, 2016]  \n",
      "26       [13774, 4009, 696, 13662, 4295, 13258, 3145]  \n",
      "27             [8007, 1578, 7867, 4368, 18302, 17011]  \n",
      "28         [3227, 8323, 5412, 5021, 7867, 2953, 4908]  \n",
      "29                    [1, 7867, 10234, 1, 5557, 7371]  \n",
      "..                                                ...  \n",
      "70               [8441, 7946, 7251, 3485, 4615, 2025]  \n",
      "71  [7349, 1, 14681, 14579, 5903, 14427, 14428, 7521]  \n",
      "72                            [1, 622, 1, 8666, 7615]  \n",
      "73          [8111, 3109, 578, 8666, 1239, 5903, 1151]  \n",
      "74                     [13285, 3109, 578, 1, 1, 9788]  \n",
      "75               [1, 1, 7868, 763, 6892, 14317, 7341]  \n",
      "76       [123, 7867, 253, 763, 2778, 646, 9418, 1625]  \n",
      "77                   [5538, 7066, 1124, 5865, 1, 794]  \n",
      "78                       [1, 11448, 5865, 6119, 4018]  \n",
      "79                   [910, 1303, 462, 578, 208, 1906]  \n",
      "80           [5013, 7867, 3708, 4991, 221, 368, 2239]  \n",
      "81              [1, 4941, 10325, 8124, 7867, 1, 5557]  \n",
      "82               [17048, 1, 631, 3065, 10783, 1, 666]  \n",
      "83                [17048, 9184, 8879, 1, 3965, 15160]  \n",
      "84                [1151, 3113, 13139, 1, 8921, 10280]  \n",
      "85          [1141, 6192, 13291, 7638, 1, 13949, 1792]  \n",
      "86                [15160, 464, 1303, 2731, 4267, 696]  \n",
      "87            [7066, 1124, 2970, 1085, 1, 4927, 1742]  \n",
      "88                          [7066, 816, 5865, 1, 368]  \n",
      "89           [7066, 15339, 2970, 1, 6579, 3074, 5239]  \n",
      "90                        [7066, 4378, 5865, 1, 2343]  \n",
      "91                    [7066, 4944, 1, 1, 12447, 2987]  \n",
      "92  [4675, 7867, 9885, 14329, 1227, 1, 5097, 221, ...  \n",
      "93            [5590, 11664, 7959, 2497, 221, 2359, 1]  \n",
      "94                              [2782, 3395, 1, 2966]  \n",
      "95                 [7430, 7867, 3708, 1, 11683, 2008]  \n",
      "96   [7430, 7867, 3982, 1621, 13351, 17862, 14239, 1]  \n",
      "97                      [450, 5367, 221, 12651, 4169]  \n",
      "98  [2348, 10987, 1522, 1, 5903, 13279, 13180, 670...  \n",
      "99             [2348, 14239, 306, 3875, 763, 5535, 1]  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 rows x 4 columns]\n",
      "SENTENCE:\taba\tdecides\tagainst\tcommunity\tbroadcasting\tlicence\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tact\tfire\twitnesses\tmust\tbe\taware\tof\tdefamation\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\ta\tg\tcalls\tfor\tinfrastructure\tprotection\tsummit\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tair\tnz\tstaff\tin\taust\tstrike\tfor\tpay\trise\n",
      "PREDICTED:\tI-gpe\tI-per\tI-per\tI-per\tI-per\tI-per\tI-per\tI-per\tI-per\n",
      "\n",
      "SENTENCE:\tair\tnz\tstrike\tto\taffect\taustralian\ttravellers\n",
      "PREDICTED:\tI-gpe\tI-per\tI-per\tI-per\tI-per\tI-per\tI-per\n",
      "\n",
      "SENTENCE:\tambitious\tolsson\twins\ttriple\tjump\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tantic\tdelighted\twith\trecord\tbreaking\tbarca\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\taussie\tqualifier\tstosur\twastes\tfour\tmemphis\tmatch\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\taust\taddresses\tun\tsecurity\tcouncil\tover\tiraq\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\taustralia\tis\tlocked\tinto\twar\ttimetable\topp\n",
      "PREDICTED:\tB-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\n",
      "\n",
      "SENTENCE:\taustralia\tto\tcontribute\t10\tmillion\tin\taid\tto\tiraq\n",
      "PREDICTED:\tB-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\n",
      "\n",
      "SENTENCE:\tbarca\ttake\trecord\tas\trobson\tcelebrates\tbirthday\tin\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tbathhouse\tplans\tmove\tahead\n",
      "PREDICTED:\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tbig\thopes\tfor\tlaunceston\tcycling\tchampionship\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tbig\tplan\tto\tboost\tparoo\twater\tsupplies\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tblizzard\tburies\tunited\tstates\tin\tbills\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tbrigadier\tdismisses\treports\ttroops\tharassed\tin\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tbritish\tcombat\ttroops\tarriving\tdaily\tin\tkuwait\n",
      "PREDICTED:\tB-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\n",
      "\n",
      "SENTENCE:\tbryant\tleads\tlakers\tto\tdouble\tovertime\twin\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tbushfire\tvictims\turged\tto\tsee\tcentrelink\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tbusinesses\tshould\tprepare\tfor\tterrorist\tattacks\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcalleri\tavenges\tfinal\tdefeat\tto\teliminate\tmassu\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcall\tfor\tethanol\tblend\tfuel\tto\tgo\tahead\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcarews\tfreak\tgoal\tleaves\troma\tin\truins\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcemeteries\tmiss\tout\ton\tfunds\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcode\tof\tconduct\ttoughens\torgan\tdonation\tregulations\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcommonwealth\tbank\tcuts\tfixed\thome\tloan\trates\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcommunity\turged\tto\thelp\thomeless\tyouth\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcouncil\tchief\texecutive\tfails\tto\tsecure\tposition\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcouncillor\tto\tcontest\twollongong\tas\tindependent\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcouncil\tmoves\tto\tprotect\ttas\theritage\tgarden\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcouncil\twelcomes\tambulance\tlevy\tdecision\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcouncil\twelcomes\tinsurance\tbreakthrough\n",
      "PREDICTED:\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tcrean\ttells\talp\tleadership\tcritics\tto\tshut\tup\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdargo\tfire\tthreat\texpected\tto\trise\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdeath\ttoll\tcontinues\tto\tclimb\tin\ts\tkorean\tsubway\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdems\thold\tplebiscite\tover\tiraqi\tconflict\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdent\tdowns\tphilippoussis\tin\ttie\tbreak\tthriller\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tde\tvilliers\tto\tlearn\tfate\ton\tmarch\t5\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdigital\ttv\twill\tbecome\tcommonplace\tsummit\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdirect\tanger\tat\tgovt\tnot\tsoldiers\tcrean\turges\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdispute\tover\tat\tsmithton\tvegetable\tprocessing\tplant\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdog\tmauls\t18\tmonth\told\ttoddler\tin\tnsw\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tdying\tkorean\tsubway\tpassengers\tphoned\tfor\thelp\n",
      "PREDICTED:\tO\tB-gpe\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tengland\tchange\tthree\tfor\twales\tmatch\n",
      "PREDICTED:\tB-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\n",
      "\n",
      "SENTENCE:\tepa\tstill\ttrying\tto\trecover\tchemical\tclean\tup\tcosts\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\texpressions\tof\tinterest\tsought\tto\tbuild\tlivestock\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfed\topp\tto\tre\tintroduce\tnational\tinsurance\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfirefighters\tcontain\tacid\tspill\n",
      "PREDICTED:\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfour\tinjured\tin\thead\ton\thighway\tcrash\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfreedom\trecords\tnet\tprofit\tfor\tthird\tsuccessive\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfunds\tallocated\tfor\tdomestic\tviolence\tvictims\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfunds\tallocated\tfor\tyouth\tat\trisk\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfunds\tannounced\tfor\tbridge\twork\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfunds\tto\tgo\tto\tcadell\tupgrade\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tfunds\tto\thelp\trestore\tcossack\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tgerman\tcourt\tto\tgive\tverdict\ton\tsept\t11\taccused\n",
      "PREDICTED:\tB-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-art\n",
      "\n",
      "SENTENCE:\tgilchrist\tbacks\trest\tpolicy\n",
      "PREDICTED:\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tgirl\tinjured\tin\thead\ton\thighway\tcrash\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tgold\tcoast\tto\thear\tabout\tbilby\tproject\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tgolf\tclub\tfeeling\tsmoking\tban\timpact\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tgovt\tis\tto\tblame\tfor\tethanols\tunpopularity\topp\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tgreens\toffer\tpolice\tstation\talternative\n",
      "PREDICTED:\tB-gpe\tO\tB-gpe\tO\tB-gpe\n",
      "\n",
      "SENTENCE:\tgriffiths\tunder\tfire\tover\tproject\tknock\tback\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tgroup\tto\tmeet\tin\tnorth\twest\twa\tover\trock\tart\n",
      "PREDICTED:\tI-gpe\tI-gpe\tI-art\tI-art\tI-art\tI-art\tI-art\tI-art\tI-art\tI-art\n",
      "\n",
      "SENTENCE:\thacker\tgains\taccess\tto\teight\tmillion\tcredit\tcards\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\thanson\tis\tgrossly\tnaive\tover\tnsw\tissues\tcosta\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\thanson\tshould\tgo\tback\twhere\tshe\tcame\tfrom\tnsw\tmp\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tharrington\traring\tto\tgo\tafter\tbreak\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\thealth\tminister\tbacks\torgan\tand\ttissue\tstorage\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\theavy\tmetal\tdeposits\tsurvey\tnearing\tend\n",
      "PREDICTED:\tB-gpe\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tinjured\trios\tpulls\tout\tof\tbuenos\taires\topen\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tinquest\tfinds\tmans\tdeath\taccidental\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tinvestigations\tunderway\tinto\tdeath\ttoll\tof\tkorean\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tinvestigation\tunderway\tinto\telster\tcreek\tspill\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tiraqs\tneighbours\tplead\tfor\tcontinued\tun\tinspections\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tiraq\tto\tpay\tfor\town\trebuilding\twhite\thouse\n",
      "PREDICTED:\tB-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\tI-geo\n",
      "\n",
      "SENTENCE:\tirish\tman\tarrested\tover\tomagh\tbombing\n",
      "PREDICTED:\tB-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\n",
      "\n",
      "SENTENCE:\tirrigators\tvote\tover\triver\tmanagement\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tisraeli\tforces\tpush\tinto\tgaza\tstrip\n",
      "PREDICTED:\tB-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\n",
      "\n",
      "SENTENCE:\tjury\tto\tconsider\tverdict\tin\tmurder\tcase\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tjuvenile\tsex\toffenders\tunlikely\tto\treoffend\tas\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tkelly\tdisgusted\tat\talleged\tbp\tethanol\tscare\n",
      "PREDICTED:\tB-gpe\tO\tB-gpe\tO\tB-gpe\tO\tB-gpe\n",
      "\n",
      "SENTENCE:\tkelly\tnot\tsurprised\tethanol\tconfidence\tlow\n",
      "PREDICTED:\tB-gpe\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tkorean\tsubway\tfire\t314\tstill\tmissing\n",
      "PREDICTED:\tB-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\tI-gpe\n",
      "\n",
      "SENTENCE:\tlast\tminute\tcall\thands\talinghi\tbig\tlead\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tlow\tdemand\tforces\tair\tservice\tcuts\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tman\tarrested\tafter\tcentral\tqld\thijack\tattempt\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tman\tcharged\tover\tcooma\tmurder\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tman\tfined\tafter\taboriginal\ttent\tembassy\traid\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tman\tjailed\tover\tkeno\tfraud\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tman\twith\tknife\thijacks\tlight\tplane\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tmartin\tto\tlobby\tagainst\tlosing\tnt\tseat\tin\tfed\n",
      "PREDICTED:\tI-per\tI-per\tI-per\tI-per\tI-per\tI-per\tI-per\tI-per\tI-per\n",
      "\n",
      "SENTENCE:\tmassive\tdrug\tcrop\tdiscovered\tin\twestern\tnsw\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tmayor\twarns\tlandfill\tprotesters\n",
      "PREDICTED:\tB-gpe\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tmeeting\tto\tconsider\ttick\tclearance\tcosts\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tmeeting\tto\tfocus\ton\tbroken\thill\twater\twoes\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tmoderate\tlift\tin\twages\tgrowth\n",
      "PREDICTED:\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tmore\tthan\t40\tpc\tof\tyoung\tmen\tdrink\talcohol\tat\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "SENTENCE:\tmore\twater\trestrictions\tpredicted\tfor\tnorthern\ttas\n",
      "PREDICTED:\tO\tO\tO\tO\tO\tO\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_ner_tag_from_words(newData, words_lexicon=all_lower_words_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
