{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Named Entity Recognition\n",
    "\n",
    "In this assignment, we are going to build a Named Entity Recognition model. With this model, we will also tag new data.\n",
    "\n",
    "More on Named Entity Recognition:\n",
    "\n",
    "https://blog.paralleldots.com/data-science/named-entity-recognition-milestone-models-papers-and-technologies/\n",
    "\n",
    "https://blog.paralleldots.com/product/applications-named-entity-recognition-api/\n",
    "\n",
    "### Steps:\n",
    "\n",
    "**1. Import the data**\n",
    "\n",
    "**2. Build the model**\n",
    "\n",
    "**3. Pick a dataset to run the model on**\n",
    "\n",
    "**4. Build a function to load new data and print the tags**\n",
    "\n",
    "Your web application will load small sections of text (such as tweets or headlines) and from that, you will tag the text based on the presence of named entities.\n",
    "\n",
    "*What you will be graded on:*\n",
    "\n",
    "1. Ability to build a model on word and tag data\n",
    "\n",
    "2. Ability to use the model to predict on new data and display that prediction\n",
    "\n",
    "*The model will be based on:*\n",
    "1. Embeddings from words\n",
    "2. Embeddings from tag inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the data\n",
    "\n",
    "Below is some code to get you started. As in the part of speech tagging example, you will have to write code to:\n",
    "\n",
    "0. Split your data into a train/test set (Do a 80/20 or 90/10 split since we'll be later applying this model to an entirely separate set of data)\n",
    "1. Find the set of all words\n",
    "2. Find the set of all tags\n",
    "3. **Create a function called ent_tagger** that will turn a sentence into this output for model building :\n",
    "``` [('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have',  'O'), ('marched',  'O'), ('through',  'O'), ('London', 'B-geo'), ('to',  'O'), ('protest',  'O'), ('the',  'O'), ('war',  'O'), ('in',  'O'), ('Iraq',  'B-geo'), ('and', 'O'), ('demand',  'O'), ('the',  'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops',  'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n",
    "```\n",
    "4. Make a dictionary of words to index and entity tag to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565  Sentence: 47958     impact   NN      O\n",
       "1048566  Sentence: 47958          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568  Sentence: 47959     forces  NNS      O\n",
       "1048569  Sentence: 47959       said  VBD      O\n",
       "1048570  Sentence: 47959       they  PRP      O\n",
       "1048571  Sentence: 47959  responded  VBD      O\n",
       "1048572  Sentence: 47959         to   TO      O\n",
       "1048573  Sentence: 47959        the   DT      O\n",
       "1048574  Sentence: 47959     attack   NN      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"../data/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data so that each sentence is put into a vector per row of a pandas dataframe\n",
    "cleanDat = data.groupby('Sentence #', sort=False).apply(lambda x: pd.DataFrame(data = {'token_sents': [x.Word.tolist()], 'token_tags': [x.Tag.tolist()]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random State\n",
    "seed = np.random.seed(10)\n",
    "\n",
    "# Split data based on sentence number\n",
    "train_sents, test_sents = train_test_split(cleanDat, test_size = .15, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return (lexicon, list(token_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:\n",
      "LEXICON SAMPLE (32876 total items):\n",
      "{'President': 2, 'Bush': 3, 'has': 4, 'outlined': 5, 'the': 6, 'agenda': 7, 'for': 8, 'his': 9, 'second': 10, 'term': 11, 'in': 12, 'office': 13, 'and': 14, 'asked': 15, 'support': 16, 'of': 17, 'all': 18, 'Americans': 19, 'weekly': 20, 'Saturday': 21}\n",
      "\n",
      "TAGS:\n",
      "LEXICON SAMPLE (18 total items):\n",
      "{'B-per': 2, 'I-per': 3, 'O': 4, 'B-gpe': 5, 'B-tim': 6, 'I-tim': 7, 'B-org': 8, 'B-geo': 9, 'I-org': 10, 'B-art': 11, 'I-geo': 12, 'B-eve': 13, 'I-eve': 14, 'I-gpe': 15, 'I-art': 16, 'B-nat': 17, 'I-nat': 18, '<UNK>': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Words:\")\n",
    "words_lexicon, all_words = make_lexicon(train_sents.token_sents)\n",
    "with open('models/words_lexicon.pkl', 'wb') as f: #save the tags lexicon by pickling it\n",
    "    pickle.dump(words_lexicon, f)\n",
    "\n",
    "print('')\n",
    "print(\"TAGS:\")\n",
    "tags_lexicon, all_tags = make_lexicon(train_sents.token_tags)\n",
    "with open('models/tags_lexicon.pkl', 'wb') as f: #save the words lexicon by pickling it\n",
    "    pickle.dump(tags_lexicon, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_tagger(sentence):\n",
    "    return [(word, tag) for word, tag in zip(sentence.token_sent, sentence.token_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Formatting the data\n",
    "Data will need to be\n",
    "\n",
    "1. Indexed\n",
    "2. Limited by vocabulary (ie replace tokens with UNKNOWN if they are too rare, come up with a reasonable limit based on your survey of the data and also model performance)\n",
    "3. Padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Make a dictionary where the string representation of a lexicon item can be retrieved from its numerical index'''\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
    "    print(dict(list(lexicon_lookup.items())[:20]))\n",
    "    return lexicon_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token_sents</th>\n",
       "      <th>Sentence_Idxs</th>\n",
       "      <th>token_tags</th>\n",
       "      <th>Tag_Idxs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence: 30966</th>\n",
       "      <th>0</th>\n",
       "      <td>[President, Bush, has, outlined, the, agenda, ...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 4...</td>\n",
       "      <td>[B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 32442</th>\n",
       "      <th>0</th>\n",
       "      <td>[Commuters, were, angered, Tuesday, morning, w...</td>\n",
       "      <td>[26, 27, 28, 29, 22, 30, 31, 32, 33, 34, 17, 3...</td>\n",
       "      <td>[O, O, O, B-tim, I-tim, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[4, 4, 4, 6, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 13584</th>\n",
       "      <th>0</th>\n",
       "      <td>[Retirement, and, social, assistance, pensions...</td>\n",
       "      <td>[37, 14, 38, 39, 40, 27, 41, 42, 25]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 38902</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, Shepherd, did, so, ,, and, the, Lion, ,,...</td>\n",
       "      <td>[43, 44, 45, 46, 47, 14, 6, 48, 47, 49, 50, 51...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 38245</th>\n",
       "      <th>0</th>\n",
       "      <td>[Lee, ,, a, former, Hyundai, executive, and, S...</td>\n",
       "      <td>[61, 47, 62, 63, 64, 65, 14, 66, 67, 68, 69, 6...</td>\n",
       "      <td>[B-per, O, O, O, B-org, O, O, B-geo, O, O, O, ...</td>\n",
       "      <td>[2, 4, 4, 4, 8, 4, 4, 9, 4, 4, 4, 4, 8, 10, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 8197</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, largest, of, sea, turtles, roams, the, w...</td>\n",
       "      <td>[43, 82, 17, 83, 84, 85, 6, 86, 73, 87, 47, 88...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 47512</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, region, is, located, along, a, major, As...</td>\n",
       "      <td>[43, 92, 93, 94, 95, 62, 96, 97, 98, 99, 8, 10...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 21414</th>\n",
       "      <th>0</th>\n",
       "      <td>[Nobel, laureate, and, former, U.S., Vice, Pre...</td>\n",
       "      <td>[101, 102, 14, 63, 103, 104, 2, 105, 106, 4, 1...</td>\n",
       "      <td>[B-art, O, O, O, B-geo, B-per, I-per, I-per, I...</td>\n",
       "      <td>[11, 4, 4, 4, 9, 2, 3, 3, 3, 4, 4, 4, 8, 4, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 30009</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, infrastructure, loans, are, part, of, $,...</td>\n",
       "      <td>[43, 114, 115, 116, 117, 17, 118, 119, 120, 12...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-geo, O,...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 25218</th>\n",
       "      <th>0</th>\n",
       "      <td>[A, major, figure, in, U.S., journalism, ,, Ti...</td>\n",
       "      <td>[131, 96, 132, 12, 103, 133, 47, 134, 135, 47,...</td>\n",
       "      <td>[O, O, O, O, B-geo, O, O, B-per, I-per, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 9, 4, 4, 2, 3, 4, 4, 4, 4, 9, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         token_sents  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [President, Bush, has, outlined, the, agenda, ...   \n",
       "Sentence: 32442 0  [Commuters, were, angered, Tuesday, morning, w...   \n",
       "Sentence: 13584 0  [Retirement, and, social, assistance, pensions...   \n",
       "Sentence: 38902 0  [The, Shepherd, did, so, ,, and, the, Lion, ,,...   \n",
       "Sentence: 38245 0  [Lee, ,, a, former, Hyundai, executive, and, S...   \n",
       "Sentence: 8197  0  [The, largest, of, sea, turtles, roams, the, w...   \n",
       "Sentence: 47512 0  [The, region, is, located, along, a, major, As...   \n",
       "Sentence: 21414 0  [Nobel, laureate, and, former, U.S., Vice, Pre...   \n",
       "Sentence: 30009 0  [The, infrastructure, loans, are, part, of, $,...   \n",
       "Sentence: 25218 0  [A, major, figure, in, U.S., journalism, ,, Ti...   \n",
       "\n",
       "                                                       Sentence_Idxs  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 4...   \n",
       "Sentence: 32442 0  [26, 27, 28, 29, 22, 30, 31, 32, 33, 34, 17, 3...   \n",
       "Sentence: 13584 0               [37, 14, 38, 39, 40, 27, 41, 42, 25]   \n",
       "Sentence: 38902 0  [43, 44, 45, 46, 47, 14, 6, 48, 47, 49, 50, 51...   \n",
       "Sentence: 38245 0  [61, 47, 62, 63, 64, 65, 14, 66, 67, 68, 69, 6...   \n",
       "Sentence: 8197  0  [43, 82, 17, 83, 84, 85, 6, 86, 73, 87, 47, 88...   \n",
       "Sentence: 47512 0  [43, 92, 93, 94, 95, 62, 96, 97, 98, 99, 8, 10...   \n",
       "Sentence: 21414 0  [101, 102, 14, 63, 103, 104, 2, 105, 106, 4, 1...   \n",
       "Sentence: 30009 0  [43, 114, 115, 116, 117, 17, 118, 119, 120, 12...   \n",
       "Sentence: 25218 0  [131, 96, 132, 12, 103, 133, 47, 134, 135, 47,...   \n",
       "\n",
       "                                                          token_tags  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...   \n",
       "Sentence: 32442 0  [O, O, O, B-tim, I-tim, O, O, O, O, O, O, O, O...   \n",
       "Sentence: 13584 0                        [O, O, O, O, O, O, O, O, O]   \n",
       "Sentence: 38902 0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "Sentence: 38245 0  [B-per, O, O, O, B-org, O, O, B-geo, O, O, O, ...   \n",
       "Sentence: 8197  0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "Sentence: 47512 0            [O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "Sentence: 21414 0  [B-art, O, O, O, B-geo, B-per, I-per, I-per, I...   \n",
       "Sentence: 30009 0  [O, O, O, O, O, O, O, O, O, O, O, O, B-geo, O,...   \n",
       "Sentence: 25218 0  [O, O, O, O, B-geo, O, O, B-per, I-per, O, O, ...   \n",
       "\n",
       "                                                            Tag_Idxs  \n",
       "Sentence #                                                            \n",
       "Sentence: 30966 0  [2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 32442 0         [4, 4, 4, 6, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 13584 0                        [4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 38902 0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 38245 0  [2, 4, 4, 4, 8, 4, 4, 9, 4, 4, 4, 4, 8, 10, 10...  \n",
       "Sentence: 8197  0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 47512 0            [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 21414 0  [11, 4, 4, 4, 9, 2, 3, 3, 3, 4, 4, 4, 8, 4, 2,...  \n",
       "Sentence: 30009 0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 4, ...  \n",
       "Sentence: 25218 0  [4, 4, 4, 4, 9, 4, 4, 2, 3, 4, 4, 4, 4, 9, 4, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_sents['Sentence_Idxs'] = tokens_to_idxs(train_sents['token_sents'], words_lexicon)\n",
    "train_sents['Tag_Idxs'] = tokens_to_idxs(train_sents['token_tags'], tags_lexicon)\n",
    "train_sents[['token_sents', 'Sentence_Idxs', 'token_tags', 'Tag_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      " [[   0    0    0 ...   23   24   25]\n",
      " [   0    0    0 ...   35   36   25]\n",
      " [   0    0    0 ...   41   42   25]\n",
      " ...\n",
      " [   0    0    0 ...    6  324   25]\n",
      " [   0    0    0 ...  265  380   25]\n",
      " [   0    0    0 ... 3720  798   25]]\n",
      "SHAPE: (40765, 105) \n",
      "\n",
      "TAGS:\n",
      " [[0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " ...\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]]\n",
      "SHAPE: (40765, 105) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_sents['Sentence_Idxs']]) # Get length of longest sequence\n",
    "train_padded_words = pad_idx_seqs(train_sents['Sentence_Idxs'], \n",
    "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "train_padded_tags = pad_idx_seqs(train_sents['Tag_Idxs'],\n",
    "                                 max_seq_len + 1)  #Add one to max length for offsetting sequence by 1\n",
    "\n",
    "print(\"WORDS:\\n\", train_padded_words)\n",
    "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n",
    "\n",
    "print(\"TAGS:\\n\", train_padded_tags)\n",
    "print(\"SHAPE:\", train_padded_tags.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Build the model\n",
    "\n",
    "Here we will build a Bidirectional LSTM-CRF model using the `Bidirectional` function from Keras and `CRF` function from Keras-contrib\n",
    "\n",
    "**Documentation and source code:**\n",
    "\n",
    "https://keras.io/layers/wrappers/#bidirectional\n",
    "\n",
    "https://github.com/keras-team/keras-contrib\n",
    "\n",
    "Fit your model with a validation split of 0.1, feel free to use as many epochs as you like. Base your predictions both from the input words **and** the tags from previous words like in the POS example.\n",
    "\n",
    "After building your model, grade your performance on your test set, both by comparing your predicted output to the actual (*at least 3 examples*) and calculate the averaged precision and recall for your tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Concatenate, TimeDistributed, Dense, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_input_len, n_word_input_nodes, n_tag_input_nodes, n_word_embedding_nodes,\n",
    "                 n_tag_embedding_nodes, n_hidden_nodes, n_dense_nodes, \n",
    "                 stateful=False, batch_size=None):\n",
    "    \n",
    "    #Layers 1\n",
    "    word_input = Input(batch_shape=(batch_size, seq_input_len), name='word_input_layer')\n",
    "    tag_input = Input(batch_shape=(batch_size, seq_input_len), name='tag_input_layer')\n",
    "\n",
    "    #Layers 2\n",
    "    word_embeddings = Embedding(input_dim=n_word_input_nodes,\n",
    "                                output_dim=n_word_embedding_nodes, \n",
    "                                mask_zero=True, name='word_embedding_layer')(word_input) #mask_zero will ignore 0 padding\n",
    "    #Output shape = (batch_size, seq_input_len, n_word_embedding_nodes)\n",
    "    tag_embeddings = Embedding(input_dim=n_tag_input_nodes,\n",
    "                               output_dim=n_tag_embedding_nodes,\n",
    "                               mask_zero=True, name='tag_embedding_layer')(tag_input) \n",
    "    #Output shape = (batch_size, seq_input_len, n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 3\n",
    "#     merged_embeddings = Concatenate(axis=-1, name='concat_embedding_layer')([word_embeddings, tag_embeddings])\n",
    "    merged_embeddings = concatenate([word_embeddings, tag_embeddings], name='concat_embedding_layer')\n",
    "    #Output shape =  (batch_size, seq_input_len, n_word_embedding_nodes + n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 4\n",
    "    hidden_layer = Bidirectional(GRU(units=n_hidden_nodes, return_sequences=True, \n",
    "                                     stateful=stateful, name='hidden_layer'))(merged_embeddings)\n",
    "#     hidden_layer = Bidirectional(GRU(units=n_hidden_nodes, return_sequences=True, \n",
    "#                                      stateful=stateful, name='hidden_layer', \n",
    "#                                      recurrent_regularizer=regularizers.l2(.01),\n",
    "#                                      kernel_regularizer=regularizers.l2(0.01),\n",
    "#                                      activity_regularizer=regularizers.l2(0.01)))(merged_embeddings)\n",
    "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 5\n",
    "    dense_layer = TimeDistributed(Dense(units=n_dense_nodes, activation='relu'), name='dense_layer')(hidden_layer)\n",
    "\n",
    "    #Layer 6\n",
    "    crf = CRF(units=n_tag_input_nodes, learn_mode='marginal', sparse_target=True, name='output_layer')\n",
    "#     output_layer = crf(hidden_layer)\n",
    "    output_layer = crf(dense_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "#     adamOpt = Adam(clipvalue = 1, clipnorm = 1)\n",
    "    model.compile(loss=crf.loss_function, optimizer=\"rmsprop\", metrics=[crf.accuracy])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#     output_layer = TimeDistributed(Dense(units=n_tag_input_nodes, \n",
    "#                                          activation='softmax'), name='output_layer')(hidden_layer)\n",
    "#     # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "#     #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "#     model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_word_embedding_nodes=300\n",
    "n_tag_embedding_nodes=150\n",
    "n_hidden_nodes=400\n",
    "n_dense_nodes=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
    "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_word_embedding_nodes=n_word_embedding_nodes,\n",
    "                     n_tag_embedding_nodes=n_tag_embedding_nodes,\n",
    "                     n_hidden_nodes=n_hidden_nodes, \n",
    "                     n_dense_nodes=n_dense_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input_layer (InputLayer)   (None, 104)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tag_input_layer (InputLayer)    (None, 104)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 104, 300)     9863100     word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tag_embedding_layer (Embedding) (None, 104, 150)     2850        tag_input_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concat_embedding_layer (Concate (None, 104, 450)     0           word_embedding_layer[0][0]       \n",
      "                                                                 tag_embedding_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 104, 800)     2042400     concat_embedding_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (TimeDistributed)   (None, 104, 100)     80100       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (CRF)              (None, 104, 19)      2318        dense_layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 11,990,768\n",
      "Trainable params: 11,990,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"ner_temp_model_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32612 samples, validate on 8153 samples\n",
      "Epoch 1/20\n",
      "32612/32612 [==============================] - 151s 5ms/step - loss: 0.1419 - acc: 0.9716 - val_loss: 3.7534e-04 - val_acc: 0.9999\n",
      "\n",
      "Epoch 00001: saving model to ner_temp_model_weights-01-0.1419.hdf5\n",
      "Epoch 2/20\n",
      "32612/32612 [==============================] - 145s 4ms/step - loss: 2.6456e-04 - acc: 0.9999 - val_loss: 1.2219e-04 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00002: saving model to ner_temp_model_weights-02-0.0003.hdf5\n",
      "Epoch 3/20\n",
      "32612/32612 [==============================] - 144s 4ms/step - loss: 1.3761e-04 - acc: 1.0000 - val_loss: 4.3282e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00003: saving model to ner_temp_model_weights-03-0.0001.hdf5\n",
      "Epoch 4/20\n",
      "32612/32612 [==============================] - 145s 4ms/step - loss: 2.8325e-05 - acc: 1.0000 - val_loss: 1.2279e-04 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00004: saving model to ner_temp_model_weights-04-0.0000.hdf5\n",
      "Epoch 5/20\n",
      "32612/32612 [==============================] - 145s 4ms/step - loss: 2.6376e-05 - acc: 1.0000 - val_loss: 5.5467e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00005: saving model to ner_temp_model_weights-05-0.0000.hdf5\n",
      "Epoch 6/20\n",
      "32612/32612 [==============================] - 144s 4ms/step - loss: 1.5845e-05 - acc: 1.0000 - val_loss: 8.1633e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00006: saving model to ner_temp_model_weights-06-0.0000.hdf5\n",
      "Epoch 7/20\n",
      "32612/32612 [==============================] - 144s 4ms/step - loss: 7.4610e-04 - acc: 0.9999 - val_loss: 7.7535e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00007: saving model to ner_temp_model_weights-07-0.0007.hdf5\n",
      "Epoch 8/20\n",
      "32612/32612 [==============================] - 144s 4ms/step - loss: 9.2936e-06 - acc: 1.0000 - val_loss: 9.0071e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00008: saving model to ner_temp_model_weights-08-0.0000.hdf5\n",
      "Epoch 9/20\n",
      "32612/32612 [==============================] - 143s 4ms/step - loss: 1.8388e-06 - acc: 1.0000 - val_loss: 8.2466e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00009: saving model to ner_temp_model_weights-09-0.0000.hdf5\n",
      "Epoch 10/20\n",
      "32612/32612 [==============================] - 144s 4ms/step - loss: 1.8078e-06 - acc: 1.0000 - val_loss: 8.5833e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00010: saving model to ner_temp_model_weights-10-0.0000.hdf5\n",
      "Epoch 11/20\n",
      "32612/32612 [==============================] - 145s 4ms/step - loss: 1.8038e-06 - acc: 1.0000 - val_loss: 8.7048e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00011: saving model to ner_temp_model_weights-11-0.0000.hdf5\n",
      "Epoch 12/20\n",
      "32612/32612 [==============================] - 143s 4ms/step - loss: 1.8021e-06 - acc: 1.0000 - val_loss: 8.8175e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00012: saving model to ner_temp_model_weights-12-0.0000.hdf5\n",
      "Epoch 13/20\n",
      "32612/32612 [==============================] - 143s 4ms/step - loss: 1.8011e-06 - acc: 1.0000 - val_loss: 8.9026e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00013: saving model to ner_temp_model_weights-13-0.0000.hdf5\n",
      "Epoch 14/20\n",
      "32612/32612 [==============================] - 144s 4ms/step - loss: 1.8004e-06 - acc: 1.0000 - val_loss: 8.9815e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00014: saving model to ner_temp_model_weights-14-0.0000.hdf5\n",
      "Epoch 15/20\n",
      "32612/32612 [==============================] - 144s 4ms/step - loss: 1.8000e-06 - acc: 1.0000 - val_loss: 9.0059e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00015: saving model to ner_temp_model_weights-15-0.0000.hdf5\n",
      "Epoch 16/20\n",
      "32612/32612 [==============================] - 142s 4ms/step - loss: 1.7996e-06 - acc: 1.0000 - val_loss: 9.0421e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00016: saving model to ner_temp_model_weights-16-0.0000.hdf5\n",
      "Epoch 17/20\n",
      "32612/32612 [==============================] - 143s 4ms/step - loss: 1.7993e-06 - acc: 1.0000 - val_loss: 9.0915e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00017: saving model to ner_temp_model_weights-17-0.0000.hdf5\n",
      "Epoch 18/20\n",
      "32612/32612 [==============================] - 143s 4ms/step - loss: 1.7991e-06 - acc: 1.0000 - val_loss: 9.1145e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00018: saving model to ner_temp_model_weights-18-0.0000.hdf5\n",
      "Epoch 19/20\n",
      "32612/32612 [==============================] - 143s 4ms/step - loss: 1.7989e-06 - acc: 1.0000 - val_loss: 9.1407e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00019: saving model to ner_temp_model_weights-19-0.0000.hdf5\n",
      "Epoch 20/20\n",
      "32612/32612 [==============================] - 141s 4ms/step - loss: 1.7987e-06 - acc: 1.0000 - val_loss: 9.1636e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00020: saving model to ner_temp_model_weights-20-0.0000.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3df1c07f98>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
    "model.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
    "          y=train_padded_tags[:, 1:, None], \n",
    "          batch_size=128, epochs=20, validation_split=.2, \n",
    "          callbacks=callbacks_list)\n",
    "# model.save_weights('models/ner_temp_model_weights.h5') #Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'B-per', 3: 'I-per', 4: 'O', 5: 'B-gpe', 6: 'B-tim', 7: 'I-tim', 8: 'B-org', 9: 'B-geo', 10: 'I-org', 11: 'B-art', 12: 'I-geo', 13: 'B-eve', 14: 'I-eve', 15: 'I-gpe', 16: 'I-art', 17: 'B-nat', 18: 'I-nat', 1: '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "with open('models/words_lexicon.pkl', 'rb') as f:\n",
    "    words_lexicon = pickle.load(f)\n",
    "    \n",
    "with open('models/tags_lexicon.pkl', 'rb') as f:\n",
    "    tags_lexicon = pickle.load(f)\n",
    "\n",
    "tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)\n",
    "\n",
    "predictor_model = create_model(seq_input_len=1,\n",
    "                               n_word_input_nodes=len(words_lexicon) + 1,\n",
    "                               n_tag_input_nodes=len(tags_lexicon) + 1,\n",
    "                               n_word_embedding_nodes=n_word_embedding_nodes,\n",
    "                               n_tag_embedding_nodes=n_tag_embedding_nodes,\n",
    "                               n_hidden_nodes=n_hidden_nodes, \n",
    "                               n_dense_nodes=n_dense_nodes,\n",
    "                               stateful=True,\n",
    "                               batch_size=1)\n",
    "\n",
    "#Transfer the weights from the trained model\n",
    "predictor_model.load_weights('./ner_temp_model_weights-20-0.0000.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "'''Load the test set and apply same processing steps performed above for training set'''\n",
    "\n",
    "test_sents['Sentence_Idxs'] = tokens_to_idxs(test_sents['token_sents'], words_lexicon)\n",
    "test_sents['Tag_Idxs'] = tokens_to_idxs(test_sents['token_tags'], tags_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predict tags for sentences in test set'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "pred_tags = []\n",
    "for _, sent in test_sents.iterrows():\n",
    "    tok_sent = sent['token_sents']\n",
    "    sent_idxs = sent['Sentence_Idxs']\n",
    "    sent_gold_tags = sent['token_tags']\n",
    "    sent_pred_tags = []\n",
    "    prev_tag = 1  #initialize predicted tag sequence with padding\n",
    "#     prev_tag = 0  #initialize predicted tag sequence with padding\n",
    "    for cur_word in sent_idxs:\n",
    "        # cur_word and prev_tag are just integers, but the model expects an input array\n",
    "        # with the shape (batch_size, seq_input_len), so prepend two dimensions to these values\n",
    "        p_next_tag = predictor_model.predict(x=[numpy.array(cur_word)[None, None],\n",
    "                                                numpy.array(prev_tag)[None, None]])[0]\n",
    "        prev_tag = numpy.argmax(p_next_tag, axis=-1)[0]\n",
    "        sent_pred_tags.append(prev_tag)\n",
    "    predictor_model.reset_states()\n",
    "\n",
    "    #Map tags back to string labels\n",
    "    sent_pred_tags = [tags_lexicon_lookup[tag] for tag in sent_pred_tags]\n",
    "    pred_tags.append(sent_pred_tags) #filter padding \n",
    "\n",
    "test_sents['Predicted_token_tags'] = pred_tags\n",
    "\n",
    "#print sample\n",
    "for _, sent in test_sents[30:50].iterrows():\n",
    "    print(\"SENTENCE:\\t{}\".format(\"\\t\".join(sent['token_sents'])))\n",
    "    print(\"PREDICTED:\\t{}\".format(\"\\t\".join(sent['Predicted_token_tags'])))\n",
    "    print(\"GOLD:\\t\\t{}\".format(\"\\t\".join(sent['token_tags'])))\n",
    "    print(\"CORRECT:\\t{}\".format(\"\\t\".join([str(x) for x in np.array(sent['token_tags']) == np.array(sent['Predicted_token_tags'])])), \"\\n\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Predicted_token_tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Predicted_token_tags'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-071b16d25a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mall_gold_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent_tags\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_tags'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mall_pred_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent_tags\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_token_tags'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_gold_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pred_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_gold_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pred_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Predicted_token_tags'"
     ]
    }
   ],
   "source": [
    "'''Evalute the model by precision, recall, and F1'''\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_gold_tags = [tag for sent_tags in test_sents['token_tags'] for tag in sent_tags]\n",
    "    all_pred_tags = [tag for sent_tags in test_sents['Predicted_token_tags'] for tag in sent_tags]\n",
    "    accuracy = accuracy_score(y_true=all_gold_tags, y_pred=all_pred_tags)\n",
    "    precision = precision_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "    recall = recall_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "    f1 = f1_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "\n",
    "    print(\"ACCURACY: {:.3f}\".format(accuracy))\n",
    "    print(\"PRECISION: {:.3f}\".format(precision))\n",
    "    print(\"RECALL: {:.3f}\".format(recall))\n",
    "    print(\"F1: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Pick a dataset\n",
    "\n",
    "Pick a dataset that has short text, similar to the sentences you just tagged. Headlines and tweets are good choices.\n",
    "\n",
    "https://www.kaggle.com/datasets?sortBy=relevance&group=public&search=news&page=1&pageSize=20&size=all&filetype=all&license=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-afec70b5fbe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtags_lexicon_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "tags_lexicon_lookup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Tag your new data!\n",
    "\n",
    "Create a modification to the **ent_tagger function** that combined words and tags from your original dataset. Now allow the function to also load new text from your new data set, and output the tags predicted from your trained model alongside the text. Make your function load five random texts from your data and output the tagged text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
