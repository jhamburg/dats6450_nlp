{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Named Entity Recognition\n",
    "\n",
    "In this assignment, we are going to build a Named Entity Recognition model. With this model, we will also tag new data.\n",
    "\n",
    "More on Named Entity Recognition:\n",
    "\n",
    "https://blog.paralleldots.com/data-science/named-entity-recognition-milestone-models-papers-and-technologies/\n",
    "\n",
    "https://blog.paralleldots.com/product/applications-named-entity-recognition-api/\n",
    "\n",
    "### Steps:\n",
    "\n",
    "**1. Import the data**\n",
    "\n",
    "**2. Build the model**\n",
    "\n",
    "**3. Pick a dataset to run the model on**\n",
    "\n",
    "**4. Build a function to load new data and print the tags**\n",
    "\n",
    "Your web application will load small sections of text (such as tweets or headlines) and from that, you will tag the text based on the presence of named entities.\n",
    "\n",
    "*What you will be graded on:*\n",
    "\n",
    "1. Ability to build a model on word and tag data\n",
    "\n",
    "2. Ability to use the model to predict on new data and display that prediction\n",
    "\n",
    "*The model will be based on:*\n",
    "1. Embeddings from words\n",
    "2. Embeddings from tag inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the data\n",
    "\n",
    "Below is some code to get you started. As in the part of speech tagging example, you will have to write code to:\n",
    "\n",
    "0. Split your data into a train/test set (Do a 80/20 or 90/10 split since we'll be later applying this model to an entirely separate set of data)\n",
    "1. Find the set of all words\n",
    "2. Find the set of all tags\n",
    "3. **Create a function called ent_tagger** that will turn a sentence into this output for model building :\n",
    "``` [('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have',  'O'), ('marched',  'O'), ('through',  'O'), ('London', 'B-geo'), ('to',  'O'), ('protest',  'O'), ('the',  'O'), ('war',  'O'), ('in',  'O'), ('Iraq',  'B-geo'), ('and', 'O'), ('demand',  'O'), ('the',  'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops',  'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n",
    "```\n",
    "4. Make a dictionary of words to index and entity tag to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Formatting the data\n",
    "Data will need to be\n",
    "\n",
    "1. Indexed\n",
    "2. Limited by vocabulary (ie replace tokens with UNKNOWN if they are too rare, come up with a reasonable limit based on your survey of the data and also model performance)\n",
    "3. Padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Build the model\n",
    "\n",
    "Here we will build a Bidirectional LSTM-CRF model using the `Bidirectional` function from Keras and `CRF` function from Keras-contrib\n",
    "\n",
    "**Documentation and source code:**\n",
    "\n",
    "https://keras.io/layers/wrappers/#bidirectional\n",
    "\n",
    "https://github.com/keras-team/keras-contrib\n",
    "\n",
    "Fit your model with a validation split of 0.1, feel free to use as many epochs as you like. Base your predictions both from the input words **and** the tags from previous words like in the POS example.\n",
    "\n",
    "After building your model, grade your performance on your test set, both by comparing your predicted output to the actual (*at least 3 examples*) and calculate the averaged precision and recall for your tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Pick a dataset\n",
    "\n",
    "Pick a dataset that has short text, similar to the sentences you just tagged. Headlines and tweets are good choices.\n",
    "\n",
    "https://www.kaggle.com/datasets?sortBy=relevance&group=public&search=news&page=1&pageSize=20&size=all&filetype=all&license=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Tag your new data!\n",
    "\n",
    "Create a modification to the **ent_tagger function** that combined words and tags from your original dataset. Now allow the function to also load new text from your new data set, and output the tags predicted from your trained model alongside the text. Make your function load five random texts from your data and output the tagged text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
