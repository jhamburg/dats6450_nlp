{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Named Entity Recognition\n",
    "\n",
    "In this assignment, we are going to build a Named Entity Recognition model. With this model, we will also tag new data.\n",
    "\n",
    "More on Named Entity Recognition:\n",
    "\n",
    "https://blog.paralleldots.com/data-science/named-entity-recognition-milestone-models-papers-and-technologies/\n",
    "\n",
    "https://blog.paralleldots.com/product/applications-named-entity-recognition-api/\n",
    "\n",
    "### Steps:\n",
    "\n",
    "**1. Import the data**\n",
    "\n",
    "**2. Build the model**\n",
    "\n",
    "**3. Pick a dataset to run the model on**\n",
    "\n",
    "**4. Build a function to load new data and print the tags**\n",
    "\n",
    "Your web application will load small sections of text (such as tweets or headlines) and from that, you will tag the text based on the presence of named entities.\n",
    "\n",
    "*What you will be graded on:*\n",
    "\n",
    "1. Ability to build a model on word and tag data\n",
    "\n",
    "2. Ability to use the model to predict on new data and display that prediction\n",
    "\n",
    "*The model will be based on:*\n",
    "1. Embeddings from words\n",
    "2. Embeddings from tag inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the data\n",
    "\n",
    "Below is some code to get you started. As in the part of speech tagging example, you will have to write code to:\n",
    "\n",
    "0. Split your data into a train/test set (Do a 80/20 or 90/10 split since we'll be later applying this model to an entirely separate set of data)\n",
    "1. Find the set of all words\n",
    "2. Find the set of all tags\n",
    "3. **Create a function called ent_tagger** that will turn a sentence into this output for model building :\n",
    "``` [('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have',  'O'), ('marched',  'O'), ('through',  'O'), ('London', 'B-geo'), ('to',  'O'), ('protest',  'O'), ('the',  'O'), ('war',  'O'), ('in',  'O'), ('Iraq',  'B-geo'), ('and', 'O'), ('demand',  'O'), ('the',  'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops',  'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n",
    "```\n",
    "4. Make a dictionary of words to index and entity tag to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565  Sentence: 47958     impact   NN      O\n",
       "1048566  Sentence: 47958          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568  Sentence: 47959     forces  NNS      O\n",
       "1048569  Sentence: 47959       said  VBD      O\n",
       "1048570  Sentence: 47959       they  PRP      O\n",
       "1048571  Sentence: 47959  responded  VBD      O\n",
       "1048572  Sentence: 47959         to   TO      O\n",
       "1048573  Sentence: 47959        the   DT      O\n",
       "1048574  Sentence: 47959     attack   NN      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"../data/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data so that each sentence is put into a vector per row of a pandas dataframe\n",
    "cleanDat = data.groupby('Sentence #', sort=False).apply(lambda x: pd.DataFrame(data = {'token_sents': [x.Word.tolist()], 'token_tags': [x.Tag.tolist()]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random State\n",
    "seed = np.random.seed(10)\n",
    "\n",
    "# Split data based on sentence number\n",
    "train_sents, test_sents = train_test_split(cleanDat, test_size = .15, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return (lexicon, list(token_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON SAMPLE (32876 total items):\n",
      "{'President': 2, 'Bush': 3, 'has': 4, 'outlined': 5, 'the': 6, 'agenda': 7, 'for': 8, 'his': 9, 'second': 10, 'term': 11, 'in': 12, 'office': 13, 'and': 14, 'asked': 15, 'support': 16, 'of': 17, 'all': 18, 'Americans': 19, 'weekly': 20, 'Saturday': 21}\n",
      "LEXICON SAMPLE (18 total items):\n",
      "{'B-per': 2, 'I-per': 3, 'O': 4, 'B-gpe': 5, 'B-tim': 6, 'I-tim': 7, 'B-org': 8, 'B-geo': 9, 'I-org': 10, 'B-art': 11, 'I-geo': 12, 'B-eve': 13, 'I-eve': 14, 'I-gpe': 15, 'I-art': 16, 'B-nat': 17, 'I-nat': 18, '<UNK>': 1}\n"
     ]
    }
   ],
   "source": [
    "words_lexicon, all_words = make_lexicon(train_sents.token_sents)\n",
    "tags_lexicon, all_tags = make_lexicon(train_sents.token_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_tagger(sentence):\n",
    "    return [(word, tag) for word, tag in zip(sentence.token_sent, sentence.token_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Formatting the data\n",
    "Data will need to be\n",
    "\n",
    "1. Indexed\n",
    "2. Limited by vocabulary (ie replace tokens with UNKNOWN if they are too rare, come up with a reasonable limit based on your survey of the data and also model performance)\n",
    "3. Padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token_sents</th>\n",
       "      <th>Sentence_Idxs</th>\n",
       "      <th>token_tags</th>\n",
       "      <th>Tag_Idxs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence: 30966</th>\n",
       "      <th>0</th>\n",
       "      <td>[President, Bush, has, outlined, the, agenda, ...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 4...</td>\n",
       "      <td>[B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 32442</th>\n",
       "      <th>0</th>\n",
       "      <td>[Commuters, were, angered, Tuesday, morning, w...</td>\n",
       "      <td>[26, 27, 28, 29, 22, 30, 31, 32, 33, 34, 17, 3...</td>\n",
       "      <td>[O, O, O, B-tim, I-tim, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[4, 4, 4, 6, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 13584</th>\n",
       "      <th>0</th>\n",
       "      <td>[Retirement, and, social, assistance, pensions...</td>\n",
       "      <td>[37, 14, 38, 39, 40, 27, 41, 42, 25]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 38902</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, Shepherd, did, so, ,, and, the, Lion, ,,...</td>\n",
       "      <td>[43, 44, 45, 46, 47, 14, 6, 48, 47, 49, 50, 51...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 38245</th>\n",
       "      <th>0</th>\n",
       "      <td>[Lee, ,, a, former, Hyundai, executive, and, S...</td>\n",
       "      <td>[61, 47, 62, 63, 64, 65, 14, 66, 67, 68, 69, 6...</td>\n",
       "      <td>[B-per, O, O, O, B-org, O, O, B-geo, O, O, O, ...</td>\n",
       "      <td>[2, 4, 4, 4, 8, 4, 4, 9, 4, 4, 4, 4, 8, 10, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 8197</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, largest, of, sea, turtles, roams, the, w...</td>\n",
       "      <td>[43, 82, 17, 83, 84, 85, 6, 86, 73, 87, 47, 88...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 47512</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, region, is, located, along, a, major, As...</td>\n",
       "      <td>[43, 92, 93, 94, 95, 62, 96, 97, 98, 99, 8, 10...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 21414</th>\n",
       "      <th>0</th>\n",
       "      <td>[Nobel, laureate, and, former, U.S., Vice, Pre...</td>\n",
       "      <td>[101, 102, 14, 63, 103, 104, 2, 105, 106, 4, 1...</td>\n",
       "      <td>[B-art, O, O, O, B-geo, B-per, I-per, I-per, I...</td>\n",
       "      <td>[11, 4, 4, 4, 9, 2, 3, 3, 3, 4, 4, 4, 8, 4, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 30009</th>\n",
       "      <th>0</th>\n",
       "      <td>[The, infrastructure, loans, are, part, of, $,...</td>\n",
       "      <td>[43, 114, 115, 116, 117, 17, 118, 119, 120, 12...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-geo, O,...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 25218</th>\n",
       "      <th>0</th>\n",
       "      <td>[A, major, figure, in, U.S., journalism, ,, Ti...</td>\n",
       "      <td>[131, 96, 132, 12, 103, 133, 47, 134, 135, 47,...</td>\n",
       "      <td>[O, O, O, O, B-geo, O, O, B-per, I-per, O, O, ...</td>\n",
       "      <td>[4, 4, 4, 4, 9, 4, 4, 2, 3, 4, 4, 4, 4, 9, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         token_sents  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [President, Bush, has, outlined, the, agenda, ...   \n",
       "Sentence: 32442 0  [Commuters, were, angered, Tuesday, morning, w...   \n",
       "Sentence: 13584 0  [Retirement, and, social, assistance, pensions...   \n",
       "Sentence: 38902 0  [The, Shepherd, did, so, ,, and, the, Lion, ,,...   \n",
       "Sentence: 38245 0  [Lee, ,, a, former, Hyundai, executive, and, S...   \n",
       "Sentence: 8197  0  [The, largest, of, sea, turtles, roams, the, w...   \n",
       "Sentence: 47512 0  [The, region, is, located, along, a, major, As...   \n",
       "Sentence: 21414 0  [Nobel, laureate, and, former, U.S., Vice, Pre...   \n",
       "Sentence: 30009 0  [The, infrastructure, loans, are, part, of, $,...   \n",
       "Sentence: 25218 0  [A, major, figure, in, U.S., journalism, ,, Ti...   \n",
       "\n",
       "                                                       Sentence_Idxs  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 4...   \n",
       "Sentence: 32442 0  [26, 27, 28, 29, 22, 30, 31, 32, 33, 34, 17, 3...   \n",
       "Sentence: 13584 0               [37, 14, 38, 39, 40, 27, 41, 42, 25]   \n",
       "Sentence: 38902 0  [43, 44, 45, 46, 47, 14, 6, 48, 47, 49, 50, 51...   \n",
       "Sentence: 38245 0  [61, 47, 62, 63, 64, 65, 14, 66, 67, 68, 69, 6...   \n",
       "Sentence: 8197  0  [43, 82, 17, 83, 84, 85, 6, 86, 73, 87, 47, 88...   \n",
       "Sentence: 47512 0  [43, 92, 93, 94, 95, 62, 96, 97, 98, 99, 8, 10...   \n",
       "Sentence: 21414 0  [101, 102, 14, 63, 103, 104, 2, 105, 106, 4, 1...   \n",
       "Sentence: 30009 0  [43, 114, 115, 116, 117, 17, 118, 119, 120, 12...   \n",
       "Sentence: 25218 0  [131, 96, 132, 12, 103, 133, 47, 134, 135, 47,...   \n",
       "\n",
       "                                                          token_tags  \\\n",
       "Sentence #                                                             \n",
       "Sentence: 30966 0  [B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...   \n",
       "Sentence: 32442 0  [O, O, O, B-tim, I-tim, O, O, O, O, O, O, O, O...   \n",
       "Sentence: 13584 0                        [O, O, O, O, O, O, O, O, O]   \n",
       "Sentence: 38902 0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "Sentence: 38245 0  [B-per, O, O, O, B-org, O, O, B-geo, O, O, O, ...   \n",
       "Sentence: 8197  0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "Sentence: 47512 0            [O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "Sentence: 21414 0  [B-art, O, O, O, B-geo, B-per, I-per, I-per, I...   \n",
       "Sentence: 30009 0  [O, O, O, O, O, O, O, O, O, O, O, O, B-geo, O,...   \n",
       "Sentence: 25218 0  [O, O, O, O, B-geo, O, O, B-per, I-per, O, O, ...   \n",
       "\n",
       "                                                            Tag_Idxs  \n",
       "Sentence #                                                            \n",
       "Sentence: 30966 0  [2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 32442 0         [4, 4, 4, 6, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 13584 0                        [4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 38902 0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 38245 0  [2, 4, 4, 4, 8, 4, 4, 9, 4, 4, 4, 4, 8, 10, 10...  \n",
       "Sentence: 8197  0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "Sentence: 47512 0            [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]  \n",
       "Sentence: 21414 0  [11, 4, 4, 4, 9, 2, 3, 3, 3, 4, 4, 4, 8, 4, 2,...  \n",
       "Sentence: 30009 0  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 4, ...  \n",
       "Sentence: 25218 0  [4, 4, 4, 4, 9, 4, 4, 2, 3, 4, 4, 4, 4, 9, 4, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_sents['Sentence_Idxs'] = tokens_to_idxs(train_sents['token_sents'], words_lexicon)\n",
    "train_sents['Tag_Idxs'] = tokens_to_idxs(train_sents['token_tags'], tags_lexicon)\n",
    "train_sents[['token_sents', 'Sentence_Idxs', 'token_tags', 'Tag_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      " [[   0    0    0 ...   23   24   25]\n",
      " [   0    0    0 ...   35   36   25]\n",
      " [   0    0    0 ...   41   42   25]\n",
      " ...\n",
      " [   0    0    0 ...    6  324   25]\n",
      " [   0    0    0 ...  265  380   25]\n",
      " [   0    0    0 ... 3720  798   25]]\n",
      "SHAPE: (40765, 105) \n",
      "\n",
      "TAGS:\n",
      " [[0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " ...\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]\n",
      " [0 0 0 ... 4 4 4]]\n",
      "SHAPE: (40765, 105) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_sents['Sentence_Idxs']]) # Get length of longest sequence\n",
    "train_padded_words = pad_idx_seqs(train_sents['Sentence_Idxs'], \n",
    "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "train_padded_tags = pad_idx_seqs(train_sents['Tag_Idxs'],\n",
    "                                 max_seq_len + 1)  #Add one to max length for offsetting sequence by 1\n",
    "\n",
    "print(\"WORDS:\\n\", train_padded_words)\n",
    "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n",
    "\n",
    "print(\"TAGS:\\n\", train_padded_tags)\n",
    "print(\"SHAPE:\", train_padded_tags.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Build the model\n",
    "\n",
    "Here we will build a Bidirectional LSTM-CRF model using the `Bidirectional` function from Keras and `CRF` function from Keras-contrib\n",
    "\n",
    "**Documentation and source code:**\n",
    "\n",
    "https://keras.io/layers/wrappers/#bidirectional\n",
    "\n",
    "https://github.com/keras-team/keras-contrib\n",
    "\n",
    "Fit your model with a validation split of 0.1, feel free to use as many epochs as you like. Base your predictions both from the input words **and** the tags from previous words like in the POS example.\n",
    "\n",
    "After building your model, grade your performance on your test set, both by comparing your predicted output to the actual (*at least 3 examples*) and calculate the averaged precision and recall for your tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Concatenate, TimeDistributed, Dense, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_input_len, n_word_input_nodes, n_tag_input_nodes, n_word_embedding_nodes,\n",
    "                 n_tag_embedding_nodes, n_hidden_nodes, n_dense_nodes, \n",
    "                 stateful=False, batch_size=None):\n",
    "    \n",
    "    #Layers 1\n",
    "    word_input = Input(batch_shape=(batch_size, seq_input_len), name='word_input_layer')\n",
    "    tag_input = Input(batch_shape=(batch_size, seq_input_len), name='tag_input_layer')\n",
    "\n",
    "    #Layers 2\n",
    "    word_embeddings = Embedding(input_dim=n_word_input_nodes,\n",
    "                                output_dim=n_word_embedding_nodes, \n",
    "                                mask_zero=True, name='word_embedding_layer')(word_input) #mask_zero will ignore 0 padding\n",
    "    #Output shape = (batch_size, seq_input_len, n_word_embedding_nodes)\n",
    "    tag_embeddings = Embedding(input_dim=n_tag_input_nodes,\n",
    "                               output_dim=n_tag_embedding_nodes,\n",
    "                               mask_zero=True, name='tag_embedding_layer')(tag_input) \n",
    "    #Output shape = (batch_size, seq_input_len, n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 3\n",
    "#     merged_embeddings = Concatenate(axis=-1, name='concat_embedding_layer')([word_embeddings, tag_embeddings])\n",
    "    merged_embeddings = concatenate([word_embeddings, tag_embeddings], name='concat_embedding_layer')\n",
    "    #Output shape =  (batch_size, seq_input_len, n_word_embedding_nodes + n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 4\n",
    "    hidden_layer = Bidirectional(GRU(units=n_hidden_nodes, return_sequences=True, \n",
    "                       stateful=stateful, name='hidden_layer'))(merged_embeddings)\n",
    "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 5\n",
    "    dense_layer = TimeDistributed(Dense(units=n_dense_nodes, activation='relu'), name='dense_layer')(hidden_layer)\n",
    "\n",
    "    #Layer 6\n",
    "    crf = CRF(units=n_tag_input_nodes, sparse_target=True, name='output_layer')\n",
    "    output_layer = crf(dense_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "    model.compile(loss=crf.loss_function,\n",
    "                  optimizer='adam', metrics=[crf.accuracy])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
    "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_word_embedding_nodes=300,\n",
    "                     n_tag_embedding_nodes=100,\n",
    "                     n_hidden_nodes=200, n_dense_nodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input_layer (InputLayer)   (None, 104)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tag_input_layer (InputLayer)    (None, 104)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 104, 300)     9863100     word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tag_embedding_layer (Embedding) (None, 104, 100)     1900        tag_input_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concat_embedding_layer (Concate (None, 104, 400)     0           word_embedding_layer[0][0]       \n",
      "                                                                 tag_embedding_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 104, 400)     721200      concat_embedding_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (TimeDistributed)   (None, 104, 100)     40100       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (CRF)              (None, 104, 19)      2318        dense_layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 10,628,618\n",
      "Trainable params: 10,628,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36688 samples, validate on 4077 samples\n",
      "Epoch 1/500\n",
      "36688/36688 [==============================] - 291s 8ms/step - loss: nan - acc: 0.5633 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "36688/36688 [==============================] - 284s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "36688/36688 [==============================] - 284s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "36688/36688 [==============================] - 285s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "36688/36688 [==============================] - 284s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "36688/36688 [==============================] - 285s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "36688/36688 [==============================] - 283s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "36688/36688 [==============================] - 282s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "36688/36688 [==============================] - 275s 7ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "36688/36688 [==============================] - 272s 7ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "36688/36688 [==============================] - 293s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "36688/36688 [==============================] - 284s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "36688/36688 [==============================] - 285s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "36688/36688 [==============================] - 284s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 53/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "36688/36688 [==============================] - 292s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "36688/36688 [==============================] - 285s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "36688/36688 [==============================] - 285s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "36688/36688 [==============================] - 285s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "36688/36688 [==============================] - 284s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "36688/36688 [==============================] - 285s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "36688/36688 [==============================] - 291s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "36688/36688 [==============================] - 284s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "36688/36688 [==============================] - 292s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "36688/36688 [==============================] - 285s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36688/36688 [==============================] - 295s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "36688/36688 [==============================] - 284s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "36688/36688 [==============================] - 288s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "36688/36688 [==============================] - 291s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "36688/36688 [==============================] - 286s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "36688/36688 [==============================] - 290s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "36688/36688 [==============================] - 289s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "36688/36688 [==============================] - 287s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "30464/36688 [=======================>......] - ETA: 44s - loss: nan - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-cdf7ca01e6b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m model.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n\u001b[0;32m      5\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_padded_tags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m           batch_size=128, epochs=500, validation_split=.1, verbose=1)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/ner_temp_model_weights.h5'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
    "model.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
    "          y=train_padded_tags[:, 1:, None], \n",
    "          batch_size=128, epochs=500, validation_split=.1, verbose=1)\n",
    "model.save_weights('models/ner_temp_model_weights.h5') #Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/ner_temp_model_weights.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Pick a dataset\n",
    "\n",
    "Pick a dataset that has short text, similar to the sentences you just tagged. Headlines and tweets are good choices.\n",
    "\n",
    "https://www.kaggle.com/datasets?sortBy=relevance&group=public&search=news&page=1&pageSize=20&size=all&filetype=all&license=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Tag your new data!\n",
    "\n",
    "Create a modification to the **ent_tagger function** that combined words and tags from your original dataset. Now allow the function to also load new text from your new data set, and output the tags predicted from your trained model alongside the text. Make your function load five random texts from your data and output the tagged text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
