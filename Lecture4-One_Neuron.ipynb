{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, run:\n",
    "```\n",
    " conda install -c conda-forge ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to train a neural net to predict a bilinear function shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "n_points = 200\n",
    "x = np.linspace(0, 2, n_points)\n",
    "y = np.array([0] * int(n_points / 2) + list(x[:int(n_points / 2)])) * 2\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(x, y, linewidth=2)\n",
    "plt.title('ridiculously simple data')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may look familiar to the ReLu activation function!\n",
    "f(x)=max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a one neuron model on this data. For a reminder this is how the neuron (or perceptron) functions within the model:\n",
    "\n",
    "![image](http://www.swanintelligence.com/images/2016q1/neuron.png)\n",
    "\n",
    "We have two choices here. One is the initialization of the weights. We choose them to be randomly drawn from a normal distribution. The second choice is the activation function. The chosen ReLu function looks similar to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "# print initial weights\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "'neural net initialized with weights w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TrainingHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.predictions = []\n",
    "        self.i = 0\n",
    "        self.save_every = 50\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.i += 1        \n",
    "        if self.i % self.save_every == 0:        \n",
    "            pred = model.predict(X_train)\n",
    "            self.predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = TrainingHistory()\n",
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          epochs=2000,\n",
    "          verbose=0,\n",
    "          batch_size=50,\n",
    "          callbacks=[history])\n",
    "\n",
    "# print trained weights\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "'neural net weights after training w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make the animation\n",
    "import matplotlib.animation as animation\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return line,\n",
    "\n",
    "def update_line(num):\n",
    "    line.set_xdata(x)\n",
    "    line.set_ydata(history.predictions[num])\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_line, init_func=init, frames=len(history.predictions),\n",
    "                                   interval=50, blit=True)\n",
    "\n",
    "ani.save('neuron_training.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try tuning the batch size - Keras uses stochastic gradient descent to update the weights. It randomly selects a subset of our data for each iteration an does a gradient descent on the error on this subset. By default Keras uses 128 data point on each iteration. In a few cases, when the sample would be very skewed, then the optimal weight update for the sample might actually make the predictions worse for the whole data set.\n",
    "\n",
    "The sample size for stochastic gradient descent is a parameter to the Model.fit() method called batch_size. If we use a larger batch size, we will see a monotonously descereasing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = TrainingHistory()\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=200,\n",
    "          epochs=2000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization also matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "history = TrainingHistory()\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net initialized with weigths w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))\n",
    "\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=200,\n",
    "          epochs=2000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net weigths after training w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))\n",
    "\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neuron's weights don't get updated during training. This is known as the dying ReLu problem. If the initial weights map all our sample points to values smaller than 0, the ReLu maps everything to 0. Even with small changes in the weights the result is still 0. This means the gradient is 0 and the weights never get updated. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
